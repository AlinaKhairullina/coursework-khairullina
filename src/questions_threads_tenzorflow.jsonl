{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyNTc5Nzkw", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/2297#discussion_r62579790", "comments": [{"message": "Could you do the one in conv_ops.cc as well? \n\ntensorflow/core/kernels/conv_ops.cc\n", "timestamp": "2016-05-09T21:53:57Z", "author": "MDQ6VXNlcjE1NzM2OTEw"}, {"message": "Done.  Can't say I wasn't consistent!\n", "timestamp": "2016-05-09T21:59:27Z", "author": "MDQ6VXNlcjQ2MzczNw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0OTkzODEy", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/2555#discussion_r64993812", "comments": [{"message": "alignment is messed up here -- this seems unrelated to this commit, maybe put in a different PR?\n", "timestamp": "2016-05-28T20:29:37Z", "author": "MDQ6VXNlcjQ2MzczNw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDg3OTI4MTM2", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/4222#discussion_r87928136", "comments": [{"message": "What does this bit do?\n", "timestamp": "2016-11-15T00:55:34Z", "author": "MDQ6VXNlcjcwNTEx"}, {"message": "I grabbed this from the GIF tests in python/ops/image_ops_test.py. The [test image](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/gif/testdata/scan.gif) is a multi-framed image of a white bar travelling across a black background. It looks like this test starts with an all-black (zeros) array and then fills in the appropriate pixels with white (255) depending on which frame it is and then checks to make sure that the read pixels match.\n\nI can comment an explanation in or remove this check if you want.\n", "timestamp": "2016-11-15T01:08:01Z", "author": "MDQ6VXNlcjExNjA3MjA1"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc4NjUzOTcz", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/4351#discussion_r78653973", "comments": [{"message": "Just to confirm, are float types the only types that could potentially added here? Or are doubles, ints, etc also possible?\n", "timestamp": "2016-09-13T22:08:44Z", "author": "MDQ6VXNlcjMzNzY4MTc="}, {"message": "Nevermind, just saw your other reply, thanks.\n", "timestamp": "2016-09-13T22:09:36Z", "author": "MDQ6VXNlcjMzNzY4MTc="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDg2NDQzNzYw", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/4826#discussion_r86443760", "comments": [{"message": "vrv@, ebrevdo@ - does it make a difference whether this is a function or a Tensor? Or performance / memory wise its all the same? \n", "timestamp": "2016-11-03T21:08:27Z", "author": "MDQ6VXNlcjE0NDExNA=="}, {"message": "If we want to use a function to handle this case, we have to define a set of functions to cooperate with it. While it would only run once and then copy value to the variable, which is not expensive. Do you think so?\n", "timestamp": "2016-11-04T01:14:00Z", "author": "MDQ6VXNlcjE2OTQzMzUz"}, {"message": "i believe zeros_initializer was recently converted to a function instead of a tensor.", "timestamp": "2016-11-30T04:57:30Z", "author": "MDQ6VXNlcjE3OTQ3MTU="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk4Mjk5MTQx", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/7117#discussion_r98299141", "comments": [{"message": "Sometimes, or always?\r\nThe issue seems to mean most of the time we have the path without \"1\"", "timestamp": "2017-01-27T22:02:31Z", "author": "MDQ6VXNlcjc5NDY4MDk="}, {"message": "CUDA seems to have the \"1\" on Linux systems. I don't know if the difference between Mac and Linux is intended. But in case it's not intended on Mac and NVIDIA fixes it later, we won't have to make changes because of the code here.", "timestamp": "2017-01-27T22:06:00Z", "author": "MDQ6VXNlcjE2ODI0NzAy"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwMjg0NjY4MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/7820#discussion_r102846681", "comments": [{"message": "Just double checking here that it's correct?", "timestamp": "2017-02-23T23:33:08Z", "author": "MDQ6VXNlcjIwOTU5ODUz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMDAyMTI5OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/8943#discussion_r110021298", "comments": [{"message": "I don't see where this is used.", "timestamp": "2017-04-05T20:35:49Z", "author": "MDQ6VXNlcjE1Njc2OTEz"}, {"message": "removed", "timestamp": "2017-04-07T08:20:46Z", "author": "MDQ6VXNlcjEyMDc1ODQ4"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMDIzMzY3MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/8968#discussion_r110233671", "comments": [{"message": "Line 247, I don't understand how this could be a rewrite; where are A, B and C after rewrite?", "timestamp": "2017-04-06T18:25:28Z", "author": "MDQ6VXNlcjEwMzQ3MTY="}, {"message": "And what is the distance in this example, is it 1?", "timestamp": "2017-04-06T18:26:00Z", "author": "MDQ6VXNlcjEwMzQ3MTY="}, {"message": "In line 247, we do not rewrite MklConv2D. We only rewrite BiasAddGrad into Conv2DWithBiasBackpropBias. MklConv2D is used in the example because that is the context that we look for while rewriting BiasAddGrad.", "timestamp": "2017-04-07T16:46:12Z", "author": "MDQ6VXNlcjIyMzA0NTAy"}, {"message": "Right, the distance in the example is 1.", "timestamp": "2017-04-07T16:46:34Z", "author": "MDQ6VXNlcjIyMzA0NTAy"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMzYwOTE3Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/9117#discussion_r113609173", "comments": [{"message": "Why is this needed ? ", "timestamp": "2017-04-27T03:36:49Z", "author": "MDQ6VXNlcjY5Njk2ODY="}, {"message": "Fixed in: https://github.com/tensorflow/tensorflow/pull/9117/commits/73e405572a59ba8ab1d92dc3f7cddff5c9fb240c \r\n\r\nThat should not be there. Auto merge conflict resolution cannot be trusted. ", "timestamp": "2017-04-27T12:43:35Z", "author": "MDQ6VXNlcjgzNzM3OTU="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMTY3NzMxMw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/9236#discussion_r111677313", "comments": [{"message": "can these be tensors?  if so, what dtype and shapes?  does this broadcast with sig?", "timestamp": "2017-04-16T03:37:17Z", "author": "MDQ6VXNlcjE3OTQ3MTU="}, {"message": "I added `tf.convert_to_tensor` for each of them.\r\nFor no, only one-dimensional (except `batch_size`) signals are supported.", "timestamp": "2017-04-17T18:29:34Z", "author": "MDQ6VXNlcjE2NDM3MTU2"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNzUzMTU2Nw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/9551#discussion_r117531567", "comments": [{"message": "are the changes in this file still required?", "timestamp": "2017-05-19T17:21:19Z", "author": "MDQ6VXNlcjQ2MzczNw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNDQ3MzMxNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/9606#discussion_r114473314", "comments": [{"message": "is this a python scalar, list, tensor?  if so, what shape?", "timestamp": "2017-05-03T05:12:10Z", "author": "MDQ6VXNlcjE3OTQ3MTU="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNjUzOTg2MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/9877#discussion_r116539861", "comments": [{"message": "This would still fail if a multi-frame (animated) GIF image is input, but perhaps that's OK?", "timestamp": "2017-05-15T16:36:11Z", "author": "MDQ6VXNlcjE2OTA3NTM0"}, {"message": "@freedomtan Perhaps you can add a line to the documentation at the top to mention that animated GIFs are not expected/supported.", "timestamp": "2017-05-15T16:38:41Z", "author": "MDQ6VXNlcjE2OTA3NTM0"}, {"message": "done", "timestamp": "2017-05-16T02:24:35Z", "author": "MDQ6VXNlcjMzOTU5OTg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNjU4ODMzOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/9889#discussion_r116588338", "comments": [{"message": "Undo the change here as well?", "timestamp": "2017-05-15T20:08:33Z", "author": "MDQ6VXNlcjUwNjE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNjQ5ODEyMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/9913#discussion_r116498121", "comments": [{"message": "Is the intent here to check the endian-ness of the encoding? (Sounds good, but probably deserves a comment.)", "timestamp": "2017-05-15T14:00:47Z", "author": "MDQ6VXNlcjM0ODkzMg=="}, {"message": "Just a check that getting data via the InternalData and MutableInternalData work as expected.    It does veryify the endianess, but also that there are no weird things going on with 'bytes'.  I don't trust the protobuf 'bytes' as it is implemented as a std::string, which seems like a weird container to choose when std::vector<uint8> would work.", "timestamp": "2017-05-15T14:41:56Z", "author": "MDQ6VXNlcjYwNjgzMQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyMzU4MjM4Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/10522#discussion_r123582383", "comments": [{"message": "What is the reason for this change?\r\n\r\nSeveral of the return paths for the function return negative numbers (INVALID_FREQUENCY), some now with implicit casts to unsigned.  \r\n\r\nThe only use in the rest of this patch adds the opposide cast of INVALID_FREQUENCY back to unsigned.  \r\n\r\nWhat am I missing here?", "timestamp": "2017-06-22T18:03:40Z", "author": "MDQ6VXNlcjExNTQ3ODAx"}, {"message": "@sandipmgiri could you address this?", "timestamp": "2017-06-26T18:58:54Z", "author": "MDQ6VXNlcjIwOTU5ODUz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyMzU4MzE4Mg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/10522#discussion_r123583182", "comments": [{"message": "As I read the posts linked to above, -np.inf is technically undefined?  \r\nI think this is a little ugly, but fine as a temporary fix, and it doesn't affect other architectures.", "timestamp": "2017-06-22T18:07:19Z", "author": "MDQ6VXNlcjExNTQ3ODAx"}, {"message": "@sandipmgiri could you fix the comment style?", "timestamp": "2017-06-26T18:59:28Z", "author": "MDQ6VXNlcjIwOTU5ODUz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyMjMwNTQzOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/10531#discussion_r122305439", "comments": [{"message": "Does std::move work properly for GPUTensor?", "timestamp": "2017-06-15T20:31:51Z", "author": "MDQ6VXNlcjEyMDc1ODQ4"}, {"message": "std::move should be ok for GPU tensor, see the code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.h#L692). Actually the [standard assignment](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.h#L161) also just an pointer assignment.", "timestamp": "2017-06-16T02:17:01Z", "author": "MDQ6VXNlcjE5MjgyOQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNTE3MzIyMA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/10598#discussion_r125173220", "comments": [{"message": "It's surprising that everything works fine on Linux and fails on Windows. It is extremely unlikely  but could it have something to do with the standard library implementation of acosh, since that is what is used in the background for computation.", "timestamp": "2017-07-02T04:05:47Z", "author": "MDQ6VXNlcjc5NzYzMTU="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1OTc3MjMxMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/10752#discussion_r159772312", "comments": [{"message": "can this happen anywhere except t=0?", "timestamp": "2018-01-04T22:32:41Z", "author": "MDQ6VXNlcjE3OTQ3MTU="}, {"message": "Do you mean if `next_beam_probs > -np.Inf` can be evaluated to be True if t>0? it can.\r\n\r\nOr maybe you mean `next_beam_probs > -np.Inf` can only be evaluated to be False when t=0? \r\nwell, when vocabulary is small then it could be evaluated to be False. I was trying to use your Beamsearch to the test phase of pointer network, and the vocabulary can't be determined when building the graph. and it usually is small.", "timestamp": "2018-01-05T14:15:39Z", "author": "MDQ6VXNlcjE3ODMwNDI3"}, {"message": "I think this can be true when t>0. I think it is generally ok since as long as vocab_size != 1, the available beam width will expand towards to the beam_width.\r\n\r\nI think we can be more careful about `finished`, for example, if all finished == False entries' log score is -neg_inf, we terminate the decoding process. This can be done in the `step()` method.  wdyt?\r\n\r\n  ", "timestamp": "2018-01-07T23:26:25Z", "author": "MDQ6VXNlcjQ2MDQ0NjQ="}, {"message": "@JerrikEph thoughts?", "timestamp": "2018-01-19T19:00:14Z", "author": "MDQ6VXNlcjE3OTQ3MTU="}, {"message": "This should be handled by init self._finished as initial log_probs.", "timestamp": "2018-01-19T20:35:24Z", "author": "MDQ6VXNlcjQ2MDQ0NjQ="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyMzU5NTg3OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/10771#discussion_r123595879", "comments": [{"message": "Where does the 2/3 come from?", "timestamp": "2017-06-22T19:00:50Z", "author": "MDQ6VXNlcjMzNzY4MTc="}, {"message": "that is a good question, in the original PR from @BrianOn99, used that value, I am not sure why ?", "timestamp": "2017-06-22T19:11:33Z", "author": "MDQ6VXNlcjQxMjA3OTY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNDE0NDk1MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/10771#discussion_r124144950", "comments": [{"message": "Did you address @andrewharp comments? It looks like this is still there.", "timestamp": "2017-06-26T23:14:55Z", "author": "MDQ6VXNlcjIwOTU5ODUz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyODA3NjMwMA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/10771#discussion_r128076300", "comments": [{"message": "Not sure I understand github's behavior here -- is this method actually removed? Why does it still show up here when viewing \"all commits\", but not when I try to edit/view the file by itself?\r\n\r\n@osdamv The log message should be indented 4 spaces here.\r\n\r\n@drpngx If I can see all the cumulative commits together in the \"files changed\" view, do you know why I can't directly edit the latest version of it in the branch here? \"View\" and \"edit\" show me a different file than the side-by-side view.\r\n", "timestamp": "2017-07-18T19:45:56Z", "author": "MDQ6VXNlcjMzNzY4MTc="}, {"message": "Done ", "timestamp": "2017-07-19T17:44:31Z", "author": "MDQ6VXNlcjQxMjA3OTY="}, {"message": "After your last commit I could see the changes in the edit view, strange!", "timestamp": "2017-07-19T19:17:03Z", "author": "MDQ6VXNlcjMzNzY4MTc="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNDA3MDk5OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/10818#discussion_r124070999", "comments": [{"message": "where do these numbers come from?", "timestamp": "2017-06-26T17:32:28Z", "author": "MDQ6VXNlcjIwOTU5ODUz"}, {"message": "From the authors' implementation at https://github.com/bioinf-jku/SNNs/blob/master/selu.py#L22", "timestamp": "2017-06-26T17:35:43Z", "author": "MDQ6VXNlcjc5NzYzMTU="}, {"message": "It's easier to debug the gradients setting one or both the magic numbers to 1 since [Elu](https://github.com/tensorflow/tensorflow/blob/86f5ab7474825da756838b34e1b4eac93f5fc68a/tensorflow/core/kernels/relu_op_functor.h#L104) is a special case of Selu.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/52dcb2590bb9274262656c958c105cb5e5cc1300/tensorflow/compiler/tf2xla/kernels/elu_op.cc", "timestamp": "2017-06-27T06:57:04Z", "author": "MDQ6VXNlcjkwMDQ1OTQ="}, {"message": "If I set both scale and alpha to 1 and the replace the tests for selu with those for elu, the test passes. ", "timestamp": "2017-06-30T17:55:03Z", "author": "MDQ6VXNlcjc5NzYzMTU="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNjA3NzYyNQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/11188#discussion_r126077625", "comments": [{"message": "Why have both `output` and `asOutput`? Seems like just the latter suffices?", "timestamp": "2017-07-07T06:31:02Z", "author": "MDQ6VXNlcjE2MDE4"}, {"message": "Like I mentioned in a previous comment, It is to represent what will be generated automatically. Even if an operation implements `Operand`, it exposes its output with a named method. But that test class might go away anyway...", "timestamp": "2017-07-07T22:28:04Z", "author": "MDQ6VXNlcjEwMTA5NTM0"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNjgzNTQyOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/11251#discussion_r126835429", "comments": [{"message": "Apologies, I'm blanking out - could you elaborate on why we need a `defaultScalar`?  (And why it's okay for some of the types to not have a default scalar)?", "timestamp": "2017-07-11T23:46:16Z", "author": "MDQ6VXNlcjE2MDE4"}, {"message": "Some ops need a zero value.", "timestamp": "2017-07-12T05:37:03Z", "author": "MDQ6VXNlcjQ2MjM4MTM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMjU2NzM0Nw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/11697#discussion_r132567347", "comments": [{"message": "does cpu changed as well?", "timestamp": "2017-08-10T21:04:20Z", "author": "MDQ6VXNlcjI4ODc4MDM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyOTQ3NjMyMA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/11749#discussion_r129476320", "comments": [{"message": "Can you clarify what scale 'iterations' is?  iterations is the number of epochs used before the first restart, correct? \r\n\r\nSo with default t_0 and t_mul, you restart after every epoch of training, correct?", "timestamp": "2017-07-26T03:45:05Z", "author": "MDQ6VXNlcjQ2MzczNw=="}, {"message": "We observe that all learning rate decay algorithms implemented in TensorFlow compute proper learning rate with respect to the global step.\r\nThis global step starts at 0 and is increased by one after each optimizer step (If we provide it as an argument when constructing optimizer).\r\nSo it would mean that t_0 is the duration of the first cycle measured as the number of minibatch updates (Iteration = minibatch update).\r\nIf one wants to use epochs, one should compute the number of iterations required for an epoch.\r\nExample:\r\nMinibatch size: 100\r\nTraining dataset size: 10 000\r\nUser wants the first decay cycle to span across 5 epochs: t_0 = 5 * 10000/100\r\n\r\nWe did it to be consistent with the other learning rate decay implementations available in TensorFlow. Original approach presented in the paper\r\nhowever defines t_0 as the number of epochs. In both cases here and in the original implementation it is important to notice that the learning rate is adjusted after each minibatch update, so there is no sudden drop in the learning rate at the end of epoch.\r\n\r\nWe might need to clarify it inside the description.", "timestamp": "2017-07-26T12:47:16Z", "author": "MDQ6VXNlcjIxMjIxMTIx"}, {"message": "Given that the implementation in the paper doesn't match the interpretation here, do you think we should consider renaming the variables to avoid confusion?  Something like\r\n\r\nt_0 would become 'initial_period_steps', so it's clear that it indicates something about 'periods', and also that it is measured in 'steps' to match global_step.  Similarly with the other variables, maybe, but I'm not sure.\r\n\r\nBasically, I'm worried that people will think there are bugs in the implementation because the variable names are the same but the 'types' are different :)", "timestamp": "2017-07-26T16:36:52Z", "author": "MDQ6VXNlcjQ2MzczNw=="}, {"message": "We changed the name of 't_0' to 'initial_period_steps' . We removed default value for it, it depends on the specific application and we think it's better if user provides it. We did not change names of other variables, those keep the same function as in the original implementation. ", "timestamp": "2017-07-31T17:27:44Z", "author": "MDQ6VXNlcjIxMjIxMTIx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyOTQ3Njg0NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/11749#discussion_r129476845", "comments": [{"message": "It currently defaults to 1000, is that a bug in the doc or the impl?", "timestamp": "2017-07-26T03:52:07Z", "author": "MDQ6VXNlcjQ2MzczNw=="}, {"message": "Bug in the doc. However it might be better to remove the default value for this argument. In the most common scenario user will want compute this value\r\nsuch that number of iterations is N * iterations_per_epoch.", "timestamp": "2017-07-26T12:48:48Z", "author": "MDQ6VXNlcjIxMjIxMTIx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMjAzODA4Nw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/11853#discussion_r132038087", "comments": [{"message": "What's going on here? It doesn't seem generally safe to do this, but I'm not familiar with this code...", "timestamp": "2017-08-08T21:30:18Z", "author": "MDQ6VXNlcjE5MjE0Mg=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMzM0NzU5Ng==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/12304#discussion_r133347596", "comments": [{"message": "@martinwicke How are you planning to sync changes from contrib and to core? (trying to track all the changes related to estimators)", "timestamp": "2017-08-16T02:35:29Z", "author": "MDQ6VXNlcjQyNjk4OTg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzNDA2NTIxMA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/12347#discussion_r134065210", "comments": [{"message": "Looks like it is not happy that it cannot find deviceQuery. https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/6417/console\r\nCan we add a check for that if we are not using shell=True?", "timestamp": "2017-08-18T22:12:05Z", "author": "MDQ6VXNlcjExOTIyNjU="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzOTc1MjQ2Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/12616#discussion_r139752463", "comments": [{"message": "Could you please explain the usage of IndexedSlices in this context?", "timestamp": "2017-09-19T16:53:29Z", "author": "MDQ6VXNlcjE5MjkzNjc3"}, {"message": "The contrib framework function exposes an internal function from `tensorflow.python.framework.ops`. Rather than do that, I rather directly call on the `ops.IndexedSlices` class from `tensorflow.python.framework.ops` which then calls its own internal function without jeopardizing protected access.", "timestamp": "2017-10-17T22:39:40Z", "author": "MDQ6VXNlcjE4NzM5OTQ="}, {"message": "It's a suprising usage of IndexedSlices. For example you're sending train_op as a dense_shape. I would revert this line. Otherwise LGTM", "timestamp": "2017-10-17T23:38:53Z", "author": "MDQ6VXNlcjE5MjkzNjc3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzOTYwMjM4MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/12645#discussion_r139602380", "comments": [{"message": "Is this the right behavior? Or would we expect the input function generated to return an empty labels dict?", "timestamp": "2017-09-19T05:55:29Z", "author": "MDQ6VXNlcjU3NzI3Nw=="}, {"message": "I have no idea. `y={}` seems a little weird. Do you prefer to returning a empty labels dict?", "timestamp": "2017-09-19T09:36:57Z", "author": "MDQ6VXNlcjExMTIyNjM="}, {"message": "By the way,  is `x = {}` or `y = {}` a valid argument?", "timestamp": "2017-09-19T09:46:05Z", "author": "MDQ6VXNlcjExMTIyNjM="}, {"message": "`x = {}` makes no sense (testing for it and raise an appropriate error would be nice), `y = {}` does make sense for unlabeled problems. As does `y=None`.\r\n\r\nHowever, I got confused because even though the docs don't mention it, this function sometimes returns only `features`, and sometimes it returns `(features, labels)`. \r\n\r\nI would find it somewhat odd if the return type changed based on the content of the dict (or non-content).", "timestamp": "2017-09-19T16:42:00Z", "author": "MDQ6VXNlcjU3NzI3Nw=="}, {"message": "I agree that we'd better return consistent result `(features, labels)`.  How about this:\r\n+ When x = {}, raise an ValueError\r\n+ When y = None, labels = None\r\n+ When y = {}, labels = {}", "timestamp": "2017-09-21T08:06:46Z", "author": "MDQ6VXNlcjExMTIyNjM="}, {"message": "That's perfect.", "timestamp": "2017-09-21T16:08:41Z", "author": "MDQ6VXNlcjU3NzI3Nw=="}, {"message": "Oh, I forget to remind you that the compatibility will be broken if we return `None` when `y=None`. At least, some code of user might be broken if behavior changed.", "timestamp": "2017-09-21T22:15:08Z", "author": "MDQ6VXNlcjExMTIyNjM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzNzgzNTc4OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/12658#discussion_r137835789", "comments": [{"message": "In this case, we'll end up copying something into target_file. Do those semantics make sense? Or should we delete target_file? What are the semantics in Unix for instance? Either way, can we document this in the header file?", "timestamp": "2017-09-08T16:38:27Z", "author": "MDQ6VXNlcjE0NDExNA=="}, {"message": "@rohan100jain That was to follow the old semantics (where no partial copy in memory). Now as we use streaming, it is not possible to rollback any way. The `cp` in Linux is not impacted by the file change so I just removed the check at the end.", "timestamp": "2017-09-16T02:23:43Z", "author": "MDQ6VXNlcjY5MzIzNDg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzNzkxMTMyMw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/12920#discussion_r137911323", "comments": [{"message": "Can we change this to a TODO when the bug is fixed?  (Here and similar places)\r\n\r\nHow will we know when the bug is fixed?  Not until CUDA 10?  Does this bug break all 2D shared arrays?\r\n\r\nAlso, I think you need need a space after the // or the linter will complain.", "timestamp": "2017-09-09T00:01:29Z", "author": "MDQ6VXNlcjI1MzMxNzQ="}, {"message": "Not all 2D shared arrays are affected. As far as I can tell, only those whose types rely on constexpr default constructors enabled by NVCC's -expt-relaxed-constexpr option.\r\n", "timestamp": "2017-09-09T00:41:23Z", "author": "MDQ6VXNlcjE4NzM2NTU="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzNzkxMTQxNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/12920#discussion_r137911414", "comments": [{"message": "Are these changes for a different compiler problem?  Are other ops affected, or only sum?", "timestamp": "2017-09-09T00:02:56Z", "author": "MDQ6VXNlcjI1MzMxNzQ="}, {"message": "These correct a separate issue also related to reductions with std::complex types. cub::Sum relies on an operator overload for operator+, this is not available (in device code) for std::complex. This results in a compiler warning about calling \"\\_\\_host\\_\\_\" functions from \"\\_\\_host\\_\\_ \\_\\_device\\_\\_\" functions, and incorrect code.", "timestamp": "2017-09-09T00:33:02Z", "author": "MDQ6VXNlcjE4NzM2NTU="}, {"message": "There are already workarounds for complex product and division; sum worked fine, but I guess is broken in nvcc 9?\r\n\r\nCan you either change this fix to be consistent with the existing work arounds, or change the existing work arounds to this style?", "timestamp": "2017-09-10T17:25:49Z", "author": "MDQ6VXNlcjI1MzMxNzQ="}, {"message": "I made the code consistent with the existing workarounds.\r\n\r\ncuda9 does have different behavior than cuda8. This may well be related to the same expt-relaxed-constexpr bug.", "timestamp": "2017-09-12T00:55:18Z", "author": "MDQ6VXNlcjE4NzM2NTU="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzODIwNDU2Nw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/12976#discussion_r138204567", "comments": [{"message": "Why this change?", "timestamp": "2017-09-11T22:04:40Z", "author": "MDQ6VXNlcjIwOTU5ODUz"}, {"message": "@drpngx The purpose for checking `f1 == f2` is for the case where `f1, f2 = inf, inf`. `assertNear` used `math.abs(f1-f2)` which will not cover `inf, inf` case. \r\n\r\nI have moved the `f1 == f2` checking to `assertNear` with comments added, so that it is easy to see the intention.", "timestamp": "2017-09-14T23:34:11Z", "author": "MDQ6VXNlcjY5MzIzNDg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDAwNDYxNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/13012#discussion_r150004614", "comments": [{"message": "This doesn't look like it's doing anything sync-replicas-related. Stale comment?", "timestamp": "2017-11-09T16:01:56Z", "author": "MDQ6VXNlcjUwNjE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzOTI0OTIyMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/13071#discussion_r139249221", "comments": [{"message": "You don't need dm2 right?", "timestamp": "2017-09-15T21:01:41Z", "author": "MDQ6VXNlcjg4ODA4"}, {"message": "Verified. Since m2 was also no longer needed, eliminated that and renamed variables.", "timestamp": "2017-09-16T23:55:28Z", "author": "MDQ6VXNlcjIwOTU5ODUz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MjQ0OTUwNg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/13239#discussion_r142449506", "comments": [{"message": "I understand the logic for the need of stop_propagation. I guess it is useful in order to compute receptive field information for specific paths of a network. (in other words, in a CNN there are multiple ways to go from input to output; stop_propagation supports measuring RF information for only part of the network).\r\nAre there some other cases where it is useful? Anyway, if you could add one or a couple more sentences to explain to the not-so-familiar reader when stop_propagation can be useful, it would be perfect.\r\n", "timestamp": "2017-10-03T16:15:35Z", "author": "MDQ6VXNlcjYzMTY0MTk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MTk4MjE5NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/13312#discussion_r141982194", "comments": [{"message": "i have to think about this.  it sounds like at each time step you're reordering the entire history.  is that right?", "timestamp": "2017-09-29T22:40:28Z", "author": "MDQ6VXNlcjE3OTQ3MTU="}, {"message": "The idea is to keep the beams in the correct order at each timestep.\r\n\r\nI guess it could also be done at the end of the decoding but the implementation here is more generic than just alignment history. The `TensorArray` might not contain time-based series.", "timestamp": "2017-10-02T08:08:22Z", "author": "MDQ6VXNlcjQ4MDU1MTM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NTEwMjUyMw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/13451#discussion_r155102523", "comments": [{"message": "What's the point of having a lambda and evaluate it right away?", "timestamp": "2017-12-05T23:03:18Z", "author": "MDQ6VXNlcjE1NzM2OTEw"}, {"message": "This allows the static is_enabled to be set inline.", "timestamp": "2017-12-11T22:08:34Z", "author": "MDQ6VXNlcjE4NzM2NTU="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzYwNjM0MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/14094#discussion_r153606341", "comments": [{"message": "What does a `type()` for a method mean? The return type?", "timestamp": "2017-11-28T19:58:52Z", "author": "MDQ6VXNlcjE2MDE4"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDA1OTYwOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/14098#discussion_r150059609", "comments": [{"message": "Not too familiar with `rdkafkacpp` but any reasons behind this hard-coded 1000? Is this a standard practice?", "timestamp": "2017-11-09T19:15:26Z", "author": "MDQ6VXNlcjQyNjk4OTg="}, {"message": "Thanks @terrytangyuan. The `1000` is the timeout (in `ms`) for consume() call to return. The call will repeat in the loop. The value was used as in the demo code of librdkafka.\r\n\r\nI have updated the PR so that an optional attribute `timeout` could be passed to control this value (instead of hard coded `1000`). Please take a look.\r\n", "timestamp": "2017-11-12T03:59:18Z", "author": "MDQ6VXNlcjY5MzIzNDg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzcxMzU4MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/14330#discussion_r153713581", "comments": [{"message": "Here has a performance problem:  For each mini-batch, how many memory allocations occurred at here?", "timestamp": "2017-11-29T07:58:59Z", "author": "MDQ6VXNlcjg1NjMxNg=="}, {"message": "+1. It would probably be more efficient to loop over `input_flat(i)` to identify the start and end positions for each label/feature, then use `StringPiece` instead of `string` as the argument to `ConvertHelper<T>()`.\r\n\r\n(It would also be fine to iterate on this in a future PR.)", "timestamp": "2017-11-29T15:09:03Z", "author": "MDQ6VXNlcjE5MjE0Mg=="}, {"message": "Thanks @mrry @snnn, I will have a follow up PR for that after this PR is merged.", "timestamp": "2017-11-29T17:56:41Z", "author": "MDQ6VXNlcjY5MzIzNDg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDcxMjgwOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/14854#discussion_r154712808", "comments": [{"message": "I see now that you're not the one introducing the idempotent issue, can you fix it here too?", "timestamp": "2017-12-04T17:12:30Z", "author": "MDQ6VXNlcjU3NzI3Nw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NzMyMTU0NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/14862#discussion_r157321545", "comments": [{"message": "Why do you need this control dependency? Isn't there a data dependency with grad already?", "timestamp": "2017-12-15T23:40:28Z", "author": "MDQ6VXNlcjE0NTA2MTQ="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDIzMjAzOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/14945#discussion_r154232038", "comments": [{"message": "well, what is the nature of this failure?\r\nWhat exactly are we checking for in this if block?", "timestamp": "2017-11-30T23:30:46Z", "author": "MDQ6VXNlcjc5NDY4MDk="}, {"message": "If sysctl cant resolve the path it returns a non zero value, but i can't find anything about a scenario where it would fail, and I cant see any of the other OS:s checking for a failure in their resolver so I just left it unhandled, should i set exe_path to an empty value if it fails perhaps?\r\n\r\nOr should i perhaps do a \r\n> return errors::Aborted(\"could not resolve path \", exe_path) ", "timestamp": "2017-12-01T09:22:00Z", "author": "MDQ6VXNlcjMwODE5ODE="}, {"message": "Having a failure like this looks more correct to me. Let's do that.", "timestamp": "2017-12-13T21:02:03Z", "author": "MDQ6VXNlcjc5NDY4MDk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1ODM2OTAzOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/14953#discussion_r158369038", "comments": [{"message": "Why the cost_per_unit estimation is a const? Is this the average CPU cycle for moving one element of type T? Would it be proportional to sizeof(T) then?", "timestamp": "2017-12-21T20:16:44Z", "author": "MDQ6VXNlcjEwMDI0MDU="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1ODM5Mjk1NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/14953#discussion_r158392955", "comments": [{"message": "Why there is a chance of getting negative values here?", "timestamp": "2017-12-21T22:28:46Z", "author": "MDQ6VXNlcjEwMDI0MDU="}, {"message": "shift_flat can have negative values if the user wants the elements shifted in the opposite direction.", "timestamp": "2017-12-23T03:45:33Z", "author": "MDQ6VXNlcjM1Mjc4Njg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MzcwMDkxMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/14953#discussion_r163700911", "comments": [{"message": "What is isd here?", "timestamp": "2018-01-24T22:38:08Z", "author": "MDQ6VXNlcjEwMDI0MDU="}, {"message": "I'll add some comments to clarify. This also stands for inner shift dimension. Basically I shift the isd and all outer dimensions. When I benchmark with a 2-D tensor if isd=0 then since dimension 1 is on the inside of dimension 0, dimension 1 will not be shifted and dimension 0 will. If isd=1 then both dimension 1 and 0 get shifted. This allows me to compare performance between just shifting the outer dimension and shifting all dimensions.", "timestamp": "2018-01-25T03:11:22Z", "author": "MDQ6VXNlcjM1Mjc4Njg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3ODA3MDU2Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/14974#discussion_r178070563", "comments": [{"message": "What happens if mechanisms is not a list but this is?", "timestamp": "2018-03-29T14:19:01Z", "author": "MDQ6VXNlcjE3OTQ3MTU="}, {"message": "`attention_mechanism` is converted to a `tuple` in the constructor when it is not a sequence. Same for this new argument.", "timestamp": "2018-03-29T14:46:01Z", "author": "MDQ6VXNlcjQ4MDU1MTM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NTY1OTYwNg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/15121#discussion_r155659606", "comments": [{"message": "So my understanding is that the main change here is you call nest.flatten and nest.pack_sequence_as around the user code in a py_func. Everything else is debugging / diagnostics, right?\r\n\r\nIf so, I think this can live in utility code instead of in the tensorflow library proper. Maybe you want to add a contrib.py_func with this API or add this py func wrapper in a third-party library somewhere?", "timestamp": "2017-12-07T22:29:03Z", "author": "MDQ6VXNlcjUwNjE="}, {"message": "You are right. The main changed are allow kwargs as input and arbitrary returns with shapes (earlier only list/scalar without shape).\r\nAnother (not so important) point is to allow shape/type inference from the input.(Useful for decorators)\r\nIf you prefer `contrib.py_func` I can move the code to contrib.\r\nIf you prefer a third-party library, I will keep the code in a private repo.\r\n \r\nWhen you need a use case inside of tf for this modification, look in `tf.data.Dataset.from_generator`.\r\nThe code there was the starting point.", "timestamp": "2017-12-07T23:02:18Z", "author": "MDQ6VXNlcjEzNzQ0MTI4"}, {"message": "I agree the dataset API for py_func is friendlier to use and I like exposing it. We could consider either tf.py_func_v2, or contrib.\r\n\r\n@martinwicke for API review as to whether we prefer _v2 or contrib", "timestamp": "2017-12-07T23:08:04Z", "author": "MDQ6VXNlcjUwNjE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Nzg2NTkzNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/15443#discussion_r157865934", "comments": [{"message": "Why do we need this change?", "timestamp": "2017-12-19T20:25:14Z", "author": "MDQ6VXNlcjcyNDQ5NDM="}, {"message": "The old version requires that `numerator` must be a scalar, and is incompatible with float64. The change makes it more general.", "timestamp": "2017-12-20T03:32:49Z", "author": "MDQ6VXNlcjExMTIyNjM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1ODM1NDkxMw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/15555#discussion_r158354913", "comments": [{"message": "Why hsv here?", "timestamp": "2017-12-21T19:07:58Z", "author": "MDQ6VXNlcjU3NzI3Nw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1OTExOTYyNQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/15714#discussion_r159119625", "comments": [{"message": "I didn't quite follow the change here. What is the \"optimization\" - is it lines of code or some performance argument being made here? ", "timestamp": "2017-12-30T09:09:03Z", "author": "MDQ6VXNlcjE2MDE4"}, {"message": "Thanks for reviewing my changes, \r\nJust have found multiple times if-statement was being used to exit function that's why decided to reduce the lines of code and making it with single statement to pass `empty return`. Just have followed the same single statement as it is done in `line no 322`. please let me know your thoughts on this.", "timestamp": "2017-12-30T13:58:23Z", "author": "MDQ6VXNlcjc1ODk1OTY="}, {"message": "I'd suggest we leave this as is. I know this is subjective, but I don't think the 4 lines saved improve readability of the logic (and it doesn't introduce questions like is it okay to use `it->second--` when there was no entry in `tensor_usage_` etc.\r\n", "timestamp": "2017-12-31T04:47:30Z", "author": "MDQ6VXNlcjE2MDE4"}, {"message": "Thanks! Changes are reverted as per your suggestion. Can we go ahead :)", "timestamp": "2018-01-01T05:18:27Z", "author": "MDQ6VXNlcjc1ODk1OTY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1OTIzNjUwOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/15780#discussion_r159236509", "comments": [{"message": "In this case, should you error out instead?", "timestamp": "2018-01-02T14:33:35Z", "author": "MDQ6VXNlcjE2ODI0NzAy"}, {"message": "Made changes to throw a `UserInputError` in this case. Thanks!", "timestamp": "2018-01-02T17:19:51Z", "author": "MDQ6VXNlcjQ5OTQ1OA=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MzEwNjYzOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/15967#discussion_r163106638", "comments": [{"message": "Can we make this work with both bazel, with having transform_graph as data dependency and pip environment?\r\nWe can have a condition in this script to check the file using resource_loader first, then use the aux_bin version?", "timestamp": "2018-01-22T23:54:08Z", "author": "MDQ6VXNlcjc5NDY4MDk="}, {"message": "It should already be available through bazel: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md\r\n", "timestamp": "2018-01-23T00:00:13Z", "author": "MDQ6VXNlcjExOTIyNjU="}, {"message": "No, I mean making the wrapper work through bazel.\r\nOr simply print an informational message if someone tries to run this through bazel.", "timestamp": "2018-01-23T00:19:57Z", "author": "MDQ6VXNlcjc5NDY4MDk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MzY4NzExMw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/16123#discussion_r163687113", "comments": [{"message": "Does this operation intentionally drop the last window if it's smaller than `slide_size`? Can you please add something about that to the documentation if so?", "timestamp": "2018-01-24T21:41:40Z", "author": "MDQ6VXNlcjE5MjE0Mg=="}, {"message": "And if so, please add optional padding like in `tf.contrib.signal.frame`!", "timestamp": "2018-01-25T22:15:35Z", "author": "MDQ6VXNlcjE1OTU5MDc="}, {"message": "Yes, it's intentional. Say, \r\n```python\r\ndataset = [1, 2, 3, 4, 5]\r\nslide_size=3\r\nslide_step=1\r\n```\r\nIn most case, I think we expect that:\r\n```\r\n[1, 2, 3]\r\n     [2, 3, 4]\r\n          [3, 4, 5]\r\n```\r\nSo I'd prefer to drop the last smaller window. I'd like to explain the behavior in the documentation later.\r\n\r\n@carlthome  Good idea, which behavior do you expect? Say, we choose 0 as padding value,\r\n+ finite sequences:\r\n```\r\n[1, 2, 3]\r\n     [2, 3, 4]\r\n          [3, 4, 5]\r\n               [4, 5, 0]\r\n                     [5, 0, 0]\r\n```\r\n+ or infinite sequences:\r\n```\r\n[1, 2, 3]\r\n     [2, 3, 4]\r\n          [3, 4, 5]\r\n               [4, 5, 0]\r\n                     [5, 0, 0]\r\n                           [0, 0, 0]\r\n                                .........\r\n```\r\n", "timestamp": "2018-01-25T23:32:27Z", "author": "MDQ6VXNlcjExMTIyNjM="}, {"message": "Because dataset supports composite structure, it seems not easy for padding, say:\r\n```\r\n[ [1, 2], [[3.0, 4.0, 5.0]], [[[1+2j]]], [\"hello\"] ]\r\n```", "timestamp": "2018-01-26T00:12:49Z", "author": "MDQ6VXNlcjExMTIyNjM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2Mjc5MDE0Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/16232#discussion_r162790143", "comments": [{"message": "Isn't DeleteFile of a file that doesn't exist supposed to return an error?", "timestamp": "2018-01-20T18:51:55Z", "author": "MDQ6VXNlcjIwOTU5ODUz"}, {"message": "Good question! [The API docs](https://github.com/tensorflow/tensorflow/blob/04b5c75aae4bdbdac7c713714a369f9b360daf70/tensorflow/core/platform/file_system.h#L150-L151) don't specify.\r\n\r\nThe posix filesystem looks like it will return ENOENT if the file doesn't exist, but the S3 filesystem currently treats this as success. I'd rather not change the S3 behavior, but I could future-proof this by removing the `TF_ASSERT_OK` - what do you think?", "timestamp": "2018-01-22T19:10:09Z", "author": "MDQ6VXNlcjE1NDgzOTI="}, {"message": "Actually, I think we should return an error if the file doesn't exist. That's the expectation, and if we have different behavior for filesystems, then it will lead to workarounds at the call site.", "timestamp": "2018-01-22T19:39:40Z", "author": "MDQ6VXNlcjIwOTU5ODUz"}, {"message": "I'm worried about making a breaking change to the API here; it also worries me that it's out-of-scope for the main fix.\r\n\r\nI can make it if you like, but it feels like it should be tracked separately.", "timestamp": "2018-01-22T19:48:55Z", "author": "MDQ6VXNlcjE1NDgzOTI="}, {"message": "Also, from what I can tell, S3 doesn't communicate the delete status back from a delete. We would need to add a check for the object existing prior to deleting (fine, but not ideal).", "timestamp": "2018-01-22T19:56:25Z", "author": "MDQ6VXNlcjE1NDgzOTI="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NzQ2NjQ0NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/16484#discussion_r167466444", "comments": [{"message": "why this change?", "timestamp": "2018-02-12T03:59:15Z", "author": "MDQ6VXNlcjIwOTU5ODUz"}, {"message": "Because many other cells , such as BasicLSTMCell \u3001 LSTMCell \u3001NASCell \u3001 LayerNormBasicLSTMCell , return h and stateTuple with c and h when call . I think sru should return the same type as lstm .\r\n ", "timestamp": "2018-02-12T05:44:27Z", "author": "MDQ6VXNlcjExMjcxNjIx"}, {"message": "So I think that should be split into another PR to keep things clean.", "timestamp": "2018-02-15T01:21:38Z", "author": "MDQ6VXNlcjIwOTU5ODUz"}, {"message": "OK\uff0cI will do that.", "timestamp": "2018-02-15T02:13:43Z", "author": "MDQ6VXNlcjExMjcxNjIx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NzQ2NjQ4Mg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/16484#discussion_r167466482", "comments": [{"message": "What's the motivation for this?", "timestamp": "2018-02-12T04:00:00Z", "author": "MDQ6VXNlcjIwOTU5ODUz"}, {"message": "Because _gather_states find out(i, j) = data(indices(i), i, j) when indices is a [batch_size] tensor . The origin method  reshape the 'data' to a 2-dim tensor and calculate the indices in the reshaped array . Use gather_nd can locate the 'indices' in the 3-dim tensor 'data' so no need for reshape . And in my experiment , gather may lead to a warning when data is a sparse tensor but use gather_nd with no warning . And use gather_nd can speed up a little .", "timestamp": "2018-02-12T05:53:37Z", "author": "MDQ6VXNlcjExMjcxNjIx"}, {"message": "OK, makes sense.", "timestamp": "2018-02-15T01:40:57Z", "author": "MDQ6VXNlcjIwOTU5ODUz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NjQ5ODQ0OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/16816#discussion_r166498449", "comments": [{"message": "do you need these changes?", "timestamp": "2018-02-07T01:45:49Z", "author": "MDQ6VXNlcjE3OTQ3MTU="}, {"message": "Oh yeah, I accidentally merged the that changed into the master branch of my copy. Removing that now.", "timestamp": "2018-02-07T02:19:58Z", "author": "MDQ6VXNlcjEzMjY4Njc1"}, {"message": "Reverted those changes. Test please.", "timestamp": "2018-02-07T02:26:25Z", "author": "MDQ6VXNlcjEzMjY4Njc1"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDQ4ODY3MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/16905#discussion_r170488670", "comments": [{"message": "Can we avoid using this?", "timestamp": "2018-02-26T03:43:02Z", "author": "MDQ6VXNlcjQxNzE3MDI="}, {"message": "Or I guess we have to add `#include <shlwapi.h>` ?", "timestamp": "2018-02-26T03:44:20Z", "author": "MDQ6VXNlcjQxNzE3MDI="}, {"message": "If you want to avoid this `pragma`, then we will have to add `shlwapi.lib` to linker flags. For Bazel it will be `-defaultlib:shlwapi.lib` or Bazel will think `shlwapi.lib` is a label.", "timestamp": "2018-02-26T07:29:32Z", "author": "MDQ6VXNlcjEzMTE1MDYw"}, {"message": "I understand, but don't we need `#include <shlwapi.h>` as well to use `SHGetValue()`", "timestamp": "2018-02-26T08:17:51Z", "author": "MDQ6VXNlcjQxNzE3MDI="}, {"message": "Sorry for missing your point. Yes, `shlwapi.h` needs to be included explicitly, I wrongly assumed that it is already included in `windows.h`. Done.", "timestamp": "2018-02-26T08:42:21Z", "author": "MDQ6VXNlcjEzMTE1MDYw"}, {"message": "Thank you!", "timestamp": "2018-02-26T08:45:54Z", "author": "MDQ6VXNlcjQxNzE3MDI="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MTM0NTMxNQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/17309#discussion_r171345315", "comments": [{"message": "Where is this macro used?", "timestamp": "2018-02-28T18:45:59Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "Removed", "timestamp": "2018-03-01T00:28:45Z", "author": "MDQ6VXNlcjEwNTM5NTQw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTgzMjQ1MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/17395#discussion_r175832450", "comments": [{"message": "Is this the only difference between adamax and adam? I don't think so, right?", "timestamp": "2018-03-20T16:23:39Z", "author": "MDQ6VXNlcjUwNjE="}, {"message": "You're right. Adamax is quite different from Adam. \r\n\r\nAdam:\r\n![image](https://user-images.githubusercontent.com/1112263/38004373-8d5e1fec-326e-11e8-9648-bebb5dab70d8.png)\r\n\r\nAdamax:\r\n![image](https://user-images.githubusercontent.com/1112263/38004389-a279a2b6-326e-11e8-9a34-f512c588aeee.png)", "timestamp": "2018-03-28T01:59:28Z", "author": "MDQ6VXNlcjExMTIyNjM="}, {"message": "So can you add this to the comment?", "timestamp": "2018-03-28T15:56:07Z", "author": "MDQ6VXNlcjUwNjE="}, {"message": "I think the formula of AdaMax has been added in its docstring.", "timestamp": "2018-03-28T22:42:48Z", "author": "MDQ6VXNlcjExMTIyNjM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MzAyNjUzOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/17524#discussion_r173026538", "comments": [{"message": "Where does the 400 come from? Can it be computed programmatically?", "timestamp": "2018-03-08T00:10:58Z", "author": "MDQ6VXNlcjMzNzY4MTc="}, {"message": "I am not sure how the textureView adjust its width in runtime.\r\nCan you point me where I can read the width?", "timestamp": "2018-03-08T20:20:44Z", "author": "MDQ6VXNlcjU0OTc5NzQ="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI3ODA1NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/17772#discussion_r175278055", "comments": [{"message": "I don't understand, why removing the fp16 logic?", "timestamp": "2018-03-18T05:48:52Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "This fp16 conversion is converting int value to fp16.\r\nWe should only convert fp32 to fp16 (we kept that one here). int values are used for attributes for supported ops in the converter.", "timestamp": "2018-03-18T07:45:44Z", "author": "MDQ6VXNlcjM3MDkyNDM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTg3ODQ3OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/17857#discussion_r175878478", "comments": [{"message": "How large is the data to transform? Does this hurt performance?", "timestamp": "2018-03-20T18:32:52Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "This is only done at conversion time. Would not have any impact on inference.", "timestamp": "2018-03-20T18:40:00Z", "author": "MDQ6VXNlcjM3MDkyNDM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MjI4NDIyOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/17943#discussion_r182284229", "comments": [{"message": "What is \"pd\"? write out.", "timestamp": "2018-04-18T01:08:55Z", "author": "MDQ6VXNlcjE2OTA3NTM0"}, {"message": "\"pd\" == primitive_desc\r\n\"md\" == memory_desc\r\n\r\nThere are terms specifically used for integrating MKL-DNN with Tensorflow framework. ", "timestamp": "2018-04-18T16:30:16Z", "author": "MDQ6VXNlcjI5MjE1MTk1"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3ODIxNzY1MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/18077#discussion_r178217651", "comments": [{"message": "What are these nodes and what are they doing in the graph ? ", "timestamp": "2018-03-30T01:27:31Z", "author": "MDQ6VXNlcjY5Njk2ODY="}, {"message": "These are nodes introduced during MKL-specific graph rewrite pass. Since TF does not expect the graph to be modified by non-standard graph rewrite passes, some unit tests fail.", "timestamp": "2018-04-06T17:12:08Z", "author": "MDQ6VXNlcjI4MTEzMjQx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MTUxMTkyMA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/18433#discussion_r181511920", "comments": [{"message": "Why not destroying?", "timestamp": "2018-04-13T21:15:33Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MjQ4NDA0Mg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/18433#discussion_r182484042", "comments": [{"message": "Where is this defined?", "timestamp": "2018-04-18T16:09:20Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NzI1Nzk5MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/18628#discussion_r187257990", "comments": [{"message": "Oh, not here. This should be in the call to `decode_png`, right?\r\n", "timestamp": "2018-05-10T07:19:29Z", "author": "MDQ6VXNlcjE2MDE4"}, {"message": "In function tf.cond(pred, fn1, fn2), fn1 and fn2 must have the same type,\r\nother formats decode function also need to returned uint16 when dtype != uint8.", "timestamp": "2018-05-10T08:51:07Z", "author": "MDQ6VXNlcjIzNTY3MzYy"}, {"message": "Perhaps I should add convert_image_dtype in decode_{jpeg, png, gif, bmp}, to make sure that returned tensors have same type.\r\n\r\nSomething like:\r\n```python\r\ndef _png():\r\n  ...\r\n  return convert_image_dtype(gen_image_ops.decode_png(contents, channels,\r\n    dtype=dtypes.uint8 if dtype==dtypes.uint8 else dtypes.uint16), dtype)\r\n\r\ndef _jpeg():\r\n  ...\r\n  return convert_image_dtype(gen_image_ops.decode_jpeg(contents, channels), dtype)\r\n```", "timestamp": "2018-05-10T09:31:57Z", "author": "MDQ6VXNlcjIzNTY3MzYy"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MjkwNTk3MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/18707#discussion_r182905971", "comments": [{"message": "IIUC, the interpretation of a `Dataset` argument is a bit different from what's written here. Is it the case that each element of (the `Dataset` version of) `weights` is a vector of `len(datasets)` and the ith element determines the weights for sampling the ith output element?\r\n\r\n(If so, can you please update the comment to clarify?)", "timestamp": "2018-04-19T22:47:52Z", "author": "MDQ6VXNlcjE5MjE0Mg=="}, {"message": "@mrry Yes you are right. I updated the docstring to (hopefully) clarify. ", "timestamp": "2018-04-19T23:51:49Z", "author": "MDQ6VXNlcjYwMjA5ODg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NTM0NjU0OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/18730#discussion_r185346548", "comments": [{"message": "(Not sure if you're still debugging this, but...) Can you please remove these `logging.warn()` calls before submitting?", "timestamp": "2018-05-01T22:02:49Z", "author": "MDQ6VXNlcjE5MjE0Mg=="}, {"message": "I am still debugging...I've just verified that the types and shapes of the datasets to `DirectedInterleaveDataset` are now the same between Windows and Linux, but I'm still getting the `kernel not registered` error. Any other suggestions for what might be causing this?", "timestamp": "2018-05-01T22:15:08Z", "author": "MDQ6VXNlcjYwMjA5ODg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NTA3NTczNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/18909#discussion_r185075734", "comments": [{"message": "This is duplicate, remove?", "timestamp": "2018-04-30T18:53:02Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MzU2NDQ2OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/19712#discussion_r193564469", "comments": [{"message": "What happens if this returns `false`? Can you add add something to report an error back to the user?", "timestamp": "2018-06-06T21:26:13Z", "author": "MDQ6VXNlcjE5MjE0Mg=="}, {"message": "@mrry I updated the PR. Now unsuccessful `Load()` will generate a warning, and the following operations will be skipped. This is similar to:\r\nhttps://github.com/aws/aws-sdk-cpp/blob/970a729a7d3541b2fd7948f413992e8a851ec329/aws-cpp-sdk-core/source/config/AWSProfileConfigLoader.cpp", "timestamp": "2018-06-13T21:00:44Z", "author": "MDQ6VXNlcjY5MzIzNDg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NDU0MjUzNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194542534", "comments": [{"message": "Doesn't trt always depend on cuda and cudnn?", "timestamp": "2018-06-11T20:47:20Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "There is a possibility of static linking TRT4.0 with cudnn. This new method doesn't assume it is always linked.", "timestamp": "2018-06-11T23:17:45Z", "author": "MDQ6VXNlcjEwNTM5NTQw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NDU2NDg1Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194564853", "comments": [{"message": "Are these for debugging purposes only?", "timestamp": "2018-06-11T22:17:01Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "Updates to grappler requires num_cores and frequency to be set since they added a check that FATALs if flops calculated using these values is not greater than 0.", "timestamp": "2018-06-12T00:42:29Z", "author": "MDQ6VXNlcjEwNTM5NTQw"}, {"message": "Would you please add a comment to explain these numbers then? Also is it about a DCHECK (I'm curious as I seem to meet the same error before)?", "timestamp": "2018-06-12T13:02:47Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "Yes FATALs are due to DCHECKs added to grappler. I added some comments", "timestamp": "2018-06-12T18:29:14Z", "author": "MDQ6VXNlcjEwNTM5NTQw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMjcwMzU4Mg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/20029#discussion_r202703582", "comments": [{"message": "I'm not familiar with this code -- why do we need to handle PNG_COLOR_TYPE_PALETTE specially? Is it because we convert palette to rgb below?", "timestamp": "2018-07-16T14:29:38Z", "author": "MDQ6VXNlcjg4ODA4"}, {"message": "Certainly, you convert palette to rgb bellow.\r\nBut grayscale conversion run after that, because channels is 1 (args channels=0)\r\n\r\nThis issue depends on meaning of:\r\n \"the number of channels in the PNG-encoded image.\"\r\nhttps://www.tensorflow.org/api_docs/python/tf/image/decode_png\r\n\r\npng_get_channels function return 1 if colortype is PALETTE.\r\nhttp://www.libpng.org/pub/png/libpng-manual.html#section-3.6\r\nPalette has indices channels so channels 1 only.\r\n\r\nI think that the current decode_png behavior is unkind.", "timestamp": "2018-07-16T18:12:18Z", "author": "MDQ6VXNlcjI2MDQw"}, {"message": "Ah I see, thank you for the explanation. I hadn't looked at the documentation before, but now it makes sense.", "timestamp": "2018-07-16T20:20:15Z", "author": "MDQ6VXNlcjg4ODA4"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMzkxMTE1Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/20611#discussion_r203911153", "comments": [{"message": "Can you change / add to this such that the gradient works for dynamic shapes?", "timestamp": "2018-07-20T00:29:11Z", "author": "MDQ6VXNlcjE2OTA3NTM0"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNTIxMjMyNg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/21092#discussion_r205212326", "comments": [{"message": "I'm sure I'm missing something, but why is the change from `long[]` to `Shape` needed?\r\n`Shape` can represent partially known shapes (unknown rank, or some dimensions unknown). In the API so far we've been using `Shape` only in places where it is okay to have a partially known shape, while `long[]` where the shape is fully known (the assumption being that all elements of the array are >= 0).\r\n\r\nIn these `create()` functions, we want fully known shapes, right?", "timestamp": "2018-07-25T18:15:33Z", "author": "MDQ6VXNlcjE2MDE4"}, {"message": "I find `Shape` more useful to expose utility methods such as `byteSize`. \r\n\r\nSince it is more flexible for future usage, I think it would make sense to standardize its usage everywhere (at least in the building API), I have a personal preference for that. And auto-generated operators actually accepts shape in argument as `Shape`s. Wdyt?", "timestamp": "2018-07-27T22:26:32Z", "author": "MDQ6VXNlcjEwMTA5NTM0"}, {"message": "Didn't quite follow the comment about `Shape.byteSize()`, but I'm guessing you mean utility methods like `numElements()`?\r\n\r\nAuto generated operators accept `Shape` as arguments because the operations work with partially known shapes. `Constant` objects require a fully known shape, no?\r\n\r\nI appreciate the interest in standardizing, but I'm not sold on it here just yet. Perhaps some sample code that we'd expect users to write where providing a `Shape` object would be better would make this clearer? Naively, I'm thinking that the type mismatch will make the developer think about the difference, vs. providing a `Shape` object and then observing a runtime error if `Shape` isn't fully specified.", "timestamp": "2018-07-29T22:09:07Z", "author": "MDQ6VXNlcjE2MDE4"}, {"message": "Yes, I meant `numElements()` here, sorry for the confusion. \r\n\r\nWhat I tried to explain is that manipulating primitive arrays as we do here can be restrictive when we want to extend the functionalities of the API. It could also lead to confusion that there is two way to represent the same thing and conversion from one to another can be required (I'll try to provide you an example of that).\r\n\r\nHaving a `Shape` object make sense to me as it is ubiquitous in TF domain. Just throwing ideas here : would it make sense to have a `FixedShape` class that extends from `Shape` to enforce that a shape must be known for some operations?", "timestamp": "2018-07-30T13:18:26Z", "author": "MDQ6VXNlcjEwMTA5NTM0"}, {"message": "To make sure I understand your proposal, you're suggesting that we use `Shape` instead of `long[]` everywhere (e.g., `Tensor.shape()` should return a `Shape`, and the factory functions should accept `Shape` instead of `long[]`), right?\r\n\r\nYou mention that the current separate \"could lead to confusion since there are two ways to represent the same thing\" - but, at least the way I was thinking about it, we represent them differently because they are *different* things :). The shape of a `Tensor` is concrete like the `.length`s of an array. While a `Shape` object is used to represent a pattern - and these patterns are consumed in specific contexts (like attributes of an operation). Hence, you can easily convert one way (`Shape.make()`), but not the other. \r\n\r\nNot a perfect analogy, but something along the lines of a `String` vs. `java.util.regex.Pattern`, or an `int` vs. a `BigInteger`. \r\n\r\nThat said, perhaps I'm missing something. Some concrete examples of where this distinction is really hurtful will help. Or perhaps you can share some ideas on what kinds of flexibility are you thinking of for future usage?", "timestamp": "2018-07-30T19:08:39Z", "author": "MDQ6VXNlcjE2MDE4"}, {"message": "No, I think it's a good idea to keep enforcing only known shapes for some operations, such as `Tensor` or `Constant` creation. My last proposal was to replace the less-flexible primitive arrays by another concrete type (`FixedShape` or something similar) so we can easily retrieve properties of that shape. \r\n\r\nFor example, to retrieve the number of elements in a tensors, it is easier to do `tensor.shape().numElements()` than doing the computation manually from a `long[]` like the `Tensor` class is doing right now. Of course, we could have those utilities exposed as a static method of some final object, for instance `Shapes.numElements(tensor.shape())`, but I always had a personal preference for more object-oriented programming.\r\n\r\nJust to give a quick example out-of-my-head-right-now (sorry if it's a bad one...) but someone who would like to initialize his NxM tensor of floats to `1.0f` would benefit from this utility as well instead of iterating through shape dimensions and sizes himself.\r\n\r\nAlso, I was not aware at first that a `long[]` shape means that it can only be concrete, maybe having another type would also make that concept more obvious to the user?  ", "timestamp": "2018-07-30T21:58:32Z", "author": "MDQ6VXNlcjEwMTA5NTM0"}, {"message": "I see. Adding a separate class and type for something this ... \"primitive\" ... seems a bit much to me. What would the API of this class be - would it provide both a constructor and a getter for the underlying array? I don't think it would be worthwhile to add something like the following, unless there is some more sophisticated functionality you have in mine:\r\n\r\n```java\r\npublic final class FixedShape {\r\n  public FixedShape(long[] shape) { this.shape = shape; }\r\n  public long[] shape() { return this.shape; }\r\n  public long numElements() {\r\n    long ret = 1;\r\n    for (int i = 0; i < this.shape.length; ++i) {\r\n      ret *= this.shape[i];\r\n    }\r\n    return ret;\r\n  }\r\n  private final long[] shape;\r\n}\r\n```\r\n\r\nFor `numElements()`, adding a `Tensor.numElements()` seems fine, if we see that this will be a commonly used/useful method. Or `Shape.numElements(long[])` as you suggested.", "timestamp": "2018-07-30T22:22:16Z", "author": "MDQ6VXNlcjE2MDE4"}, {"message": "Close yes, I had in mind something like that, hiding completely the arrays:\r\n```\r\npublic final class KnownShape implements Shape {\r\n    public static KnownShape make(long...) { ... }\r\n    ...\r\n    public long numElements() { ... }\r\n    ...\r\n}\r\n\r\npublic final class Tensor {\r\n    ...\r\n    public KnownShape shape() { ... }\r\n    ...\r\n}\r\n\r\npublic final class Zeros<T> implements Op, Operand<T> {\r\n   ...\r\n   public static <T> Zeros<T> create(Scope scope, Class<T> type, KnownShape shape) { ... }\r\n   ...\r\n}\r\n```\r\nand so on. I just find it more clear that we are dealing with the \"known\" constraint of the shape and easier to provide utilities around shapes in general.\r\n\r\nNot arguing here by the way, just find the discussion interesting :)", "timestamp": "2018-07-31T01:04:30Z", "author": "MDQ6VXNlcjEwMTA5NTM0"}, {"message": "The arrays (or at least the values of individual dimensions) will be useful since they have meaning (e..g, batch size, number of channels in an image, sequence length etc.). So, if we hide that, we'll want to add accessors for each dimension.\r\n\r\nI guess there are some utility functions (like `numElements`, or perhaps `rank`, or `equals` overrides or something) that may be nice to have. However, I feel that at this stage having a class for this is overkill. Let's see some more usage / pain with `long[]` before we go that route", "timestamp": "2018-07-31T01:37:25Z", "author": "MDQ6VXNlcjE2MDE4"}, {"message": ">>> The arrays (or at least the values of individual dimensions) will be useful since they have meaning\r\n\r\nYes of course, `shape.size(int)` would still be present\r\n\r\n>>> Let's see some more usage / pain with long[] before we go that route\r\n\r\nOk, I'll update the PR consequently, thanks", "timestamp": "2018-07-31T02:17:13Z", "author": "MDQ6VXNlcjEwMTA5NTM0"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNDkxOTY3OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/21100#discussion_r204919678", "comments": [{"message": "Could this ever fail?", "timestamp": "2018-07-24T21:36:59Z", "author": "MDQ6VXNlcjEwNTM5NTQw"}, {"message": "I'm not 100% sure. If it will fail, it must be also fail in native execution unless there is a bug in the conversion. In case of bugs we'd better retry.", "timestamp": "2018-07-24T22:15:29Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "In case of the errors in conversion, networks should be unusable. There is no point in trying and failing all the time. Effectively this will slow down the execution of the network.", "timestamp": "2018-07-24T22:22:55Z", "author": "MDQ6VXNlcjEwNTM5NTQw"}, {"message": "What do you think if I add a class-wide variable saying that the network is broken and retry at the very beginning of CompuAsync()?", "timestamp": "2018-07-24T22:28:11Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTMzNDA0MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/21286#discussion_r209334041", "comments": [{"message": "Can you undo this change? I was under the impression this CL was doc updates only.", "timestamp": "2018-08-10T17:37:38Z", "author": "MDQ6VXNlcjEyOTk2MzY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTY3NjAyNQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/21486#discussion_r209676025", "comments": [{"message": "Why changing the default value here?", "timestamp": "2018-08-13T16:31:49Z", "author": "MDQ6VXNlcjUwNjE="}, {"message": "1. in async mode, rho is not used in original paper\r\n2. in my experiments, rho=0 works well\r\n    when rho=None, default value: self._rho = self._moving_rate / self._opt._learning_rate, may result a large value.  e..g, moving_rate=self.BETA / communication_period / num_worker=0.9/8/4=0.028125, while the initial learning_rate might be 1e-4, which makes rho=281.25, and this makes the workers not able to do perform exploitation.", "timestamp": "2018-08-13T18:37:26Z", "author": "MDQ6VXNlcjQyMTU2NTY0"}, {"message": "Regardless we shouldn't change default values like this as this affects reproducibility of existing work which uses this code.", "timestamp": "2018-08-13T19:48:56Z", "author": "MDQ6VXNlcjUwNjE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMTEyOTAyOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/21508#discussion_r211129029", "comments": [{"message": "The max is hard coded to 100.\r\nHow would this work in a virtual environment (e.g. with kubernetes) where a single GPU can be shared by possibly more than 100 users?", "timestamp": "2018-08-20T02:31:34Z", "author": "MDQ6VXNlcjcwNzY5NjE="}, {"message": "That's a good point. Currently TfGpuId is always [starting from 0](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L998), so if there are any gpu device initialized before, gpu 0 should always be available. Note that TfGpuId is a virtual identifier of the gpu device owned by the process, not the physical gpu id. But if we hard coded 0 here, changes to BaseGpuDevice initialization flow can break the integration, so I added the loop here to reduce that risk. ", "timestamp": "2018-08-20T22:58:28Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "Got it. Looks okay.", "timestamp": "2018-08-20T23:28:56Z", "author": "MDQ6VXNlcjcwNzY5NjE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjQzMDMzMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/21658#discussion_r212430331", "comments": [{"message": "The LeakyRelu gradient does seem to use the inputs? Same above. ", "timestamp": "2018-08-23T19:31:30Z", "author": "MDQ6VXNlcjEyMjkxMQ=="}, {"message": "My fault, 'LeakyRelu' and 'LeakyReluGrad' both use the input and don't use the output, corrected in the new commit.", "timestamp": "2018-08-24T04:04:55Z", "author": "MDQ6VXNlcjEwNjY5MTEx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjQwMDg4MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/21803#discussion_r212400880", "comments": [{"message": "bt stood for boosted_trees\r\ntensor_forest? ", "timestamp": "2018-08-23T17:53:21Z", "author": "MDQ6VXNlcjQzMTMxMDk="}, {"message": "yes yes... sorry i was actually confused about the bt part", "timestamp": "2018-08-23T19:10:02Z", "author": "MDQ6VXNlcjc0MTU0NA=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjQwMTY1NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/21803#discussion_r212401654", "comments": [{"message": "i think u can have it over the classification probs outputs no?", "timestamp": "2018-08-23T17:55:37Z", "author": "MDQ6VXNlcjQzMTMxMDk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjQwNzk2NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/21803#discussion_r212407965", "comments": [{"message": "how is it going to be used (pathes?)", "timestamp": "2018-08-23T18:15:15Z", "author": "MDQ6VXNlcjQzMTMxMDk="}, {"message": "right now the model default would output tree paths.  \r\nand this function is going to be used in two kernels `TreePrediction` and `TreeTraverse` \r\nDuring `TreePrediction`, we need to output tree path.\r\n\r\nDuring `TreeTraverse` which is used for training the tree. we only need the leaf id.", "timestamp": "2018-08-23T18:56:47Z", "author": "MDQ6VXNlcjc0MTU0NA=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjc0NDgyNw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/21859#discussion_r212744827", "comments": [{"message": "Why do you need this?", "timestamp": "2018-08-24T20:20:19Z", "author": "MDQ6VXNlcjUwNjE="}, {"message": "this is to move the input optimizer's internal variables to LOCAL_VARIABLES collection.\r\nwhen these variables, e.g., momentum/adam, are in GLOBAL_VARIABLES collection, non-chief workers fail to initialize them, and I will see warning like following, and non-chief workers just got stuck.\r\n\r\n`\r\n[2018-08-25 11:04:45.353021] [INFO] [MainThread] [...on_manager.py:423] Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: NmtModel/beta1_power, NmtModel/beta2_power, NmtModel/NmtModel/Source_Side/WordEmbedding/C/Adam......\r\n`\r\n\r\ntried following approaches:\r\n1. in hook, set `self._global_init_op = variables.global_variables_initializer()` for all workers, NOT work\r\n2. in hook, add the them to var_list and set `self._local_init_op = variables.variables_initializer(var_list)`, NOT work\r\n3. in get_init_op(xx), explicitly add a `op=variables.variables_initializer(self._incremental)` into the `init_ops` array, NOT work\r\n\r\nverified that these variables are no worker side\r\n```\r\n/job:worker/task:1/device:GPU:0 NmtModel/beta1_power\r\n/job:worker/task:1/device:GPU:0 NmtModel/beta2_power\r\n/job:worker/task:1/device:GPU:0 NmtModel/NmtModel/Source_Side/WordEmbedding/C/Adam\r\n```\r\nIt feels like GLOBAL_VARIABLES can't be initialized on non-chief worker. ", "timestamp": "2018-08-25T03:47:32Z", "author": "MDQ6VXNlcjQyMTU2NTY0"}, {"message": "Can't you add these variables to those collections when those variables are created instead of retrofitting them later?", "timestamp": "2018-08-27T15:26:51Z", "author": "MDQ6VXNlcjUwNjE="}, {"message": "these variables are created by optimziers.\r\nI tried it with custom getter, to put the global variables into local, like following, and ask getter to put them into local.\r\n```\r\n    with variable_scope.variable_scope('', self._getter):\r\n      local_update_op = self._opt.apply_gradients(grads_and_vars)\r\n\r\n````\r\nhowever, beta1_power,beta2_power are not created by get_variable(), but by variable_scope.variable(), in which custom_getter doesn't take affect.\r\n", "timestamp": "2018-08-27T18:58:51Z", "author": "MDQ6VXNlcjQyMTU2NTY0"}, {"message": "You can hook over all variable creation using variable_creator_scope: https://github.com/tensorflow/tensorflow/blob/229652512f745b9ef110c02deb9dce98fd33b5be/tensorflow/python/ops/variable_scope.py#L2441", "timestamp": "2018-08-27T20:24:05Z", "author": "MDQ6VXNlcjUwNjE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNjY5MzYzMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/22210#discussion_r216693632", "comments": [{"message": "Is endianness a concern here (and for the subsequent methods)? Some users run TF on non-x86 platforms.", "timestamp": "2018-09-11T14:37:02Z", "author": "MDQ6VXNlcjE5MjE0Mg=="}, {"message": "Yep, good concern. I added today so called `ByteSwapper` that fixes endianness when it's appropriate. Please, take a look.", "timestamp": "2018-09-12T17:58:03Z", "author": "MDQ6VXNlcjEwMjg5Njk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxOTk3MTg2Ng==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/22429#discussion_r219971866", "comments": [{"message": "What is this testing that `testPerformance` is not? The fact that shuffling work correctly? I don't see how that's related to the logic introduced in this CL.", "timestamp": "2018-09-24T20:09:23Z", "author": "MDQ6VXNlcjEwNzIwNzk="}, {"message": "This is not related to the logic introduced in this CL. Have removed this testing.", "timestamp": "2018-09-26T14:53:51Z", "author": "MDQ6VXNlcjUwNTc3NDA="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMTU4MjgyMA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/22788#discussion_r231582820", "comments": [{"message": "Why do we need this? Is it used anywhere in this PR?\r\n\r\nAlso I believe the conversion for int8 above in line 96 is a bug, DT_INT8 is not quantize type, DT_QINT8 is. I need to verify, but just FYI.", "timestamp": "2018-11-07T16:41:33Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "Looks like this no longer needed now that QuantizeV2 is removed. I will delete it.\r\n\r\nOh I see, are DT_INT8 and DT_QINT8 fundamentally the same but with different names?", "timestamp": "2018-11-08T00:25:59Z", "author": "MDQ6VXNlcjEyOTgxNDc0"}, {"message": "No they're not the same:\r\n\r\n- DT_INT8 is not quantize type, it doesn't require min/max range information for computation (similar to DT_INT32, DT_FLOAT32, etc)\r\n- DT_QINT8 is quantize type and we do need range information for any computation. I think this is equivalent to TRT's INT8, since TRT's INT8 needs either calibration to populate the ranges or use setDynamicRange(). This is why I think converting DT_INT8 to TRT's INT8 may be wrong.", "timestamp": "2018-11-08T23:13:36Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNTY4MTU0Ng==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/22984#discussion_r225681546", "comments": [{"message": "What if there are multiple GTEs with the same index?", "timestamp": "2018-10-16T19:40:43Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "Changed it to a InlinedVecor", "timestamp": "2018-10-22T08:59:43Z", "author": "MDQ6VXNlcjY2NDI2NjM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyODMzMTQ4NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/23152#discussion_r228331485", "comments": [{"message": "(Not related to this PR)\r\nUnder what circumstances? Is it an expected behavior? If not, please file a bug and we'll look into it.", "timestamp": "2018-10-25T20:55:58Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "@penpornk according to my observation, in keras-based models, all \"transpose\" ops' int_val/int64_val/float_val... are not set entirely (all zero), so I have to parse the raw tensor data from \"tensor_context\" here myself. This is what I called \"some circumstance\" above. I don't think it's a desired behavior, and I will fire a bug to discuss about it.", "timestamp": "2018-10-26T05:44:57Z", "author": "MDQ6VXNlcjMzNjExMzI2"}, {"message": "Got it. Thank you very much!", "timestamp": "2018-10-30T17:07:04Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyODM2NjkyOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/23152#discussion_r228366929", "comments": [{"message": "What about control edges?", "timestamp": "2018-10-25T23:25:46Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "@penpornk I did a check in \"CheckForTranspose()\" to make sure transpose ops have no control edges, so this won't be a problem.", "timestamp": "2018-10-26T06:28:25Z", "author": "MDQ6VXNlcjMzNjExMzI2"}, {"message": "True. Please add a comment saying so in the code.", "timestamp": "2018-10-30T17:19:15Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyODM2NzEyMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/23152#discussion_r228367122", "comments": [{"message": "Why is this `WithBias`?", "timestamp": "2018-10-25T23:26:49Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "> Why is this `WithBias`?\r\n\r\nThat's a type, fixed.", "timestamp": "2018-10-26T06:43:33Z", "author": "MDQ6VXNlcjMzNjExMzI2"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMTY2MDQ1Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/23152#discussion_r231660453", "comments": [{"message": "Why is this commented?", "timestamp": "2018-11-07T20:12:08Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMTY2MDU0OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/23152#discussion_r231660548", "comments": [{"message": "Why is this commented?", "timestamp": "2018-11-07T20:12:24Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyOTUzMTU4MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/23388#discussion_r229531580", "comments": [{"message": "Instead of doing this, can you edit https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/traceable_stack.py#L64 to return FAILURE if negative_offset is out of range? I think that's a more general solution, and I don't think the stack frame will be useful from R anyway.", "timestamp": "2018-10-31T00:25:32Z", "author": "MDQ6VXNlcjg4ODA4"}, {"message": "@skye For sure, updated. Looking good?", "timestamp": "2018-10-31T01:12:45Z", "author": "MDQ6VXNlcjM0Nzg4NDc="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMjQyMDczMw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/23567#discussion_r232420733", "comments": [{"message": "why batch_dim unknown?", "timestamp": "2018-11-09T23:16:03Z", "author": "MDQ6VXNlcjQwMzY1Mzgy"}, {"message": "Updated", "timestamp": "2018-11-12T22:35:41Z", "author": "MDQ6VXNlcjI2NTc0ODk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzNzI5ODIwNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/24021#discussion_r237298204", "comments": [{"message": "Is this the only place for the bug fix? Just wanted to make sure.", "timestamp": "2018-11-28T23:10:14Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzNzk5NzY2OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/24051#discussion_r237997669", "comments": [{"message": "with variant, do you mean argument?\r\nvariant may be confusing in tensorflow context", "timestamp": "2018-11-30T20:47:46Z", "author": "MDQ6VXNlcjc5NDY4MDk="}, {"message": "That's actually a remnant from the older spec, so I deleted it.", "timestamp": "2018-11-30T20:55:06Z", "author": "MDQ6VXNlcjMyNDY1NDcy"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI1MTY3NzUzNg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/24245#discussion_r251677536", "comments": [{"message": "you don't need any of these older changes now, afaik. is that right? ", "timestamp": "2019-01-29T03:07:01Z", "author": "MDQ6VXNlcjE0MTA0ODU1"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NTgyNDY4NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/24327#discussion_r245824684", "comments": [{"message": "Why removing this?", "timestamp": "2019-01-07T22:46:47Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "L44: Since LRUCache is now created by LookupOrCreate ``` auto status = res_mgr->LookupOrCreate(\r\n      funcdef_name_, \"EngineCache\", &cache_res,\r\n      {[this](TRTEngineCacheResource** cr) -> tensorflow::Status {\r\n        *cr = new TRTEngineCacheResource(this->max_cached_engines_);\r\n        return Status::OK();\r\n      }});```\r\n\r\nL45-46: fixed_input_size is an unused attribute so I am removing it", "timestamp": "2019-01-07T23:13:48Z", "author": "MDQ6VXNlcjEyOTgxNDc0"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0Nzc4MDc3Nw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/24588#discussion_r247780777", "comments": [{"message": "How do you check if the data is actually cached and being reused?", "timestamp": "2019-01-15T07:03:00Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "@penpornk Since the persistent tensor is being held within the kernel, I don't know of a straightforward way (such as an API call) to retrieve it here. \r\n\r\nWe have performance tests that run internally to detect if the filter is being cached or not - if it isn't cached, then the performance would drop.", "timestamp": "2019-01-18T22:33:07Z", "author": "MDQ6VXNlcjI4MTEzMjQx"}, {"message": "Got it. Could you please add a comment (in the testcase) explaining that this is tested with performance tests?", "timestamp": "2019-01-19T02:02:28Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDkxNjg4Ng==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/24674#discussion_r244916886", "comments": [{"message": "what do you think about extracting out the graph building part as well to avoid duplication in the tests?\r\n\r\nSomewhat related example:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/conv2d_benchmark.py#L44", "timestamp": "2019-01-03T06:04:02Z", "author": "MDQ6VXNlcjE5OTAwNzk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NTM3MTA1MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/24675#discussion_r245371051", "comments": [{"message": "@aaroey do you know the difference between requested devices and assigned devices? Which one has a higher priority?\r\n\r\nWe need to specify our expectation precisely. \r\n- If user requests CPU for an op, should we include it in a segment? \r\n- If user doesn't specify CPU but TF assigns CPU to an op (our new NMS op is like this case), should we include the op in a segment? \r\n- If user requests GPU, but TF assigns CPU, should we include the op in a segment?", "timestamp": "2019-01-04T17:50:44Z", "author": "MDQ6VXNlcjcwNzY5NjE="}, {"message": "From my observations, nodes all have a blank assigned_device during the segmentation stage. The requested device is set whether using `with tf.device()` or not. So there is no way to distinguish between a user specified device or what TF requests by default.\r\n\r\nI am removing the segmenter rule to exclude CPU ops because in some cases an op may be assigned to CPU but is supported by TRT and we want to include the op in that case.", "timestamp": "2019-01-04T22:18:00Z", "author": "MDQ6VXNlcjEyOTgxNDc0"}, {"message": "Just chatted with @pooyadavoodi today. I think it's fine to leave it as is, meaning we can overwrite CPU placement whenever TF-TRT thinks appropriate, but we need to document this explicitly and clearly in the API doc or README or somewhere.\r\n\r\nassigned_device is set by the placer, which is not triggered by `create_inference_graph()`, but will be triggered if we run the TRT optimizer as part of grappler inside the first `sess.run()` call. So assigned_device is set by TF not by the user.", "timestamp": "2019-01-10T01:28:03Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0ODg3OTMwMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/24944#discussion_r248879301", "comments": [{"message": "Is this too clunky, or does it solve the problem? (We could have separate messages depending on the level of failure, if that's desirable)", "timestamp": "2019-01-17T23:24:49Z", "author": "MDQ6VXNlcjEwODkxNzA0"}, {"message": "I believe the review I'd left was about a way to make this a little less clunky, but it's nbd.  Let's just take this patch, it's sat for long enough.\r\n\r\nThank you!", "timestamp": "2019-02-13T20:08:28Z", "author": "MDQ6VXNlcjE1MDY2Mw=="}, {"message": "Gotcha, glad to help.\r\nThanks!", "timestamp": "2019-02-14T00:44:23Z", "author": "MDQ6VXNlcjEwODkxNzA0"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI1MTYwMTk5MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/25146#discussion_r251601991", "comments": [{"message": "Is the error that big? I'm curious.", "timestamp": "2019-01-28T21:32:02Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "Actually, here we used the same tolerance that Eigen uses for the similar concat test.", "timestamp": "2019-02-02T02:16:06Z", "author": "MDQ6VXNlcjI0OTYzMDYx"}, {"message": "Got it. Could you please explain that in the code as a comment? Thank you!", "timestamp": "2019-02-05T00:01:37Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "Done.", "timestamp": "2019-02-05T00:40:52Z", "author": "MDQ6VXNlcjI0OTYzMDYx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI1MjkwNjMyNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/25368#discussion_r252906324", "comments": [{"message": "Is there a reason for the -2 offset?", "timestamp": "2019-02-01T01:25:00Z", "author": "MDQ6VXNlcjE3MDE3OQ=="}, {"message": "The JVM will throw an OutOfMemoryError when exceed INT_MAX-2, like this:\r\n\r\nhdfsPread: NewByteArray error:\r\nOutOfMemoryError: Requested array size exceeds VM limitjava.lang.OutOfMemoryError: Requested array size exceeds VM limit", "timestamp": "2019-02-01T02:31:15Z", "author": "MDQ6VXNlcjQxOTkwMjc2"}, {"message": "Can you add a comment mentioning the reason?", "timestamp": "2019-02-01T02:50:54Z", "author": "MDQ6VXNlcjE3MDE3OQ=="}, {"message": "Comment is added", "timestamp": "2019-02-01T05:57:18Z", "author": "MDQ6VXNlcjQxOTkwMjc2"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI1MzI3MzUwMA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/25457#discussion_r253273500", "comments": [{"message": "Will it be obvious to readers why we have to do this check here?", "timestamp": "2019-02-02T18:13:30Z", "author": "MDQ6VXNlcjE1MDY2Mw=="}, {"message": "Done :)", "timestamp": "2019-02-04T09:19:16Z", "author": "MDQ6VXNlcjY2NDI2NjM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI2NTg1NDEzMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/26342#discussion_r265854132", "comments": [{"message": "This logic seems to prevent, e.g., a StackPushV2 from being black and a StackPopV2 from being white, right? I have two questions\r\n1. What if some other ops not listed here has the same issue. Would things break if, say, a QueueEnqueueV2 op is white and a QueueDequeueV2 is black?\r\n2. How can such ops ever be colored white or black? They do not have both inputs and outputs.", "timestamp": "2019-03-15T05:32:25Z", "author": "MDQ6VXNlcjY1MTAyMDM="}, {"message": "1. Yes the same logic would need to be applied to Queue*V2 ops if they were added to the clearlist. Currently they are unlisted and so are never painted white.\r\n\r\n2. These ops are on the clearlist, so they can be painted white without being between white nodes. In addition, ephemeral edges are added between writers and readers of the same data structure, so in this sense they do have both inputs and outputs.", "timestamp": "2019-03-26T03:00:18Z", "author": "MDQ6VXNlcjM5NzkwOTY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI2NzY3NzczMw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/26585#discussion_r267677733", "comments": [{"message": "Just curious, why shouldn't `min` be more than zero?", "timestamp": "2019-03-21T09:39:23Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "Since we only support scaled mode, the range has to be [0,m] for unsigned input. Updated the comment to clarify that.", "timestamp": "2019-03-28T23:16:31Z", "author": "MDQ6VXNlcjI0OTYzMDYx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI2NzU4MDI0NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/26729#discussion_r267580244", "comments": [{"message": "Why is this different from the original one?", "timestamp": "2019-03-20T23:03:28Z", "author": "MDQ6VXNlcjYzMTY5MjE="}, {"message": "@haozha111 , thanks for the review i have updated the code as per your comments, kindly check and approve.\r\n\r\nRegards\r\nAmit", "timestamp": "2019-03-21T03:05:09Z", "author": "MDQ6VXNlcjMwODUzMDU0"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI4ODM0NTg2OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/26911#discussion_r288345869", "comments": [{"message": "Just curious, is 256 (not 255) intentional?", "timestamp": "2019-05-28T23:50:18Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI2ODA0OTgyOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/27001#discussion_r268049828", "comments": [{"message": "So the bug fix is removing the check if `summand` is an MKL tensor? Are the following code okay when `summand` is a TF tensor?", "timestamp": "2019-03-22T06:35:37Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "Yes. \r\nActually the fixed code has been working since Q3 2018, in a private branch (Intel) for all our INT8 work. ", "timestamp": "2019-03-27T17:43:35Z", "author": "MDQ6VXNlcjI5MjE1MTk1"}, {"message": "The private branch is called \"int8-master\". However, when people of my team submitted public PR's, they mostly did from other private branches.  That is why this fix was never included. ", "timestamp": "2019-03-27T17:46:55Z", "author": "MDQ6VXNlcjI5MjE1MTk1"}, {"message": "Thanks for your clarifications!", "timestamp": "2019-03-28T02:45:46Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI2OTI5NjEyNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/27157#discussion_r269296124", "comments": [{"message": "Can we convert to tensor instead?", "timestamp": "2019-03-26T20:16:57Z", "author": "MDQ6VXNlcjEzMzI2NzU4"}, {"message": "yes, at first I thought of that only.", "timestamp": "2019-03-26T21:30:20Z", "author": "MDQ6VXNlcjE2ODEyODE2"}, {"message": "Yes, a test case for the use case that wasn't supported before this change.", "timestamp": "2019-03-26T21:48:41Z", "author": "MDQ6VXNlcjEzMzI2NzU4"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3MDQ5NjMxNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/27194#discussion_r270496314", "comments": [{"message": "Why not here as well?", "timestamp": "2019-03-29T16:57:36Z", "author": "MDQ6VXNlcjI5MTAwODE4"}, {"message": "@omalleyt12 , thanks for the review, i have updated the code as per your suggestion. Kindly check and approve.\r\n\r\nRegards\r\nAmit", "timestamp": "2019-03-29T18:36:36Z", "author": "MDQ6VXNlcjMwODUzMDU0"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3NzIxNDQ5NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/27271#discussion_r277214495", "comments": [{"message": "What is the problem that this function solves?  Can we not rely on the destructor of SliceDelayer to delete its state?", "timestamp": "2019-04-22T06:15:15Z", "author": "MDQ6VXNlcjE1MDY2Mw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3NzIxNDUyOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/27271#discussion_r277214529", "comments": [{"message": "What is the problem that this call solves?  Can we not rely on dead code elimination to handle this for us?", "timestamp": "2019-04-22T06:15:28Z", "author": "MDQ6VXNlcjE1MDY2Mw=="}, {"message": "Yes, this function is similar as DCE pass. It eliminates dead code which generates in this pass. \r\nDo you think that it is better to leave the dead code to another DCE pass rather than to handle it in the pass?\r\n", "timestamp": "2019-04-23T09:02:55Z", "author": "MDQ6VXNlcjQxODU4NzA3"}, {"message": "> Do you think that it is better to leave the dead code to another DCE pass rather than to handle it in the pass?\r\n\r\nYes.  This is how most other passes work in XLA.  If you look at e.g. nvptx_compiler, this pass would be part of the \"simplification pipeline\", which includes DCE and runs to a fixed point, i.e. until no changes are made.  So there would be a loop in the compiler that runs both this pass and DCE.", "timestamp": "2019-04-23T15:01:07Z", "author": "MDQ6VXNlcjE1MDY2Mw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3MTM2OTUzNw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/27362#discussion_r271369537", "comments": [{"message": "Will GetOutput method work here?", "timestamp": "2019-04-02T15:37:47Z", "author": "MDQ6VXNlcjM0MjkwMDYz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3Nzg1MDk0OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/27389#discussion_r277850949", "comments": [{"message": "Would it be possible to keep the CPU implementations in the functions at their original location? I assume there is no functional change here, but the move makes it hard to tell.", "timestamp": "2019-04-23T20:10:27Z", "author": "MDQ6VXNlcjc1MjM5ODI="}, {"message": "@chsigg, thank you for your feedback. I just back from holiday and will work on it in the next few days.", "timestamp": "2019-05-02T15:46:27Z", "author": "MDQ6VXNlcjEyOTMyMzU1"}, {"message": "@chsigg, I updated the roll_op.cc as you suggested.", "timestamp": "2019-05-03T12:41:08Z", "author": "MDQ6VXNlcjEyOTMyMzU1"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3MzMxODg5Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/27655#discussion_r273318893", "comments": [{"message": "Why have a separate case here? Won't the codepath below also handle fixed loss scaling?", "timestamp": "2019-04-09T04:12:01Z", "author": "MDQ6VXNlcjY1MTAyMDM="}, {"message": "The intent was to minimize overheads, but the below code should handle fixed as well", "timestamp": "2019-04-09T18:44:39Z", "author": "MDQ6VXNlcjEwODkxNzA0"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI4ODMzODYxMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/27756#discussion_r288338612", "comments": [{"message": "(Sorry I'm not familiar with the details of cudnnrnn) What is dir_count? Will this new check about dir_count break any existing model?", "timestamp": "2019-05-28T23:18:16Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3OTAzMjQ2Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/28011#discussion_r279032463", "comments": [{"message": "I think this needs fixing?", "timestamp": "2019-04-26T17:11:34Z", "author": "MDQ6VXNlcjQ3OTExNw=="}, {"message": "my bad :-)", "timestamp": "2019-04-28T01:36:04Z", "author": "MDQ6VXNlcjMzOTU5OTg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3OTgxODI1MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/28011#discussion_r279818251", "comments": [{"message": "Just confirming that you've tested this path on Android and it works?", "timestamp": "2019-04-30T15:50:11Z", "author": "MDQ6VXNlcjQ3OTExNw=="}, {"message": "Yes, it works on various devices I tested. Some of them need extra setAllowFp16PrecisionForFp32() though.", "timestamp": "2019-05-01T02:35:20Z", "author": "MDQ6VXNlcjMzOTU5OTg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3NzkwNTUzOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/28092#discussion_r277905538", "comments": [{"message": "How do we know that this is true:\r\n\r\n```\r\n        // This is correct as long as the input and output resources\r\n        // are in the same order.\r\n```\r\n?", "timestamp": "2019-04-23T23:09:40Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "Because:\r\n1. outputs for resources are always created when return_updated_values_for_all_resources is set to true.\r\n2. the resource output retvals are sorted according to the input argument position ([here](https://github.com/tensorflow/tensorflow/blob/408cea8c638a9873c8a23887fadf74314bd2a0b9/tensorflow/compiler/tf2xla/xla_compiler.cc#L258))\r\n3. So, as long as the outputs are in the same order as the inputs, they will be pushed into elems in the correct order ([here](https://github.com/tensorflow/tensorflow/blob/408cea8c638a9873c8a23887fadf74314bd2a0b9/tensorflow/compiler/tf2xla/xla_compiler.cc#L304)).", "timestamp": "2019-04-24T00:06:57Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}, {"message": "> So, as long as the outputs are in the same order as the inputs\r\n\r\nI meant what guarantees that the `DT_RESOURCE` outputs are in the same order as the inputs?", "timestamp": "2019-04-24T01:02:26Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "Ah. Got it. It is guaranteed by the functionalization while pass, since it always adds a corresponding output to a DT_RESOURCE input.", "timestamp": "2019-04-24T02:03:39Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}, {"message": "Ok, great, can you add this to the comment?", "timestamp": "2019-04-24T02:18:58Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "BTW, I guess you may also ask in terms of cases other than tf.while (although this commit changes nothing about them). My understanding is as follows--functions other than the while loop body do not have DT_RESOURCE as outputs when they are generated by TF. So, this case may never be hit by them. I, however, wonder why the original author of the codes does not choose to bail out in this case. Do you have any insight?", "timestamp": "2019-04-24T02:20:20Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}, {"message": "Added.", "timestamp": "2019-04-24T05:41:09Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}, {"message": "> Ah. Got it. It is guaranteed by the functionalization while pass, since it always adds a corresponding output to a DT_RESOURCE input.\r\n\r\nBut it isn't guaranteed for `While` loops generated directly by TF.  Right now we have this `RearrangeFunctionArgumentPass` that (I think unsoundly, but I'm double checking with the author) rewrites TF `While` loop bodies to *not* return `DT_RESOURCE`s (so making it look like what the old functionalization pass would have generated).  But we'll have to disable that pass when we check this in since otherwise we won't do the right thing for TF generated while loops.\r\n\r\n> My understanding is as follows--functions other than the while loop body do not have DT_RESOURCE as outputs when they are generated by TF. So, this case may never be hit by them.\r\n\r\nI agree -- we've just been getting lucky because TF happens not to generate such functions.  But IMO we should at least return an `UnimplementedError` instead of silently miscompiling if such odd functions do show up.", "timestamp": "2019-04-25T07:02:01Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "Thank your for bringing up the potential issue related to `RearrangeFunctionArgumentPass` for discussions. I read the code in the pass and summarize my current understanding as follows. Let's see if they are aligned to your understanding.\r\n\r\n1. `xla_compiler` supports the following two forms of TF functions.\r\n  a. `DT_RESOURCE` are only inputs and placed in the last.\r\n  b. `DT_RESOURCE` are both inputs and outputs. They still need to be placed in the last and the input/output order needs to be aligned.\r\n2. TF can generate functions that do not conform to the two forms listed in item 1, but `RearrangeFunctionArgumentPass` will fix them by rewriting the functions to form 1.a. However, note that it rewrites the while loop body **only if** DT_RESOURCE arguments are not placed in the last.\r\n3. Based on 2, `RearrangeFunctionArgumentPass` will _not_ rewrite the functionalized while, as its DT_RESOURCE arguments are always placed in the last.\r\n4. We should not turn off `RearrangeFunctionArgumentPass`, as it is needed to fix functions generated from TF. Also, it does not affect the functionalized while.\r\n", "timestamp": "2019-04-25T22:16:21Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}, {"message": "\r\n> Thank your for bringing up the potential issue related to `RearrangeFunctionArgumentPass` for discussions. I read the code in the pass and summarize my current understanding as follows. Let's see if they are aligned to your understanding.\r\n> \r\n> 1. `xla_compiler` supports the following two forms of TF functions.\r\n>    a. `DT_RESOURCE` are only inputs and placed in the last.\r\n>    b. `DT_RESOURCE` are both inputs and outputs. They still need to be placed in the last and the input/output order needs to be aligned.\r\n> 2. TF can generate functions that do not conform to the two forms listed in item 1, but `RearrangeFunctionArgumentPass` will fix them by rewriting the functions to form 1.a. However, note that it rewrites the while loop body **only if** DT_RESOURCE arguments are not placed in the last.\r\n\r\nMoreover, 1a. for while loops is not correct.\r\n\r\nThe behavior of a while loop is, roughly, `while (cond(state)) { state = body(state); }` which means the input and output state from the while loop body needs to have the same shape.  So the while loop body cannot be edited to remove the `DT_RESOURCE` outputs -- if they're present in the inputs, the need to be present in the outputs.  In that sense `RearrangeFunctionArgumentPass` is unsound because it will convert a correct While loop generated by TF into an incorrect while loop.  Now these incorrect while loops with \"work\" when run through the XLA compiler because the XLA compiler always places `DT_RESOURCE` outputs at the end of the output tuple from a function call, even if the function did not have those `DT_RESOURCE` outputs.\r\n\r\n> 3. Based on 2, `RearrangeFunctionArgumentPass` will _not_ rewrite the functionalized while, as its DT_RESOURCE arguments are always placed in the last.\r\n\r\nI agree.\r\n\r\n> 4. We should not turn off `RearrangeFunctionArgumentPass`, as it is needed to fix functions generated from TF. Also, it does not affect the functionalized while.\r\n\r\nWe will still need the \"rearrange\" aspect of `RearrangeFunctionArgumentPass`.  However, once your change is checked in, we can change the XLA compiler to be stricter about the While loops it accepts (possibly as an explicit `TF_RET_CHECK` or something) by ensuring that the output shape of the TF body computation matches its input shape (*you* don't have to do this, but that would be a logical change to make).  Then the XLA compiler would start failing with while nodes generated by TF and mangled by the `RearrangeFunctionArgumentPass` pass.  So `RearrangeFunctionArgumentPass` needs to be fixed in the same change that makes the XLA compiler stricter.\r\n\r\n> But we'll have to disable that pass when we check this in\r\n\r\nI was wrong here.  I don't think we need to disable `RearrangeFunctionArgumentPass` when we check *this* in, but we'll need to disable it when we make XLA compiler stricter as I suggested above.", "timestamp": "2019-04-29T20:22:01Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3OTEyNzY0MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/28098#discussion_r279127641", "comments": [{"message": "Why is this check needed here?", "timestamp": "2019-04-26T23:09:47Z", "author": "MDQ6VXNlcjExNzQzNzg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3OTU4NjU4NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/28212#discussion_r279586585", "comments": [{"message": "what's the deal with the left shift?", "timestamp": "2019-04-30T00:34:22Z", "author": "MDQ6VXNlcjQ2NDE1Mzk="}, {"message": "Ah, beautiful JNI code... that basically a optimized way to copy `shapes_length` 64-bits values from `shapes_elems` to `cshapes` (i << 3 == i * 8 bytes) ", "timestamp": "2019-04-30T13:12:44Z", "author": "MDQ6VXNlcjEwMTA5NTM0"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3OTgxODU2Mg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/28278#discussion_r279818562", "comments": [{"message": "did clang format reformat this?", "timestamp": "2019-04-30T15:50:50Z", "author": "MDQ6VXNlcjE3OTQ3MTU="}, {"message": "yes.", "timestamp": "2019-04-30T18:24:18Z", "author": "MDQ6VXNlcjI5NjE1Mjgx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI4MDY2Mzg1MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/28349#discussion_r280663851", "comments": [{"message": "Just curious: do we want to consider uint64 and uint32 as well?", "timestamp": "2019-05-03T05:03:25Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "@trevor-m Do we want to support these types?", "timestamp": "2019-05-03T16:37:50Z", "author": "MDQ6VXNlcjQ5MjQ2OTU4"}, {"message": "Sure, we might as well", "timestamp": "2019-05-03T16:47:10Z", "author": "MDQ6VXNlcjEyOTgxNDc0"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI4NTg2NDQ5Ng==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/28451#discussion_r285864496", "comments": [{"message": "Have you considered introducing a wrapper for this?", "timestamp": "2019-05-21T06:15:00Z", "author": "MDQ6VXNlcjc1MjM5ODI="}, {"message": "I had not until now.\r\n\r\nwould something along the following lines work\r\n\r\n```\r\n<in tensorflow/core/gpu_kernel_helper.h>\r\n\r\n#if GOOGLE_CUDA\r\n  #define GPU_DYNAMIC_SHARED_MEM_DECL(ALIGN, TYPE, NAME)     extern __shared__ __align__(ALIGN) TYPE NAME[]\r\n#elif TENSORFLOW_USE_ROCM\r\n  #define GPU_DYNAMIC_SHARED_MEM_DECL(ALIGN, TYPE, NAME)     HIP_DYNAMIC_SHARED(TYPE, NAME)\r\n#endif \r\n\r\n\r\n<and then in the file where we need the shared memory decl, like right here>\r\n\r\nGPU_DYNAMIC_SHARED_MEM_DECL(sizeof(T), unsigned char, smem);\r\n\r\n```\r\n\r\nThere are 8 instances of HIP_DYNAMIC_SHARED in our fork...so this should not be a major change.\r\n\r\nAssuming the above change is the route we want to go, can we track it in a different PR, and leave the change here as is?\r\n\r\nthanks", "timestamp": "2019-05-21T15:32:40Z", "author": "MDQ6VXNlcjM2ODU4MzMy"}, {"message": "@chsigg , gentle ping ... let me know if you are okay with the proposal above, and whether we can address it in a separate PR.", "timestamp": "2019-05-23T18:25:42Z", "author": "MDQ6VXNlcjM2ODU4MzMy"}, {"message": "cool. expect a PR for it sometime next week.\r\n\r\nalso this PR should be good to go now.\r\n\r\nthanks again ", "timestamp": "2019-05-23T19:50:08Z", "author": "MDQ6VXNlcjM2ODU4MzMy"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI4MzI1NjMyOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/28571#discussion_r283256328", "comments": [{"message": "Wasn't this removed at HEAD?", "timestamp": "2019-05-13T09:20:26Z", "author": "MDQ6VXNlcjc1MjM5ODI="}, {"message": "That's right. Done", "timestamp": "2019-05-13T15:40:12Z", "author": "MDQ6VXNlcjM0NTQ1MDE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNDU3MzA2Ng==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/28754#discussion_r304573066", "comments": [{"message": "What is d_inout_scores?", "timestamp": "2019-07-17T18:13:31Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI4NTIwMTY4Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/28780#discussion_r285201683", "comments": [{"message": "Hang on to m[name]? Not that there'll be that many dims.\r\n\r\nDo we also want to validate num_dims?", "timestamp": "2019-05-17T16:38:20Z", "author": "MDQ6VXNlcjM3MzEwMjU="}, {"message": "I try to follow some api design in other c code, I think the `num_dims` should be get from the `TF_CheckpointReaderGetVariableNumDims`?", "timestamp": "2019-05-17T17:51:34Z", "author": "MDQ6VXNlcjEzODY4OTI4"}, {"message": "Right, that's the expected usage. A couple of the others claim to except if the wrong value is passed in.", "timestamp": "2019-05-17T18:03:05Z", "author": "MDQ6VXNlcjM3MzEwMjU="}, {"message": "Then do we need to also pass a `TF_Status` for validating the `num_dims`?", "timestamp": "2019-05-17T18:13:16Z", "author": "MDQ6VXNlcjEzODY4OTI4"}, {"message": "Sounds reasonable to me, for consistency with https://github.com/tensorflow/tensorflow/blob/4d56e9709f1864699be7241a3d1938816d9f31c5/tensorflow/c/c_api.cc#L1172", "timestamp": "2019-05-17T19:09:49Z", "author": "MDQ6VXNlcjM3MzEwMjU="}, {"message": "added", "timestamp": "2019-05-17T19:26:56Z", "author": "MDQ6VXNlcjEzODY4OTI4"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI4OTA0NDY2NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/29143#discussion_r289044665", "comments": [{"message": "Could you fix this as well? It seems to be a small change.", "timestamp": "2019-05-30T15:34:42Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI4ODc5MTU3NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/29147#discussion_r288791574", "comments": [{"message": "Just to double check: if tensor core is not available it'll still fallback to non-tensorcore operations right?", "timestamp": "2019-05-29T22:19:44Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "Yes, after 7.2.1 the tensor_core_op_math is only a hint.", "timestamp": "2019-05-29T22:44:00Z", "author": "MDQ6VXNlcjQwMDE0MjQ="}, {"message": "In that case please document the behavior.", "timestamp": "2019-05-29T23:19:49Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "This has already been doc'ed above. Someone has already mentioned why they didn't use the tensor core op as default one. And I added that  `CuDNN 7.2.1 fixed this issue`.", "timestamp": "2019-05-29T23:28:24Z", "author": "MDQ6VXNlcjQwMDE0MjQ="}, {"message": "I see, sorry I missed that.", "timestamp": "2019-05-29T23:55:49Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI4OTc2MzcxOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/29185#discussion_r289763719", "comments": [{"message": "No 'explicit'.\r\n\r\nCould you please document how scratch_size is used and whether/how it differs between CUDA and ROCm?\r\n\r\nWould it make sense for the CUDA path to report the required scratch size as well?", "timestamp": "2019-06-03T09:39:41Z", "author": "MDQ6VXNlcjc1MjM5ODI="}, {"message": "@chsigg. I've removed `explicit` from this ctor. Also I put more comments to explain the rationale behind this `scratch_size` field.\r\n\r\n\r\nHere's an more elaborated explaination from @deven-amd \r\n\r\nThat need is primarily due to a difference between the cudnn/miopen apis on the cudnn side, there is a API ( cudnnGetConv*WorkspaceSize ) which will return the scratch_size needed for a given conv parameters tuple (algo, conv shape).\u00a0 This API is called during each convolution call to determine the required scratch size.\r\n\r\nOn the miopen side, there does not exist such an API, and therefore things need to be done a little differently. During the auto-tuning phase, we know the scratch_size required by the best algorithm (corresponding to a given conv parameter tuple), and we need to store this information for subsequent use. That is the reason why we need to add the scratch_size field to the AlgorithmConfig\r\n\r\nThis also means that the scratch_size field in AlgorithmConfig is populated and used in ROCm only (since there is no need to do so on the CUDA side).", "timestamp": "2019-06-03T15:56:21Z", "author": "MDQ6VXNlcjE2NzM1NzQ="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI4OTk0MDYyNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/29366#discussion_r289940624", "comments": [{"message": "Why not a verified module?", "timestamp": "2019-06-03T16:54:00Z", "author": "MDQ6VXNlcjE1MDY2Mw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI4OTk0MDk5NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/29366#discussion_r289940995", "comments": [{"message": "Same, why not a verified module?", "timestamp": "2019-06-03T16:54:54Z", "author": "MDQ6VXNlcjE1MDY2Mw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI5NzM1MDk3NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/29754#discussion_r297350975", "comments": [{"message": "s/registerd/registered\r\n\r\nIs there a way to print some information about the already-registered op? Maybe line/file?", "timestamp": "2019-06-25T19:14:57Z", "author": "MDQ6VXNlcjUwNjE="}, {"message": "Fixed.", "timestamp": "2019-06-26T16:14:24Z", "author": "MDQ6VXNlcjI0OTYzMDYx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI5Mzg3Mjc4NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/29774#discussion_r293872784", "comments": [{"message": "I don't know what value this third example is adding", "timestamp": "2019-06-14T16:05:48Z", "author": "MDQ6VXNlcjUwNjE="}, {"message": "Removed third example. Thanks!", "timestamp": "2019-06-14T17:23:56Z", "author": "MDQ6VXNlcjQ2MDU4MTcz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwMzA5MzUyMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/29939#discussion_r303093521", "comments": [{"message": "What about sorted_values? Wouldn't it have the same issue?", "timestamp": "2019-07-12T18:12:26Z", "author": "MDQ6VXNlcjQ3OTExNw=="}, {"message": "@jdduke Thank you for your reviewing.\r\n\r\nI don't think sorted_values has the same issue.\r\n\r\nThe access range of sorted_values is [0, **num_indices_to_sort**).\r\n```\r\nfor (int row = 0; row < num_indices_to_sort; row++) {\r\n  sorted_values[row] = ....\r\n}\r\n```\r\n\r\nWhere **num_indices_to_sort** is guaranteed to be less than max_detections.\r\n\r\n```\r\nint num_indices_to_sort = std::min(output_index, max_detections);\r\n```\r\n\r\nSo, I'm  thinking my first patch is fine, what do you think ?\r\n", "timestamp": "2019-07-13T01:53:29Z", "author": "MDQ6VXNlcjQwMzc0NDY1"}, {"message": "That sounds right. I wonder if there's a simple test case we can add which exercises this? If not, no worries.", "timestamp": "2019-07-15T16:35:15Z", "author": "MDQ6VXNlcjQ3OTExNw=="}, {"message": "@jdduke Thank you for your reviewing.\r\n\r\nI also think it would be better if there is a test.\r\nBut, I'm afraid that I can't add the test case because I'm not familiar with the test framework.", "timestamp": "2019-07-16T00:15:33Z", "author": "MDQ6VXNlcjQwMzc0NDY1"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI5NzM1NTg2OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/30098#discussion_r297355868", "comments": [{"message": "It'd be nice if we could show something here at all. If I read this code and not execute it I get almost no information out of it (other than the requirements on the input shapes of the op).\r\n\r\nCan we pick a known small one-channel image and show what a glimpse is?", "timestamp": "2019-06-25T19:27:43Z", "author": "MDQ6VXNlcjUwNjE="}, {"message": "Sure let me add that", "timestamp": "2019-06-25T20:03:22Z", "author": "MDQ6VXNlcjEyOTUwNDM5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMzNjMyMzY1Mg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/30107#discussion_r336323652", "comments": [{"message": "It looks like we should have also watched complex types here?\r\n(context: https://github.com/tensorflow/tensorflow/issues/32774)", "timestamp": "2019-10-18T05:19:16Z", "author": "MDQ6VXNlcjI2NTI3"}, {"message": "This had been fixed in 507325c5b3fa5943485f3f994048fe7683e0f95d", "timestamp": "2019-11-01T17:11:55Z", "author": "MDQ6VXNlcjg0Mzcy"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwMTIxNDM2Mg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/30258#discussion_r301214362", "comments": [{"message": "I don't see the reason to mock the output, are u trying to validate any output here?", "timestamp": "2019-07-08T17:30:08Z", "author": "MDQ6VXNlcjUxMTg4ODE="}, {"message": "Thanks for the comment. Removed.", "timestamp": "2019-07-09T22:44:53Z", "author": "MDQ6VXNlcjY5MzIzNDg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwMTU2NzIzOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/30377#discussion_r301567238", "comments": [{"message": "Do AMDGPU support atomic intrinsics? Does it make sense to add a TODO to support them here?", "timestamp": "2019-07-09T12:50:23Z", "author": "MDQ6VXNlcjM5OTQ3OTIx"}, {"message": "There are atomic intrinsics but they don't support floating point types. So we really can't put a TODO here.", "timestamp": "2019-07-09T13:33:52Z", "author": "MDQ6VXNlcjE2NzM1NzQ="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNjc3NTkxNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/30752#discussion_r306775914", "comments": [{"message": "Why can't we use the native https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/framework/tensor_spec.py#L103? Basically print(tensor_spec) or repr(tensor_spec).", "timestamp": "2019-07-24T12:12:27Z", "author": "MDQ6VXNlcjM3NDEwNzAy"}, {"message": "We obviously can. But in reality most of the newbies would have no idea about TensorSpec. So, if a user downloads a model from TFHub and tries to check with saved_model_cli, there's a chance of confusion. So I thought maybe it's better to show \"Tensor\", (which is a the exact thing the model expects as it's input), instead of \"TensorSpec\". What do you suggest?", "timestamp": "2019-07-24T12:56:35Z", "author": "MDQ6VXNlcjEzOTk0MjAx"}, {"message": "I am really not getting the educational point here. If we present things differently, it will just cause more confusion long term. Please remove this redundant code.", "timestamp": "2019-07-25T11:26:06Z", "author": "MDQ6VXNlcjM3NDEwNzAy"}, {"message": "> I am really not getting the educational point here. If we present things differently, it will just cause more confusion long term. Please remove this redundant code.\r\n\r\nOkay! :)", "timestamp": "2019-07-25T12:04:47Z", "author": "MDQ6VXNlcjEzOTk0MjAx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNjc5MDA0MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/30752#discussion_r306790040", "comments": [{"message": "> Why don't we print dictionaries? What if the function returns a dictionary?\r\n\r\n@vbardiovskyg , we are printing dictionaries, that is needed since, dictionary is nested and it is the only simple structure that has key-value pair, and one of the values might be list or with much deeper nesting, so I'm handling it differently.", "timestamp": "2019-07-24T12:48:13Z", "author": "MDQ6VXNlcjEzOTk0MjAx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNDg3NzA5MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/30759#discussion_r304877090", "comments": [{"message": "This VLOG would triggers for the same cases as the one on line 643, right? Is either one good enough?", "timestamp": "2019-07-18T12:00:15Z", "author": "MDQ6VXNlcjM5OTQ3OTIx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNTA1OTQ0NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/30789#discussion_r305059445", "comments": [{"message": "Remove this loop? Is it for debugging purposes?", "timestamp": "2019-07-18T18:36:44Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNDcyMjg5OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/30825#discussion_r304722899", "comments": [{"message": "does this not work in graph mode?", "timestamp": "2019-07-18T03:36:39Z", "author": "MDQ6VXNlcjM0NTUxNzc="}, {"message": "This test case does not work in graph mode. If I understand correctly, the failure may be expected, because the iterator resource is still held when the iteration is not finished and the iterator deconstructor can not be triggered. The error message is in below.\r\n```\r\nThere appears to be a concurrent caching iterator running - cache lockfile already exists ('/var/folders/44/dzfy03t93h990mbh3qjy3pjm0000gn/T/tmp6bpwffxz/cache_0.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1563470964\r\n``` ", "timestamp": "2019-07-18T17:37:45Z", "author": "MDQ6VXNlcjUwNTc3NDA="}, {"message": "Got it, makes sense. Can we update the check to\r\n```python\r\nif not context.executing_eagerly():\r\n  self.skipTest(\"Test requires eager mode for iterators to be deconstructed\")\r\n\r\nfor i in [0, 3, 10, 12, 15]:\r\n  do_test(i)\r\n```", "timestamp": "2019-07-18T23:01:38Z", "author": "MDQ6VXNlcjM0NTUxNzc="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNzM3MzI2OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31012#discussion_r307373269", "comments": [{"message": "Normally we file a bug internally with some details on why the op is slow.  Since you can't file an internal bug do you mind adding some comments on why these ops are slow / undesirable?", "timestamp": "2019-07-25T15:53:14Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "To be precise, I am opting out these ops, no because they are slow, but because they create convolutions that CuDNN can't handle.  I looked around for lists similar to OpIsSlow, but didn't find anything more proper.  Please let me if there is a better way, or whether to create another list.\r\n\r\nI have explained how the error is triggered in my first comment in this PR.  I am attaching a python code reproducing the error.  You can switch on/off XLA to observe the difference.  Hopefully it is sufficient for a bug description.\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nimage = tf.placeholder(tf.float32, shape=[16, 256, 256, 16], name='image')\r\nresize_nearest_neighbor = tf.image.resize_images(image, size=[512,512], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR, align_corners=True)\r\nfeed_dict={image: np.random.random_sample([16, 256, 256, 16])}\r\n\r\nsess = tf.Session()\r\nwith sess.as_default():\r\n    actual_resize_nearest_neighbor = resize_nearest_neighbor.eval(feed_dict)\r\n", "timestamp": "2019-07-25T16:47:20Z", "author": "MDQ6VXNlcjQ5MjExOTAz"}, {"message": "Sorry, I meant comments in the code.\r\n\r\nAlthough I guess just liking to this discussion thread would be fine too.", "timestamp": "2019-07-26T15:35:40Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "@sanjoy, I have added comments that link to this thread.\r\nBTW, I suppose that users can switch cluster/allow_slow_ops at run-time.  Can you show me how to do that?\r\nP.S. I will make merge to push forward https://github.com/tensorflow/tensorflow/pull/30336, as I don't change image_resize_ops.cc.  I will need your help to review it again.", "timestamp": "2019-07-26T17:13:27Z", "author": "MDQ6VXNlcjQ5MjExOTAz"}, {"message": "> @sanjoy, I have added comments that link to this thread.\r\n\r\nThanks!\r\n\r\n> BTW, I suppose that users can switch cluster/allow_slow_ops at run-time. Can you show me how to do that?\r\n\r\nTF users can't change these options directly.  These bits are mainly used to control the behavior of `RecursiveCompilabilityChecker` by the various clients of the class (none of which are directly user facing).\r\n\r\n> P.S. I will make merge to push forward #30336, as I don't change image_resize_ops.cc. I will need your help to review it again.\r\n\r\n", "timestamp": "2019-07-26T17:44:32Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "Is there any further comment for me to address?  If not, please approve the PR.", "timestamp": "2019-07-29T03:36:11Z", "author": "MDQ6VXNlcjQ5MjExOTAz"}, {"message": "I already approved it the last time, but it looks like a CI build failed.  I'll re-trigger the CI.", "timestamp": "2019-07-29T15:37:16Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNzk0MTgxMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31090#discussion_r307941811", "comments": [{"message": "Just curious, why don't we need this for `int8` too?", "timestamp": "2019-07-26T23:55:37Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "Int8/Quantized version of MKL MaxPooling Op does not have that extra workspace tensor defined. For some reason, this workspace tensor is unique to MaxPooling FP32 op. @yiqianglee can help clarify please.", "timestamp": "2019-07-27T06:10:13Z", "author": "MDQ6VXNlcjIyMzA2ODQ2"}, {"message": "Workspace as second output tensor only happen on MklMaxPooling's OP definition, in MklQuantizeMaxPool, it doesn't need workspace tensor (because workspace tensor is used by MaxPool's backward op, int8 is inference only).\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L2168\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/mkl_nn_ops.cc#L120", "timestamp": "2019-07-27T08:37:52Z", "author": "MDQ6VXNlcjE5OTQwOTM5"}, {"message": "Thank you for the explanation! ", "timestamp": "2019-07-29T18:20:10Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxMDIxMjk0Nw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31259#discussion_r310212947", "comments": [{"message": "Would you please explain how this test relates to the heuristic change?", "timestamp": "2019-08-02T16:45:50Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Sorry, this test change was for a different PR https://github.com/tensorflow/tensorflow/pull/31156.  I will revert it in this branch.", "timestamp": "2019-08-02T17:24:10Z", "author": "MDQ6VXNlcjQ5MjExOTAz"}, {"message": "Please ignore my previous comment on this change.  I thought that it mixed into a different branch.\r\n\r\nRegarding https://github.com/tensorflow/tensorflow/pull/31156, this test ensures that integer-type nearest neighbor resizing proceeds without any error.  It would trigger error for GPU without the heuristic change in https://github.com/tensorflow/tensorflow/commit/fd34d066ec5f514d737d771efaf0f6cf93925cf7#diff-e46056ad1b86847e16150ab7906bab3b.  According to https://github.com/tensorflow/tensorflow/pull/30336#discussion_r303168563, CPU backend would also fail.", "timestamp": "2019-08-02T17:47:02Z", "author": "MDQ6VXNlcjQ5MjExOTAz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxMDY0NTk3OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31294#discussion_r310645979", "comments": [{"message": "if the kernel supports batch major input, does it mean this transpose should be skipped?", "timestamp": "2019-08-05T14:54:20Z", "author": "MDQ6VXNlcjUxMTg4ODE="}, {"message": "This PR will skip the transpose when sequence_lengths/mask is provided (indicating the v3 API will be used and only v3 supports batch major).", "timestamp": "2019-08-05T17:56:10Z", "author": "MDQ6VXNlcjQwMDE0MjQ="}, {"message": "Agreed, I wasn't notice this when I look at the start of the function. I can approve the PR as is, but I think it will be nice to have v3 enabled for non-masking case as well.", "timestamp": "2019-08-05T18:00:09Z", "author": "MDQ6VXNlcjUxMTg4ODE="}, {"message": "Sure. Actually, the cudnn rnn ops in contrib has already done it by preparing the seq_len for users. https://github.com/tensorflow/tensorflow/blob/a232fda68d692da0f409608dd5960d63ac700e5d/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py#L1133.\r\n\r\nI can do another PR for this feature when this is done.\r\n\r\n", "timestamp": "2019-08-05T18:08:27Z", "author": "MDQ6VXNlcjQwMDE0MjQ="}, {"message": "oh, I was wondering if this can be done on the c op level for performance reason. It will be nice If the implementation of cudnnrnn_v3 can take care of the absent of sequence length rather than force user to provide one.", "timestamp": "2019-08-05T20:34:24Z", "author": "MDQ6VXNlcjUxMTg4ODE="}, {"message": "I think it could be done. But I don't see the performance issue for letting python prepare the seq_len, since this array is on host memory and will be on host memory even for cudnn calls.", "timestamp": "2019-08-05T20:41:34Z", "author": "MDQ6VXNlcjQwMDE0MjQ="}, {"message": "I guess you have better knowledge for the cudnn kernel, my original idea was that for non-mask input, the generation of seq_len tensor is not needed at all (just process the whole input sequence). ", "timestamp": "2019-08-05T20:46:58Z", "author": "MDQ6VXNlcjUxMTg4ODE="}, {"message": "Yes, let me check how much work need to be done for generating seq_len inside the op (which might require the API change, because V3 requires seq_len).\r\n\r\nBut for this PR, I think it is ready to go.", "timestamp": "2019-08-05T20:52:58Z", "author": "MDQ6VXNlcjQwMDE0MjQ="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMzMzY1MTAzMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31465#discussion_r333651032", "comments": [{"message": "This test is failing during the merging, it seems the result is not deterministic, we're still investigating. Do you have any idea?", "timestamp": "2019-10-10T17:43:28Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "After some looking / thinking, I don't know why this would fail. One hypothesis is that one of the ops used to implement the solution has changed and now operates non-deterministically, such as the reductions used in the back-prop of implicit broadcast. I think this is very unlikely however, and I'm seeing this test pass on TF `1.15.0rc2` with the same changes to `bias_add` as in this PR applied via a dynamic patch. Is there a publicly-visible commit hash that you could give me that I could try rebasing the changes onto in order to try to reproduce?", "timestamp": "2019-10-10T18:20:57Z", "author": "MDQ6VXNlcjMzNTMyOTQx"}, {"message": "I'm wondering if the test is failing when run using XLA and that the XLA kernel fusion is replacing part or all of the deterministic solution with a non-deterministic kernel.", "timestamp": "2019-10-10T23:48:26Z", "author": "MDQ6VXNlcjMzNTMyOTQx"}, {"message": "In `tensorflow/tensorflow.bzl` in the master branch, it looks like `xla_enable_strict_auto_jit` is ignored. I'm wondering if it's not being ignored in the CI setup that the test is failing in. I'm wondering if `xla_enable_strict_auto_jit = True` should be removed from `tensorflow/python/kernel_tests/BUILD` for `bias_op_deterministic_test`. As an experiment, do you think it's worth me pushing a commit to this PR branch to remove it, @aaroey ?", "timestamp": "2019-10-11T00:04:37Z", "author": "MDQ6VXNlcjMzNTMyOTQx"}, {"message": "I  have reproduced what appears to be the same issue in a slightly different context. Enabling auto-JIT does make that test fail. XLA is probably replacing the deterministic mini-graph of ops with the original non-deterministic op kernel. I'll push a commit with auto-JIT disabled in a minute.", "timestamp": "2019-10-11T01:06:08Z", "author": "MDQ6VXNlcjMzNTMyOTQx"}, {"message": "Hi @aaroey, I just disabled XLA JIT for this test in the most recent commit. Please will you review and run tests again and see if that resolves the issue.", "timestamp": "2019-10-11T01:17:06Z", "author": "MDQ6VXNlcjMzNTMyOTQx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxNDA5ODUzNw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31623#discussion_r314098537", "comments": [{"message": "What if people decide to run an old graph with eager mode?", "timestamp": "2019-08-14T21:48:51Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "@penpornk I can add it also, but I thought since python is code is calling V2 now we won't see V1 here.", "timestamp": "2019-08-14T23:35:32Z", "author": "MDQ6VXNlcjI1Mzc0ODA4"}, {"message": "It can still be called in Python through `gen_math_ops.batch_mat_mul`, but up to you. :)", "timestamp": "2019-08-14T23:49:08Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "Thanks @penpornk  will add it also.", "timestamp": "2019-08-15T00:04:53Z", "author": "MDQ6VXNlcjI1Mzc0ODA4"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxODI3MzIyMw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31635#discussion_r318273223", "comments": [{"message": "why do you needs this?", "timestamp": "2019-08-27T20:07:32Z", "author": "MDQ6VXNlcjEwNzIwNzk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxODI3MzI3OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31635#discussion_r318273278", "comments": [{"message": "why do you need this?", "timestamp": "2019-08-27T20:07:38Z", "author": "MDQ6VXNlcjEwNzIwNzk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxODI3MzUwOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31635#discussion_r318273509", "comments": [{"message": "why do we need this?", "timestamp": "2019-08-27T20:08:15Z", "author": "MDQ6VXNlcjEwNzIwNzk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxOTMyMDU5Mg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31635#discussion_r319320592", "comments": [{"message": "why is this needed?", "timestamp": "2019-08-30T00:29:00Z", "author": "MDQ6VXNlcjEwNzIwNzk="}, {"message": "Some places like [here](https://github.com/tensorflow/tensorflow/pull/31635/files#diff-ba4d21ca0c437c902fd8c791e06ba44dR308) need it to convert [AttributeVector](https://github.com/tensorflow/tensorflow/pull/31635/files#diff-a4a3b8bfea4ac6f0185c2a132dadbb8bR32) to `Attrs`.", "timestamp": "2019-08-30T00:46:31Z", "author": "MDQ6VXNlcjUwNTc3NDA="}, {"message": "`AttributeVector` can not be replaced by `Attrs` as it is required by `test::function::NDef` like [here](https://github.com/tensorflow/tensorflow/pull/31635/files#diff-4085eb633f02b68967febffed95bc6edR41).", "timestamp": "2019-08-30T16:46:40Z", "author": "MDQ6VXNlcjUwNTc3NDA="}, {"message": "sounds good", "timestamp": "2019-08-30T18:43:53Z", "author": "MDQ6VXNlcjEwNzIwNzk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjYyMjA3NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31700#discussion_r376622075", "comments": [{"message": "Isn't this only using the second output still? Not sure this needs to change\r\n\r\nIt might be cleaner if you can just re-run the update script: https://github.com/tensorflow/tensorflow/blob/392249966a19d0f1923dc1e1b743ccdfb2887ee6/tensorflow/python/eager/gradient_input_output_exclusions.py#L19\r\n\r\nIf that runs into issues we can ping @saxenasaurabh for help. Sorry for the major merge conflict.", "timestamp": "2020-02-07T21:44:46Z", "author": "MDQ6VXNlcjM3MzEwMjU="}, {"message": "Yep, I think you got the inputs and outputs switched. +1 to running the update script. There is a test that checks that `pywrap_gradient_exclusions.cc` matches the update script output so that would fail if there isn't an exact match. (Hopefully we will get rid of the circular dependencies and convert this into a genrule).\r\n\r\nThanks!", "timestamp": "2020-02-07T23:06:18Z", "author": "MDQ6VXNlcjM5Njc0ODg="}, {"message": "@allenlavoie you are right, only the second output is required and I fixed by running the script as you proposed. ", "timestamp": "2020-02-07T23:12:10Z", "author": "MDQ6VXNlcjExMzY4NzE4"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxNTI4OTc1NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31717#discussion_r315289754", "comments": [{"message": "Why should all the inputs have the same shapes?", "timestamp": "2019-08-19T16:01:46Z", "author": "MDQ6VXNlcjcwNzY5NjE="}, {"message": "Fixed the one for build(). For calibration this is required since it can only support single engine today.", "timestamp": "2019-08-19T16:28:01Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxNTkxODI0MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31809#discussion_r315918240", "comments": [{"message": "If there is a TensorListPushBackBatch op, will the grappler pass incorrectly change the dtype of other TensorList ops for the same TensorList? Or will it keep all ops for that tensor list in fp32? I think it's the former case, if I'm reading the code correctly\r\n\r\nIf it's the former case, you should add a comment stating it will have a different dtype from it's tensor list which can cause the graph to crash when run.", "timestamp": "2019-08-20T21:46:54Z", "author": "MDQ6VXNlcjY1MTAyMDM="}, {"message": "Thanks for pointing this out. I've added a workaround that will prevent it from breaking the graph.", "timestamp": "2019-09-10T04:47:48Z", "author": "MDQ6VXNlcjM5NzkwOTY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxNTkyMDMzMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/31809#discussion_r315920332", "comments": [{"message": "I think I asked before, but to clarify: These ephermal edges aren't to ensure all the ops for a tensor list have the same dtype, right? It only determines whether the tensor list and all its edges will be changed to fp16 or not?", "timestamp": "2019-08-20T21:53:41Z", "author": "MDQ6VXNlcjY1MTAyMDM="}, {"message": "That's right. The forcing of ops to have the same dtype is handled independently of the ephemeral edges.", "timestamp": "2019-09-10T04:47:55Z", "author": "MDQ6VXNlcjM5NzkwOTY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyMDk0MDI3OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/32216#discussion_r320940278", "comments": [{"message": "Why the change?", "timestamp": "2019-09-04T19:42:04Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "This is not necessary either.", "timestamp": "2019-09-04T20:13:35Z", "author": "MDQ6VXNlcjQ5MjExOTAz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyMjIyMjA3Nw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/32287#discussion_r322222077", "comments": [{"message": "you get `int` if you use `int / int * int` and there exist some numerical problems.\r\nis this what you expect?", "timestamp": "2019-09-09T12:46:42Z", "author": "MDQ6VXNlcjUyODQ5MjQ="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyMjUxMTYzMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/32364#discussion_r322511631", "comments": [{"message": "I don't quite understand the logic here; but it seems very strange that CUDA-specific file has ROCM-specific workaround?", "timestamp": "2019-09-10T00:47:34Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "This file is shared across `ROCm` and `Cuda`. I can open a new PR after this one with renaming changes. \r\n\r\nRegarding the workaround, it has a long history here. If I remember it right, the check for whether algorithm config is empty is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/rocm/rocm_dnn.cc#L2623).", "timestamp": "2019-09-10T15:23:30Z", "author": "MDQ6VXNlcjM0NTQ1MDE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODM4OTA2MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/32364#discussion_r328389060", "comments": [{"message": "Could we make this ROCm-specific? Or even better, could we have a cleaner workaround?", "timestamp": "2019-09-26T00:10:29Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "Reverting this change makes CUDA tests pass again.", "timestamp": "2019-09-26T17:47:36Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "As long as miopen find mode exist in `ROCm`, the passed in algorithm has to be empty for [this check to work](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/rocm/rocm_dnn.cc#L2623). If this is causing trouble from your end, I can add an additional condition in this if statement to check if the stream is under `se::PlatformKind::kROCm`.", "timestamp": "2019-09-26T22:45:44Z", "author": "MDQ6VXNlcjM0NTQ1MDE="}, {"message": "Could you expand on what is contained in the check (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/rocm/rocm_dnn.cc#L2623) and why is it necessary?", "timestamp": "2019-09-26T22:48:15Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "This conditional check helps to determine if it is in profiling stage or not. In profiling stage, `ThenConvolveWithAlgorithm()` will call with [an empty `AlgorithmConfig()`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L1015). rocm_dnn use the empty AlgorithmConfig() to know it is in profiling stage (High overhead, for miopen to find and compile the algorithm). \r\n\r\nWhen profiling is over, AlgorithmConfig will not be empty and it includes both the algorithm and the scratch_size. This scenario has low overhead because all information are known so `miopen` can act on known parameters.", "timestamp": "2019-09-26T23:05:58Z", "author": "MDQ6VXNlcjM0NTQ1MDE="}, {"message": "Sorry I still don't quite understand. What is the difference between CUDA trace and ROCm trace? Why CUDA does not need the same trick?", "timestamp": "2019-09-26T23:11:48Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "That's a fair question. If I remember correctly, this is due to `miopen` and `cudnn` library requires different arguments for getting a convolution algorithm. A detailed explanation please see [here](https://github.com/tensorflow/tensorflow/blob/10a42250f398a2d8b295a8d23603fc0286a67051/tensorflow/stream_executor/dnn.h#L828). I was not involved in this part of code changes. @deven-amd if you need further information.", "timestamp": "2019-09-27T15:47:29Z", "author": "MDQ6VXNlcjM0NTQ1MDE="}, {"message": "Sorry I still do not understand the issue here. If there is a trigger required for ROCm, can we make it explicit and trigger it only in ROCm mode? Moreover, maybe we could trigger it inside the actual ROCm-specific function using e.g. `std::call_once`?", "timestamp": "2019-10-01T17:38:43Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "I make it explicitly to be triggered under ROCm only. I believe this should address the failures caused by this.", "timestamp": "2019-10-01T18:43:05Z", "author": "MDQ6VXNlcjM0NTQ1MDE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODg2ODEwOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/32747#discussion_r328868109", "comments": [{"message": "I think you need a print(output) here right?", "timestamp": "2019-09-26T23:44:11Z", "author": "MDQ6VXNlcjE0MjYyNDE3"}, {"message": "Yes, thanks @yashk2810. Updated. ", "timestamp": "2019-09-27T18:41:16Z", "author": "MDQ6VXNlcjQxNDQ3MDQ5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMzMjcxOTgzOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/32887#discussion_r332719838", "comments": [{"message": "The reason for having these here is different than what the comment says right?  These ops are not \"passthrough\" like `kWhile`, they genuinely have mixed-precision operands?", "timestamp": "2019-10-08T20:34:38Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "Hmmm...I think I missed the comments. However, the reason I added these is that I got an error from the `hlo_verifier` without this change. I might have missed something. I'll take a look. ", "timestamp": "2019-10-08T20:54:43Z", "author": "MDQ6VXNlcjQyOTg0Njc2"}, {"message": "You are right..I am not sure what it was..I removed these cases. These are incorrect. Sorry for the oversight.", "timestamp": "2019-10-09T18:22:09Z", "author": "MDQ6VXNlcjQyOTg0Njc2"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMzMzU2MDY2Mg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/33021#discussion_r333560662", "comments": [{"message": "Would it be better to use GpuAtomicSub instead?\r\n\r\nI'm very puzzled why we seem to be updating the real part twice, here and in ADD. Looks like a bug to me.", "timestamp": "2019-10-10T14:40:26Z", "author": "MDQ6VXNlcjc1MjM5ODI="}, {"message": "It just unwrap the function call to `ADD`.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc#L58", "timestamp": "2019-10-16T03:30:15Z", "author": "MDQ6VXNlcjEwMjg1ODY5"}, {"message": "Yes, I see that you are unwrapping the ADD. But if you are unwrapping, why not use GpuAtomicSub instead? And is the ADD implementation actually correct?", "timestamp": "2019-10-16T12:33:42Z", "author": "MDQ6VXNlcjc1MjM5ODI="}, {"message": "https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add\r\nhttps://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_sub\r\n\r\nYou mean for complex type, should be the real part plus real part, imaginary part plus the imaginary part?", "timestamp": "2019-10-17T06:40:52Z", "author": "MDQ6VXNlcjEwMjg1ODY5"}, {"message": "That's how complex numbers are added, AFAIK. Thanks.", "timestamp": "2019-10-21T18:09:11Z", "author": "MDQ6VXNlcjc1MjM5ODI="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMzMjA1MzYxMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/33063#discussion_r332053612", "comments": [{"message": "Does this work for `DT_RESOURCE` outputs?", "timestamp": "2019-10-07T14:32:53Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "It should. haven't seen any issue locally.", "timestamp": "2019-10-08T22:01:00Z", "author": "MDQ6VXNlcjEwMDkxMDQ4"}, {"message": "Looks like both strings and resources are handled specially and automatically placed on the host: https://github.com/tensorflow/tensorflow/blob/cca6113667e10b1f3405dbe3b1bd3d983f9cf420/tensorflow/core/framework/memory_types.cc#L146", "timestamp": "2019-10-09T14:46:06Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM0NjA4NjMxOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/33583#discussion_r346086318", "comments": [{"message": "It isn't just a performance consideration right, it looks like this expansion step is also working around some issue in `CudnnBatchNormRewriter`?", "timestamp": "2019-11-14T01:20:51Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "Technically yes...`CudnnBatchNormRewriter` creates input descs by collapsing all physical dimensions \"major/slower\" than the channel dimension to  dim[0] and creating an `NCHW` descriptor by default. Although that is not incorrect, cudnn has restrictions on dim[0] of input desc for batchnorm inference. This will work around that restriction/unsupported configuration.", "timestamp": "2019-11-14T01:36:37Z", "author": "MDQ6VXNlcjQyOTg0Njc2"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM0MzIyNTcxMw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/33703#discussion_r343225713", "comments": [{"message": "Did this happen in practice? It's probably a bug somewhere if node def in null.", "timestamp": "2019-11-06T17:23:04Z", "author": "MDQ6VXNlcjExNzQzNzg="}, {"message": "@ezhulenev No, this just pass some tools check.", "timestamp": "2019-11-07T00:40:09Z", "author": "MDQ6VXNlcjQwMDU1OTE3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkyODU0OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/33955#discussion_r373928548", "comments": [{"message": "Is the problem solved with this line only?   Could you elaborate a bit more on the initialization error? Thx.", "timestamp": "2020-02-03T05:37:37Z", "author": "MDQ6VXNlcjM0MjkwMDYz"}, {"message": "pdm_start_dma() is the function that sets g_pdm_dma_error_reporter using the function argument.\r\nAs such the first call in InitAudioRecording() must call it with the real error_reporter (and not the uninitialized variable g_pdm_dma_error_reporter)\r\nI wanted to keep the change small as the logic had been copied at least once for sparkfun artemis boards. Otherwise I would have moved setting g_pdm_dma_error_reporter to InitAudioRecording () and removed it from pdm_start_dma()", "timestamp": "2020-02-03T06:00:39Z", "author": "MDQ6VXNlcjIzMzM0NzM="}, {"message": "Sry for coming back late. I meant you had the following code on line 462\r\n\r\n```suggestion\r\n  g_pdm_dma_error_reporter = error_reporter;\r\n```\r\n\r\nwhich means that even if you don't change this line (467), it will effectively behave the same. \r\n\r\nI think the main question is: is there a place where `g_pdm_dma_error_reporter` can't be simply replaced by error_reporter? It looks that in all places in the method, g_pdm_dma_error_reporter is either nullptr or error_reporter", "timestamp": "2020-03-09T09:14:26Z", "author": "MDQ6VXNlcjM0MjkwMDYz"}, {"message": "@wangtz : g_pdm_dma_error_reporter is used to allow the pdm interrupt handler (am_pdm0_isr) to access the error_reporter. As such setting g_pdm_dma_error_reporter early (line 462) prevents interrupt races as much as possible without adding explicit synchronization (Should implicitly happen during hal access) . Line 467 does not really need to change as InitAudioRecording() is the only place that modifies g_pdm_dma_error_reporter - however I believe using the local error_reporter instead of  g_pdm_dma_error_reporter as parameter to pdm_start_dma() is a tiny bit more readable. ", "timestamp": "2020-03-09T21:25:02Z", "author": "MDQ6VXNlcjIzMzM0NzM="}, {"message": "@suphoff Can you please check @wangtz's comments and keep us posted. Thanks!", "timestamp": "2020-03-24T11:27:27Z", "author": "MDQ6VXNlcjQ4MjE1NzE3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1MzMwNTY4NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/34660#discussion_r353305684", "comments": [{"message": "Where is the mnist test data?", "timestamp": "2019-12-03T17:02:48Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "ooops. Just added back.", "timestamp": "2019-12-04T00:31:43Z", "author": "MDQ6VXNlcjcwNzY5NjE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODUyMzkzMw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/34869#discussion_r378523933", "comments": [{"message": "Do you need to filter out this case when running on < 1.2 NNAPI?", "timestamp": "2020-02-12T21:31:10Z", "author": "MDQ6VXNlcjQ3OTExNw=="}, {"message": "Yes, thanks for the review. But the NNAPI >=  1.2 check was already done (next line to the Op version check, see [1]). So I guess no further checking needed. Because the split op is not in NNAPI till 1.2.\r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc#L2199-L2200", "timestamp": "2020-02-13T01:21:14Z", "author": "MDQ6VXNlcjMzOTU5OTg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NjYxMjMwOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/34882#discussion_r356612309", "comments": [{"message": "Are all loop fusions supported? Note that loop fusions may contain non-element-wise ops.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/gpu/gpu_fusible.cc#L183", "timestamp": "2019-12-11T13:57:39Z", "author": "MDQ6VXNlcjM5OTQ3OTIx"}, {"message": "In terms of functionality, non-element-wise ops are fine to be supported. The only requirement for the computation itself should be that the fused computations have the same number of outputs. Of course, performance is another consideration. But, if the ops are in loop fusions, the code generation quality should remain the same after horizontal fusion.\r\n\r\nMore info: I actually also tried horizontal fusion to fuse _input_ fusions. It works functionally but the issue is performance. One case is input-fusible reduces; if we horizontally fuse them, the code generation quality is worsen because it becomes slice input fusion and we then use the elemental code gen for reduces (instead of the special code gen of, e.g., tiled reduces).", "timestamp": "2019-12-11T23:51:36Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}, {"message": "> The only requirement for the computation itself should be that the fused computations have the same number of outputs.\r\n\r\nInteresting, which non-element-wise ops that satisfy this property? I can think of bitcast, reshape, reverse, and transpose. Is that right? Do reshapes and transposes make sense for horizontal fusion?", "timestamp": "2019-12-19T13:59:56Z", "author": "MDQ6VXNlcjM5OTQ3OTIx"}, {"message": "They are all fine as long as they are in the kLoop fusion.\r\n\r\nIn other words, we rely on _all_ the concatenated outputs in the (slice) input fusion to have the same shape, so that the kernel generation can use this shape as kernel launch dim. To ensure the concatenated outputs having the same shape, we currently rely on two requirements--(a) the fused computations are kLoop fusion and (b) the fused computations have the same number of outputs.\r\n\r\nThen, it guarantees that the following two concatenated outputs have the same shapes, assuming we have two fused kLoop computations, each having two outputs (`o1` and `o2`):\r\n`concatenate(computation1.o1, computation2.o1)`\r\nand `concatenate(computation1.o2, computation2.o2)`\r\n\r\n\r\n\r\n\r\n", "timestamp": "2019-12-20T02:15:32Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NjYyMjI3Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/34882#discussion_r356622273", "comments": [{"message": "Is the rationale here that the kernel may become compute bound?", "timestamp": "2019-12-11T14:16:04Z", "author": "MDQ6VXNlcjM5OTQ3OTIx"}, {"message": "Yes.", "timestamp": "2019-12-11T23:59:14Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NjYyMzIwNw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/34882#discussion_r356623207", "comments": [{"message": "Can you say why it may be illegal. This is about cycles, right?", "timestamp": "2019-12-11T14:17:50Z", "author": "MDQ6VXNlcjM5OTQ3OTIx"}, {"message": "Yes. Will make it explicit by adding some comments.", "timestamp": "2019-12-11T23:59:41Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NjYyNTA3Mg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/34882#discussion_r356625072", "comments": [{"message": "Did you see FusionWouldBeTooLarge? Would this be useful here too?\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/gpu/gpu_fusible.cc#L298", "timestamp": "2019-12-11T14:21:17Z", "author": "MDQ6VXNlcjM5OTQ3OTIx"}, {"message": "I have considered it but there are some difficulties I see:\r\n1. The horizontally fused kernel is much larger (e.g., in terms of the number of instructions) than the vertical fusions. So, they need different metrics.\r\n2. We consider fusing many computations at a time, while `FusionWouldBeTooLarge` considers two fusion instructions at a time.", "timestamp": "2019-12-12T00:03:53Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1OTg3NzEwNg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/34882#discussion_r359877106", "comments": [{"message": "How specific are these thresholds to a GPU architecture?", "timestamp": "2019-12-19T14:09:44Z", "author": "MDQ6VXNlcjM5OTQ3OTIx"}, {"message": "I'd be surprised if they vary a lot across architectures.\r\n\r\nThere are two parts of launch overhead--the host and device parts. The host part is the time spent on the host CPU to enqueue the job to GPU, and the device part is for the kernel to be executed by SMs (i.e., there is a minimum latency no matter how small the kernel is). The device part is usually ~2-3us and the host is ~5us on my machines (GV100 and Titan V).\r\n\r\nI just searched online and found some nice descriptions about CUDA kernel launch overhead on [stackoverflow](https://stackoverflow.com/questions/27038162/how-bad-is-it-to-launch-many-small-kernels-in-cuda). It is a 2014 post and the cited launch overheads (10us for host and 3-4us for device) are not too far away from my aforementioned numbers. So, I think kernel launch overheads should not vary too much across archs.\r\n\r\nKernel computation latency is another dimension. Theoretically, if we run on a slower GPUs, we may see longer computation latency given the same kernel. However, even if the computation latency is (slightly) longer than the launch latency, it does not mean horizontal fusion of kernels will slow them down since the parallelism is retained/increased and the horizontal fusion does not introduce much overhead to the fused kernel (assuming absence of divergence and memory non-coalescing).", "timestamp": "2019-12-20T01:58:54Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1OTg3OTU3MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/34882#discussion_r359879571", "comments": [{"message": "The same number of instructions could still result in control divergence, right? It may be worth pointing out that this is a heuristic.", "timestamp": "2019-12-19T14:14:41Z", "author": "MDQ6VXNlcjM5OTQ3OTIx"}, {"message": "Right. Revised the comments to make it clearer.", "timestamp": "2019-12-20T02:03:51Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjYzNzgxMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/34882#discussion_r362637812", "comments": [{"message": "I assume SPARSE is not really used (for GPU at least). Let me know if my understanding is not accurate.\r\n\r\nIf the format is SPARSE, we should return false too? as the backend cannot really support it?", "timestamp": "2020-01-02T21:32:43Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NjcxNDI0MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/34973#discussion_r356714241", "comments": [{"message": "Do you know what the semantic is when sorted is false? TF API doc doesn't say anything about that, that means it can be in any order? If that's the case, sorted order is also a valid order for sorted=false?", "timestamp": "2019-12-11T16:50:13Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "My guess is that the `k` returned values are sorted by index in case of `sorted=False`.\r\nLet me see if I can find that quickly in the op definition.", "timestamp": "2019-12-11T17:39:09Z", "author": "MDQ6VXNlcjcwNzY5NjE="}, {"message": "Yeah, I think they are sorted by index according to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/topk_op.cc#L190-L207\r\n", "timestamp": "2019-12-11T17:50:35Z", "author": "MDQ6VXNlcjcwNzY5NjE="}, {"message": "Thanks, but is that part of the API semantic? I could not find anything in https://www.tensorflow.org/api_docs/python/tf/math/top_k saying that. If that's not an API guarantee, it means any order is fine?\r\n\r\nFor example, did you see not fixing this causing any problem for any models?", "timestamp": "2019-12-11T18:16:24Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "You are right, looks like there is no guarantee in the API in case of `sorted=False`.\r\n\r\nI didn't find any model failing. I just see that TensorRT only supports `sorted=True`, and I thought we should check for that.", "timestamp": "2019-12-11T18:27:00Z", "author": "MDQ6VXNlcjcwNzY5NjE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1OTE1NDk3Mg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/35207#discussion_r359154972", "comments": [{"message": "What happen if height/width_diff is 0? Please comment.\r\nAlso, is it possible to get negatives?", "timestamp": "2019-12-18T05:11:55Z", "author": "MDQ6VXNlcjMxNzQzNTEw"}, {"message": "Done", "timestamp": "2019-12-19T01:09:25Z", "author": "MDQ6VXNlcjcwNzY5NjE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODEyMjM5MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/35985#discussion_r368122391", "comments": [{"message": "What is AddLSTM?", "timestamp": "2020-01-17T20:27:13Z", "author": "MDQ6VXNlcjE0NTA2MTQ="}, {"message": "The present version needs scales to be POT as they are for LSTM. This PR extends this version to include general scales as for 8-bit. I renamed the current version to AddLSTM to indicate that this function is specific to LSTM.", "timestamp": "2020-01-20T10:30:26Z", "author": "MDQ6VXNlcjE1MTMzMzY="}, {"message": "I am not sure I understand, this Add and Sub operation are specific to LSTM? \r\n\r\nHow are they created in the TFLite graph. If possible we should avoid having unfused versions of add and sub that are specific to LSTM.", "timestamp": "2020-02-06T21:39:06Z", "author": "MDQ6VXNlcjE0NTA2MTQ="}, {"message": "No, the existed function was named Add before, but it was only with POT scale. We needed a general version, so I added the implementation similar to 8-bit and renamed the existed function to AddLSTM to indicate that this function is used when scale is POT.\r\nThis is probably a bad name. ", "timestamp": "2020-02-07T09:15:46Z", "author": "MDQ6VXNlcjE1MTMzMzY="}, {"message": "Ok sorry for the delay, was figuring out what the right thing to do here was.\r\n\r\nThis is unfortunate that there was an exisiting version of the 16bit code was specific to POT.\r\n\r\nWe have to take extra care for versioning in this case. \r\n\r\nHere is what i propose:\r\n1) Add a new attribute for this operation called pot_scale. This attribute must default to True.\r\n\r\n2) Add your new code path under the case where pot_scale=false.\r\n\r\n3) when we use this operation in the converter and add the new op version, we will set the attribute to False. \r\n\r\nDoes that seem reasonable, and make sense?", "timestamp": "2020-03-18T20:15:18Z", "author": "MDQ6VXNlcjE0NTA2MTQ="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTM0MTM4Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/36101#discussion_r369341383", "comments": [{"message": "are you sure symmetric quantization is the only case?\r\n\r\ndidn't see the specifications: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/operator_property.cc#L772-L775", "timestamp": "2020-01-22T02:17:29Z", "author": "MDQ6VXNlcjM2MjQ3MTkz"}, {"message": "Hi @renjie-liu, \r\nThank you for the review.\r\n\r\nWe introduce this kernel for the quantization scheme: activations in 16-bit and weights in 8-bit. The range of 16-bit is quite large and it is sufficient, if we do only symmetric quantization. That's why we consider only symmetric quantization for 16-bit reference kernels.\r\nShould operator specifications reflect this case ?  Should we have a special \"restricted_value\" property for 16-bit that shows that zero point is zero in this case ?", "timestamp": "2020-01-22T10:18:51Z", "author": "MDQ6VXNlcjE1MTMzMzY="}, {"message": "For 16bit we are only supporting symmetric for now.", "timestamp": "2020-01-24T22:35:33Z", "author": "MDQ6VXNlcjE0NTA2MTQ="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTMyMjA2NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/36130#discussion_r385322064", "comments": [{"message": "``third_party/tensorflow/lite/kernels/pooling_test.cc:923:44: error: shifting a negative signed value is undefined`\r\n\r\ncan you please check above error ?", "timestamp": "2020-02-27T19:25:09Z", "author": "MDQ6VXNlcjQzOTcyNjA2"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjExOTYyOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/36664#discussion_r382119628", "comments": [{"message": "In fact, all shapes from actual_shapes have the same batch size, similar for cached_shapes, right? The checking here doesn't enforce this and can be misleading. Is there anything we can do to address this?", "timestamp": "2020-02-20T16:39:15Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Yes that is right. It is assumed that TRTEngineOp::VerifyInputShapes did enforce that. It is one of the [first checks](https://github.com/tensorflow/tensorflow/blob/554f16e9701a34399f45617ea90675f30d30e0d0/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc#L608) we do in TrtEngineOp. I have added a comment about this assumption. If you think it would be better to enforce this here, we could do that too.", "timestamp": "2020-02-20T22:38:04Z", "author": "MDQ6VXNlcjM2NzExMDY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODgzOTYwNQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/36838#discussion_r398839605", "comments": [{"message": "Can we just bump it up for all platforms?", "timestamp": "2020-03-26T19:33:33Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2MDQzNw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/36989#discussion_r384060437", "comments": [{"message": "The earlier examples already demonstrate how `from_tensor_slices` will slice the input tensor, what new information does this example add?", "timestamp": "2020-02-25T18:57:13Z", "author": "MDQ6VXNlcjM0NTUxNzc="}, {"message": "This example doesn't add new information. But since I am adding example in `from_tensors`, I have also added similar example in `from_tensor_slices` so that one can differentiate both.", "timestamp": "2020-02-26T08:05:43Z", "author": "MDQ6VXNlcjIwODQzNTk2"}, {"message": "Changed it as described in below comment.", "timestamp": "2020-02-26T08:29:19Z", "author": "MDQ6VXNlcjIwODQzNTk2"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDUzNjQxOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37074#discussion_r394536419", "comments": [{"message": "How does this work for CPU only jobs?", "timestamp": "2020-03-18T17:54:53Z", "author": "MDQ6VXNlcjMwODIxODg="}, {"message": "There will be no influence on CPU only jobs, because we only check ops that relate to the kernels profiled in \"/stream:all\", which are all of the gpu kernels. Also, the current profiling method on CPU only ops in tensorflow does not have the async problem, so it would be ok to leave them where they were.", "timestamp": "2020-03-19T05:47:30Z", "author": "MDQ6VXNlcjEwNDI4MzI0"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg4MzI4MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37101#discussion_r384883281", "comments": [{"message": "@rmlarsen do you know of a better way to tell if an op is a source or sink?", "timestamp": "2020-02-27T02:35:31Z", "author": "MDQ6VXNlcjY1MTAyMDM="}, {"message": "Talked to @rmlarsen offline. We don't think there's a great way of checking this. It might be possible to have some heuristics, e.g. an op that takes a DT_VARIANT as input and has no outputs, or vice versa. But I'm not sure if it's worth implementing this.\r\n\r\nHaving Python tests to catch this, in addition to the C++ tests, would be nice in case we change which op is used on the Python side. But this doesn't need to be done in this PR.", "timestamp": "2020-02-27T20:48:58Z", "author": "MDQ6VXNlcjY1MTAyMDM="}, {"message": "Added a TODO to the Python tests.", "timestamp": "2020-02-28T09:17:02Z", "author": "MDQ6VXNlcjM5NzkwOTY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzgwNDQwMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37260#discussion_r387804401", "comments": [{"message": "Could you expand on what exactly is being done here?", "timestamp": "2020-03-04T17:01:27Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "This isn't needed anymore, so I'll remove this.", "timestamp": "2020-03-04T20:44:07Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzgwNTAzNg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37260#discussion_r387805036", "comments": [{"message": "Is the branch worth it here? LLVM will optimize away the addition with zero.", "timestamp": "2020-03-04T17:02:34Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "This isn't needed anymore, so I'll remove this.", "timestamp": "2020-03-04T20:45:25Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzgwNzM3Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37260#discussion_r387807373", "comments": [{"message": "What's `2` here?", "timestamp": "2020-03-04T17:06:32Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzgxMTg2NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37260#discussion_r387811864", "comments": [{"message": "What's the TODO item here? Could you expand on what is not working and why?", "timestamp": "2020-03-04T17:14:42Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "I didn't fully investigate this yet. But there is problem in the optimization phase in LLVM.\r\nMy expectation is that the branch optimization are done before the vectorization optimization (as they are in the NVPTX/AMD back-end). Those optimization interact together. As both branch are similar, I think the end of both branch end up being the same. I suppose LLVM hoist that out of the branch. Then we do not have both loads in the same basic block. So they aren't optimized.\r\n\r\nThis is my current not validate hypothesis.\r\n\r\nBut this already vectorize 3 out of 4 loads and give speed up in my benchmark. So I think it is good to split all the changes and merge now this part that is working.", "timestamp": "2020-03-04T18:45:22Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}, {"message": "I do not to fix this TODO in this PR. Do you have an idea or a comment that would be more clear for you? Just \"TODO: make the missing vectorization happen\"?", "timestamp": "2020-03-04T18:46:45Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}, {"message": "OK I understand, usually we have a tracking bug or the googler owning each TODO, but I understand neither makes too much sense here yet.", "timestamp": "2020-03-05T01:24:16Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAyNTcxOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37260#discussion_r388025719", "comments": [{"message": "Do we need to be specific about what reduction is vectorized?", "timestamp": "2020-03-05T01:08:00Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "I suppose you mean to check what inside the to_apply() HloComputation of the reduction.\r\nIn theory yes. But it wasn't done before. I'm not sure it is worth doing it.\r\nSo I'll add a todo about this to let people know about this.", "timestamp": "2020-03-05T14:22:48Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}, {"message": "Maybe let's make it more specific: state what kind of reductions still prevent vectorization.", "timestamp": "2020-03-05T18:35:17Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "Reduction doesn't prevent vectorization. This is the point. The function MayPreventVectorization() return true when it may prevent vectorization. False when it doesn't.\r\n\r\nMaybe I could add a comment that tell reduction doesn't block vectorization or add a comment to the default case, that we suppose by default all operation block vectorization?", "timestamp": "2020-03-05T19:12:32Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}, {"message": "> Reduction doesn't prevent vectorization. This is the point\r\n\r\nI understand, I mean: could you specify in the comment explicitly what needs to be checked and why, e.g. \"return false only for row reductions which are ...\"?", "timestamp": "2020-03-07T01:35:16Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "This function is independent of the reduction being a row or a columns reductions. I updated it to be tell what to check.", "timestamp": "2020-03-09T14:29:39Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTkyODg3OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37277#discussion_r405928878", "comments": [{"message": "Why are are we not checking the iterator device placement in TF2 graph mode?", "timestamp": "2020-04-09T02:59:44Z", "author": "MDQ6VXNlcjEwNzIwNzk="}, {"message": "If I do check check device I obtain the error that the attribute `_device` does not exist in this scenario. Sounds like an issue to me but I didn't want to fix something disconnected from this PR.", "timestamp": "2020-04-09T18:04:23Z", "author": "MDQ6VXNlcjEwOTIzNTk5"}, {"message": "This is somewhat worrisome because if the test code cannot access `.device` that neither will non-test code and since the code you have modified will run in TF2 graph mode, I suspect that this is an issue.\r\n\r\n@alextp does it make sense that the `.device` attribute is no present in TF2 graph mode?", "timestamp": "2020-04-09T18:08:32Z", "author": "MDQ6VXNlcjEwNzIwNzk="}, {"message": "The Tensor.device attribute is present in tf2 graph mode. Or are we not getting the device of a tensor? Can you print the object which doesn't have the device attribute on a failure and paste the result here or in a gist?", "timestamp": "2020-04-09T18:13:31Z", "author": "MDQ6VXNlcjUwNjE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODA1NTM4NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37279#discussion_r388055385", "comments": [{"message": "why you need to change this implementation?\r\n\r\nwhat's the reason to hard code zero point & scale?", "timestamp": "2020-03-05T03:04:14Z", "author": "MDQ6VXNlcjM2MjQ3MTkz"}, {"message": "Hi @renjie-liu ,\r\nThis piece of code is used to quantize alpha. Originally the scale of alpha was chosen to be the same as input scale (`input->params.scale`). However, the input scale can vary a lot depending on the the distribution of the input. This can cause a lot of potential problems. For example, if the input scale is close to 1, and alpha_float = 0.1, then the quantization of alpha will truncate q_alpha to zero_point, which will introduce huge error.\r\n\r\nTherefore, I think it is more appropriate to choose the scale of alpha independent of input scale. The value of 200 was chosen because it is the largest within UINT8 limit and divisible by 100, since mostly alpha_float was chosen to be 0.1, 0.01, 0.05, etc. Choosing a number that is divisible by 100 minimize the loss of quantization.\r\n", "timestamp": "2020-03-05T03:38:38Z", "author": "MDQ6VXNlcjUxMDQ3MTk="}, {"message": "why input_scale close to 1 and alpha close to 0.1 will introduce huge error?\r\n\r\noutput_float = input_float > 0 ? input_float : input_float * alpha\r\n\r\ninput_float = (input_q - input_zp) * input_scale\r\n\r\noutput_float = (output_q - output_zp) * output_scale\r\n\r\n=>\r\nif input_q - input_zp > 0:\r\n(output_q - output_zp) * output_scale = (input_q - input_zp) * input_scale => \r\noutput_q = (input_q -input_zp) * (input_scale / output_scale) + output_zp\r\n\r\nif input_q - input_zp <= 0:\r\n(output_q - output_zp) * output_scale = (input_q - input_zp) * input_scale * alpha =>\r\noutput_q = (input_q -input_zp) * (input_scale / output_scale * alpha) + output_zp\r\n\r\nLooks to me we only need an additional set of output_multiplier & shift (we don't need q_alpha).\r\n\r\nDoes that makes sense?", "timestamp": "2020-03-05T04:19:15Z", "author": "MDQ6VXNlcjM2MjQ3MTkz"}, {"message": "Hi @renjie-liu \r\n\r\nThat makes sense... I pushed a new commit for your review. Thank you!", "timestamp": "2020-03-05T06:20:21Z", "author": "MDQ6VXNlcjUxMDQ3MTk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODA1NTc3Ng==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37279#discussion_r388055776", "comments": [{"message": "again, why you need to change the implementation?", "timestamp": "2020-03-05T03:05:49Z", "author": "MDQ6VXNlcjM2MjQ3MTkz"}, {"message": "This piece of code implements the quantized inference of Leaky ReLU.\r\n\r\nThe problem of the original implementation is that when `input_value >= 0`, the quantized input was directly transferred to output. This seems OK if the input scale and output scale are the same, since the UINT8/INT8 encoding will be the same if they represent the same float number. However, if the input scale and output scale are different, then the same UINT8/INT8 encoding will represent different float value between input and output. \r\n\r\nTherefore, it is important to transform the quantized input to the quantized output in a proper way. It is similar to how the negative part was implemented. The only difference is that the negative part has `alpha_value` multiplied. In the positive part, we need to multiply the input by quantized identity, which was passed into the function as `params.q_identity`.\r\n\r\nThe original test was implemented in a way such that input and output scale are different, thus it didn't catch the error.", "timestamp": "2020-03-05T03:48:28Z", "author": "MDQ6VXNlcjUxMDQ3MTk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTgyOTk0OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37438#discussion_r389829948", "comments": [{"message": "Why do you want to change this to a dict?", "timestamp": "2020-03-09T17:04:21Z", "author": "MDQ6VXNlcjEzMzI2NzU4"}, {"message": "AFAIK, the `in` operator works faster on sets (not dict) than lists. I can write a `timeit` test if you want, but this is already discussed in many places such as this [stackoverflow post](https://stackoverflow.com/questions/2831212/python-sets-vs-lists). Honestly, because the list is so short, the performance boost would be small, but I figured it wouldn't hurt to make this edit. Thanks!", "timestamp": "2020-03-09T17:37:00Z", "author": "MDQ6VXNlcjI1MzYwNDQw"}, {"message": "Thanks for the note! We don't need tests for this.", "timestamp": "2020-03-09T17:43:20Z", "author": "MDQ6VXNlcjEzMzI2NzU4"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY2MDY2MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37438#discussion_r430660660", "comments": [{"message": "Why is this needed? We are overriding the doc below right?\r\n\r\nSame for decode_predictions", "timestamp": "2020-05-26T19:38:07Z", "author": "MDQ6VXNlcjEzMzI2NzU4"}, {"message": "My bad, you're right. I just pushed a commit removing the unnecessary docs. Thank you for the review!", "timestamp": "2020-05-27T13:13:58Z", "author": "MDQ6VXNlcjI1MzYwNDQw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU4NzUxMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37624#discussion_r407587511", "comments": [{"message": "What do you think about just merging the `FuseDepthwiseConv2DWithBias` case with here by adding a string `\"None\"` (or something with similar meaning) to this list?", "timestamp": "2020-04-13T16:58:35Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjM5NDcxOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/37813#discussion_r396394719", "comments": [{"message": "I think the PR is meant to fix exactly this issue, is it not?", "timestamp": "2020-03-23T11:51:59Z", "author": "MDQ6VXNlcjI2NjI4NTQ3"}, {"message": "I'm really sorry about this... I just copied the test from break_statement_test.py for the reason I mentioned above and didn't notice the extra self.converted... Anyway, those two unnecessary tests are removed...", "timestamp": "2020-03-23T14:08:21Z", "author": "MDQ6VXNlcjEwNDI4MzI0"}, {"message": "Ah, I see. It strikes me that break_statement_test.py now has a comment that is stale, because the PR fixes the bug - feel free to update it as well.", "timestamp": "2020-03-23T14:26:53Z", "author": "MDQ6VXNlcjI2NjI4NTQ3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYwMjI5NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/38016#discussion_r424602294", "comments": [{"message": "Just curious. What does Ex mean here?", "timestamp": "2020-05-13T17:17:02Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "I have no idea :( It is defined in remapper.cc.", "timestamp": "2020-05-14T02:48:56Z", "author": "MDQ6VXNlcjUyNzY5MTgy"}, {"message": "I see. Thank you!", "timestamp": "2020-05-14T22:09:00Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDI0NTg4OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/38258#discussion_r404245889", "comments": [{"message": "I don't like the change here. Can we keep the original? If not, why? Can we document what the binary data is supposed to be?", "timestamp": "2020-04-06T16:58:46Z", "author": "MDQ6VXNlcjMyMzE5OQ=="}, {"message": "Hi, @mihaimaruseac .\r\n\r\nThis is not an issue about correctness. It is an issue about performance. \r\n\r\nIn tensorflow we have 2 ways to serialize tensor data in [protobuf](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.proto). \r\n\r\n- The first one is `repeated`. This method is to serialize data as many typed individual values. And it requires complex sterilization computation in dense tensor scenario.\r\n- The second one is `tensor_content` which stores dense tensor data as unserialized binary. As the document says, \"The purpose of this representation is to reduce serialization overhead during RPC call by avoiding serialization of many repeated small items.\"\r\n\r\nIn short, to achieve good performance, we should store dense tensor data in `tensor_content` in protobuf. However, the current API of `tf.make_tensor_proto` only store some very common types of data into `tensor_content`(not including `float16`).\r\n\r\nIn my case, I used the official ResNet50 model which uses `float16` tensors as input. And I got very expensive performance overhead in serialization(as what I said [here](https://github.com/tensorflow/tensorflow/pull/38258#issue-399389454)).\r\n\r\nI think we'd better let `tf.make_tensor_proto` to store `float16` data in `tensor_content` rather than `repeated int32 half_val`. \r\n\r\nBecause:\r\n1. **Performance**. By using `tensor_content`, we can reduce 75% serialization overhead in RPC. In my case, from 280ms to 60ms, which is a very great performance improvement.\r\n2. What's more. The origin `half_val` is also unreadable because it will regard the `float16` as `int`.\r\n3. **Memory efficient**. `repeated int32 half_val` stores `float16` with 16 bits padding, while `tensor_content` just stores the raw layout.\r\n\r\nHence, I think this PR can bring better performance by default in the RPC scenario and won't cause any drawbacks.\r\n\r\nI'm looking forward to your opinions.\r\n\r\n", "timestamp": "2020-04-06T17:39:12Z", "author": "MDQ6VXNlcjM4MDc0Nzc3"}, {"message": "But this is only a test and we're changing the test from checking behavior when `repeated half_val` values are used to checking behavior when `float16` is used.\r\n\r\nI think we should create a separate PR for the performance and enhancement of the proto and get that through a larger review, since it will need input from more people and multiple teams as well as changes throughout the base.", "timestamp": "2020-04-07T17:31:25Z", "author": "MDQ6VXNlcjMyMzE5OQ=="}, {"message": "Thanks for your reply. \r\n\r\nI modified the content of `_TENSOR_CONTENT_TYPES` and append it with `float16`. So I think it's not \"only a test\". And this will make `tf.make_tensor_proto` to store `float16` data in `tensor_content` rather than `repeated half_val`. As I changed the content of `_TENSOR_CONTENT_TYPES`, I also need to modify the test file, as the test result of `tf.make_tensor_proto` for `float16` was originally designed for `repeated half_val` other than `tensor_content`.\r\n\r\nIf you do not like this PR(though I think it only brings benefits), I'd like to recommend the users who met problems with serious serialization overhead to have a look at this PR. They can manually set the tensor buffer like `numpy.array.tostring()` into the `tensor_content` of tensor proto rather than using `tf.make_tensor_proto` who only set some common types into `tensor_content`. And be aware that the [official TensorFlow resnet50 NHWC model](https://github.com/tensorflow/models/tree/master/official/r1/resnet#pre-trained-model) used `float16` as input types and it will trigger the performance punishment if they use it with `tf.make_tensor_proto`.", "timestamp": "2020-04-08T02:48:38Z", "author": "MDQ6VXNlcjM4MDc0Nzc3"}, {"message": "This is `testHalf`, it should test that `make_tensor_proto` works with a `np.float16`\r\n\r\nThere was a miscomunication here. Once you added the `float16` support the output of this test changed. Now it makes more sense.\r\n\r\nThank you for the patience", "timestamp": "2020-04-08T22:34:30Z", "author": "MDQ6VXNlcjMyMzE5OQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTIyMTkwNw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/38968#discussion_r419221907", "comments": [{"message": "Does placing on the GPU work only w DALI? Would it work with non DALI input pipelines?\r\nI think as long as inside the `dataset_fn` you place the input ops etc on the GPU devices, things should work, even if there is a `with tf.device(\"cpu\")` wrapper here - which works in the general non DALI cases. \r\n", "timestamp": "2020-05-04T05:43:47Z", "author": "MDQ6VXNlcjE0MTA0ODU1"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA4NzI5Ng==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/39042#discussion_r418087296", "comments": [{"message": "This is an implicit API change where you now ask users to explicitly pass trainable_variables to the decorated function.\r\n\r\nCan you also edit the documentation? Can you add tests that show behavior which is broken without this explicit hatchet? Can you clarify what's the plan for code that creates variables when it's called?", "timestamp": "2020-04-30T15:15:30Z", "author": "MDQ6VXNlcjUwNjE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDQ3ODcyMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/39183#discussion_r420478721", "comments": [{"message": "What is the reason that we can't/don't-want-to do this for implicit batch mode, for all the non-batch dimensions?", "timestamp": "2020-05-06T00:06:44Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "We do not know what the batch size will be during conversion time. If we just omit squeezing batch dim, and squeeze the others that would be an incorrect op conversion for an input that has batch size 1. I have updated the PR description with this info. ", "timestamp": "2020-05-06T21:36:01Z", "author": "MDQ6VXNlcjM2NzExMDY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDk2NTMzNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/39764#discussion_r434965334", "comments": [{"message": "I don't see any references to `CUDNN_FMA_MATH` in the cuDNN 8 documentation. Maybe I don't have the latest documentation? Does `CUDNN_DEFAULT_MATH` now use tensor cores by default? If so, why doesn't `CUBLAS_DEFAULT_MATH` do the same?", "timestamp": "2020-06-04T03:00:46Z", "author": "MDQ6VXNlcjY1MTAyMDM="}, {"message": "In CUDNN 8 and CUDA 11, CUDNN_DEFAULT_MATH and CUBLAS_DEFAULT_MATH now both use TensorCores for fp16 operations.\r\n\r\nFor fp32 they differ because CUBLAS is used extensively in HPC applications that require full fp32 precision. So for FP32, cublas is opt-in for TensorCores while cudnn is opt-out.", "timestamp": "2020-06-08T16:22:23Z", "author": "MDQ6VXNlcjE4NzM2NTU="}, {"message": "The cuDNN documentation has been updated to include `CUDNN_FMA_MATH` and is now public. But [it still claims `CUDNN_DEFAULT_MATH` does not use tensor cores](https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMathType_t).\r\n\r\nIs the documentation still out of date? In any case, a comment would be helpful. E.g. \"In cuDNN 8, CUDNN_DEFAULT_MATH uses tensor cores, unlike in cuDNN 7\".", "timestamp": "2020-06-08T18:50:59Z", "author": "MDQ6VXNlcjY1MTAyMDM="}, {"message": "Good catch. This is wrong. The behavior of CUDNN_DEFAULT_MATH is as you say for float16 inputs, but DOES enable tensor cores for float32.", "timestamp": "2020-06-09T20:36:25Z", "author": "MDQ6VXNlcjE4NzM2NTU="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODEwNzU0Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/39799#discussion_r438107543", "comments": [{"message": "@rachellim Should this be changed for older versions which doesn't have an exclude_cols input?", "timestamp": "2020-06-10T13:08:24Z", "author": "MDQ6VXNlcjQ0MTU0MDc1"}, {"message": "Good catch, this should only be serialized if op_version_ == 2", "timestamp": "2020-06-10T17:21:49Z", "author": "MDQ6VXNlcjk1ODkwMzc="}, {"message": "I don't see this change, maybe you forgot to commit?", "timestamp": "2020-06-11T16:53:51Z", "author": "MDQ6VXNlcjk1ODkwMzc="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU0MDUzMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/40062#discussion_r433540531", "comments": [{"message": "Can you think of a way to show the effect of this change, such as modifying OpConverterTest.ConvertCombinedNMS?", "timestamp": "2020-06-01T23:26:23Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "I have a test case that shows the error, but it's a bit convoluted and complex to be packaged in a unittest. I'll try to see what I can do ;)", "timestamp": "2020-06-02T20:15:26Z", "author": "MDQ6VXNlcjEwOTIzNTk5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTcyNTQ2Ng==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/40064#discussion_r435725466", "comments": [{"message": "I'm still missing where is this carried covered in the code?", "timestamp": "2020-06-05T06:59:01Z", "author": "MDQ6VXNlcjMzNzIzMDA="}, {"message": "I see - sorry about that. This comment should have been just been on the check for validity as you originally commented. Fixed, thanks. ", "timestamp": "2020-06-05T07:04:41Z", "author": "MDQ6VXNlcjcwMDg0MDE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI5MzE2MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/40093#discussion_r435293161", "comments": [{"message": "Is this an accidental revert of a previous change?", "timestamp": "2020-06-04T14:16:52Z", "author": "MDQ6VXNlcjc1MjM5ODI="}, {"message": "that is intentional, as that fix does not seem to work 100% of time.  We are working on debugging it, but in the meantime wanted to remove the previous fix, and put a no_rocm tag on that test instead", "timestamp": "2020-06-04T14:40:36Z", "author": "MDQ6VXNlcjM2ODU4MzMy"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY1OTI3Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/40330#discussion_r439659273", "comments": [{"message": "I'm not sure I understand why we have Intel(R) here", "timestamp": "2020-06-12T22:00:46Z", "author": "MDQ6VXNlcjE5MjEwMjg4"}, {"message": "If it's not required (since this is all open source), I'd like to removed it and make two strings the same.\r\n\r\n@nammbash Can we use one string (without `Intel(R)`) for both?", "timestamp": "2020-06-12T23:08:41Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "@penpornk have to check internally on final conclusion. Will revert soon.(with or without changes)", "timestamp": "2020-06-13T18:12:29Z", "author": "MDQ6VXNlcjM4ODY5Njg1"}, {"message": "@penpornk  @vpirogov \r\nthe string is the same now. Need Re-review", "timestamp": "2020-06-16T19:04:41Z", "author": "MDQ6VXNlcjM4ODY5Njg1"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY1MjIwNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/40405#discussion_r439652204", "comments": [{"message": "What does this check guard against?", "timestamp": "2020-06-12T21:36:46Z", "author": "MDQ6VXNlcjEwNzIwNzk="}, {"message": "@jsimsa \r\nI'm not sure if the following code will make the map dataset have 2 outputs:\r\n```python\r\nds = ...\r\nds = ds.map(lambda x: x+1)\r\nds1 = ds.repeat()\r\nds2 = ds.take(10)\r\n```\r\nIf there will be no such issue, I'll remove the check.", "timestamp": "2020-06-13T02:30:49Z", "author": "MDQ6VXNlcjEwNDI4MzI0"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQyNDU0OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/40554#discussion_r446424549", "comments": [{"message": "Do all the strategies work? If not, how about adding a TODO in line 108 where distribution strategy is created?", "timestamp": "2020-06-26T21:45:27Z", "author": "MDQ6VXNlcjM2Mjg1NzYz"}, {"message": "I couldn't test with the TPU strategy and parameter_server strategy seems to need users to provide TF_CONFIG. So before, I just use it from the model garden team. Maybe I can delete these two and list them as TODO. What do you think? ", "timestamp": "2020-06-26T22:00:32Z", "author": "MDQ6VXNlcjIwMzg5MzY1"}, {"message": "Yeah, makes sense to me.", "timestamp": "2020-06-26T23:25:06Z", "author": "MDQ6VXNlcjM2Mjg1NzYz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkzOTg3NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/40557#discussion_r443939874", "comments": [{"message": "What was the lint error here?", "timestamp": "2020-06-23T03:24:50Z", "author": "MDQ6VXNlcjI2NjI4NTQ3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwOTEwMA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/40596#discussion_r442509100", "comments": [{"message": "Would go/ifthisthenthatlint work here?", "timestamp": "2020-06-18T21:20:31Z", "author": "MDQ6VXNlcjExNzQzNzg="}, {"message": "Done", "timestamp": "2020-06-18T23:53:49Z", "author": "MDQ6VXNlcjY1MTAyMDM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk0MjQ5MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/40601#discussion_r443942491", "comments": [{"message": "Not sure why this is included in this PR.", "timestamp": "2020-06-23T03:36:10Z", "author": "MDQ6VXNlcjUxMTg4ODE="}, {"message": "```suggestion\r\n    return value\r\n```\r\nIt shouldn't be, pushed to the wrong branch. Sorry for the unnecessary delay. ", "timestamp": "2020-06-23T18:25:24Z", "author": "MDQ6VXNlcjUxMzQ1NTQx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAzMDcxNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/40621#discussion_r443030714", "comments": [{"message": "What items get filtered by this? can u provide an example here?", "timestamp": "2020-06-19T20:27:51Z", "author": "MDQ6VXNlcjUxMTg4ODE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA4ODgwMA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/40993#discussion_r454088800", "comments": [{"message": "Any reason to convert the output to a tensor?", "timestamp": "2020-07-14T04:14:35Z", "author": "MDQ6VXNlcjUxMTg4ODE="}, {"message": "Yes, because the return values of the `_get_shape_tuple` function will be used within `nest.map_structure` calls and if they were a list or tuple the `nest.map_structure` call would apply the specified function to every element of the list. However, the provided functions should be applied to every shape (as a whole) and not to every element of the shape. Therefore, the output is converted to a tensor.", "timestamp": "2020-07-17T18:27:53Z", "author": "MDQ6VXNlcjU4MzI2ODMx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2Nzc5OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/41249#discussion_r453867798", "comments": [{"message": "why pylint is here? is it necessary?", "timestamp": "2020-07-13T19:04:19Z", "author": "MDQ6VXNlcjM2Mjg1NzYz"}, {"message": "If we don't add it, the `ubuntu_sanity` check will show the class not callable. But this class can be used. So I add this comments. Also, I ran locally and it works.", "timestamp": "2020-07-13T19:16:55Z", "author": "MDQ6VXNlcjIwMzg5MzY1"}, {"message": "SGTM.", "timestamp": "2020-07-14T01:33:34Z", "author": "MDQ6VXNlcjM2Mjg1NzYz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDUwNDk3Nw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/41249#discussion_r454504977", "comments": [{"message": "nit: exp_per_sec, epochs.\r\n\r\nHow about distribution_strategy? This is also useful info. Can you return it as `extras` as well? ", "timestamp": "2020-07-14T16:59:59Z", "author": "MDQ6VXNlcjM2Mjg1NzYz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY5NTEzNQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/41259#discussion_r452695135", "comments": [{"message": "Can you explain why we need these? Does the code rely on 1k threads per block as the upper limit, which only holds for CUDA but not HIP?", "timestamp": "2020-07-10T08:18:22Z", "author": "MDQ6VXNlcjc1MjM5ODI="}, {"message": "ROCm 3.5+ (more specifically the hip-clang compiler) assumes a default value of 256 (for max threads per block) for GPU kernels, in case where that value is not explicitly specified via the `__launch_bounds__ `attribute. \r\n\r\nIf such a kernel (which has no `__launch_bounds__ `attribute specified) is called at runtime with a threads_per_block value greater than 256, it is possible to run into some undetermined behaviour. This will be changed to become a runtime error instead (in a forthcoming ROCm release).", "timestamp": "2020-07-15T02:22:28Z", "author": "MDQ6VXNlcjM2ODU4MzMy"}, {"message": "I see. Thanks for the explanation.", "timestamp": "2020-07-23T06:58:27Z", "author": "MDQ6VXNlcjc1MjM5ODI="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEyMTkxMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/41515#discussion_r457121911", "comments": [{"message": "I guess this was just for easier testing? Although we also want to flip the flag soon for cuda devices, there are still some blockers for that, for example it seems we still use the old kernels in some tensorflow codepaths even when enabling this flag, and then there can be small numeric differences in the results. But maybe this is not such a big deal, because these differences would also show up when running on different backends. We are still discussing this internally whether it is really a blocker, and what we can do about it.", "timestamp": "2020-07-20T07:16:21Z", "author": "MDQ6VXNlcjE0MzA5Nzcy"}, {"message": ">I guess this was just for easier testing?\r\n\r\nyes. In the long run, the expectation would be that this flag would have the same default value in ROCm and CUDA.", "timestamp": "2020-07-20T12:12:03Z", "author": "MDQ6VXNlcjM2ODU4MzMy"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY2MTY0MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/41641#discussion_r466661641", "comments": [{"message": "Can you bump this to VLOG(1)?", "timestamp": "2020-08-06T20:16:25Z", "author": "MDQ6VXNlcjc1MjM5ODI="}, {"message": "Yes", "timestamp": "2020-08-06T23:26:03Z", "author": "MDQ6VXNlcjEyMjA1NDI5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQxODA3MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/41734#discussion_r462418071", "comments": [{"message": "I am unsure about this change. What is the warning that this handles?", "timestamp": "2020-07-29T16:10:14Z", "author": "MDQ6VXNlcjMyMzE5OQ=="}, {"message": "```\r\ntensorflow/compiler/jit/extract_outside_compilation_pass.cc:1807:56: warning: comparison of integer expressions of different signedness: 'int' and 'std::vector<tensorflow::NodeDefBuilder::NodeOut>::size_type' {aka 'long unsigned int'} [-Wsign-compare]\r\n 1807 |     TF_RET_CHECK(e->dst_input() >= 0 && e->dst_input() < inputs.size());\r\n      |                                         ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~\r\n```\r\nleft is `int`, right is `size_type` redeclared as `int` for resolution.", "timestamp": "2020-07-29T20:31:34Z", "author": "MDQ6VXNlcjY1Nzc3MTk2"}, {"message": "But it will be the same warning if you extract the exact same comparison above. I think you need a `static_cast` on one of the terms. Then the `input_size_check` type should be `bool`", "timestamp": "2020-07-29T21:12:57Z", "author": "MDQ6VXNlcjMyMzE5OQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTAwNTQyMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/41749#discussion_r461005421", "comments": [{"message": "This looks unrelated?", "timestamp": "2020-07-27T16:12:57Z", "author": "MDQ6VXNlcjMyMzE5OQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQyMTMwOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/41749#discussion_r462421308", "comments": [{"message": "Unsure about this change. What is the warning here?", "timestamp": "2020-07-29T16:15:10Z", "author": "MDQ6VXNlcjMyMzE5OQ=="}, {"message": "Here auto seems to assign the objects being compared different types.\r\nMy attempt was to manually set the type of one of the two objects to resolve the warning.\r\n\r\nI could use `int64_t` if `long int` is less appropriate. ", "timestamp": "2020-07-29T20:13:25Z", "author": "MDQ6VXNlcjY1Nzc3MTk2"}, {"message": "`int64_t` is better", "timestamp": "2020-07-29T20:16:35Z", "author": "MDQ6VXNlcjMyMzE5OQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ1MTY0MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/41791#discussion_r462451641", "comments": [{"message": "why is this needed?", "timestamp": "2020-07-29T17:04:26Z", "author": "MDQ6VXNlcjM5MDEyMDA="}, {"message": "Just to make sure the variable was initialized at object creation, since it only happens in the `input_signature is not None` case. In theory, I don't think the input_signature attribute changes after a Function object is initialized, and it doesn't look like we are doing any error handling if asserts fail, so I can remove if it feels extraneous. Thoughts?", "timestamp": "2020-07-29T17:21:52Z", "author": "MDQ6VXNlcjIxMDQ2MDU5"}, {"message": "i think you can remove it, it is extra. ", "timestamp": "2020-07-29T18:32:03Z", "author": "MDQ6VXNlcjM5MDEyMDA="}, {"message": "Done", "timestamp": "2020-07-29T18:36:59Z", "author": "MDQ6VXNlcjIxMDQ2MDU5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDA3MjgxMA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/41880#discussion_r470072810", "comments": [{"message": "what happened to the earlier approach of passing _eager_only_test_objects containing ragged tensors as parameters to the existing tests? If that doesn't work for some reason, I would like to understand why before duplicating the test body.", "timestamp": "2020-08-13T16:23:23Z", "author": "MDQ6VXNlcjM0NTUxNzc="}, {"message": "@aaudiber , there was an internal build issue with the previous approach where we were passing the `_eager_only_test_objects` to the `@combinations` decorator with the eager test base.\r\n\r\n```\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: RaggedConstant/Const:0\r\n```\r\n\r\nI was assuming that the ragged test cases coming from the eagerly initialized test base was causing this issue.\r\nAnd, since the tests pass in both the scenarios (with and without `@combinations`  in a local env) I wanted to cross-check with the approach where we don't use `@combinations`. (specific to RaggedTensors)\r\n\r\nLet me know if we can approach this differently.", "timestamp": "2020-08-13T18:38:16Z", "author": "MDQ6VXNlcjI1MzkzOTQ1"}, {"message": "If you write the tests using @combinations, I can run the test internally to understand the problem.", "timestamp": "2020-08-13T18:50:38Z", "author": "MDQ6VXNlcjM0NTUxNzc="}, {"message": "addressed in the new commit", "timestamp": "2020-08-13T19:14:13Z", "author": "MDQ6VXNlcjI1MzkzOTQ1"}, {"message": "@aaudiber, did you get a chance to check the internal tests?", "timestamp": "2020-08-20T05:15:12Z", "author": "MDQ6VXNlcjI1MzkzOTQ1"}, {"message": "It seems that the error only happens when using ragged tensors in TF1. Can you exclude that test combination from the test? Currently these compression ops are only used in TF2 code anyway.", "timestamp": "2020-08-20T06:17:22Z", "author": "MDQ6VXNlcjM0NTUxNzc="}, {"message": "addressed in the commit", "timestamp": "2020-08-20T06:37:52Z", "author": "MDQ6VXNlcjI1MzkzOTQ1"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0MzY0Mg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/41916#discussion_r466743642", "comments": [{"message": "Is it a good idea to do that? Reduction in floating point numbers might yield different numbers.\r\n@sanjoy WDYT?", "timestamp": "2020-08-06T23:45:27Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "I don't think this is reducing in floating point, it is just doing the final sqrt in floating point.", "timestamp": "2020-08-07T00:19:26Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "@sanjoy is right, it's the finalizer that processes output of reduction.", "timestamp": "2020-08-07T16:51:16Z", "author": "MDQ6VXNlcjExNjE1Mzkz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI0OTkwMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/41916#discussion_r467249902", "comments": [{"message": "Why did this need to change?  Earlier we were passing in the result of `ConvertElementType` right?", "timestamp": "2020-08-07T20:18:13Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "You're right. My original thought is to make codes unchanged in this part, so maybe we should do something like\r\n```cpp\r\n  auto input = xla::ConvertElementType(ctx->Input(0), type);\r\n  auto data = PreprocessInput(b, input);\r\n  ...\r\n  auto finalized = BuildFinalizer(b, input, reduce, xla_axes);\r\n```\r\nWhat do you think about this? Thanks!", "timestamp": "2020-08-07T20:36:45Z", "author": "MDQ6VXNlcjExNjE1Mzkz"}, {"message": "LGTM, except that I'd prefer s/input/converted_input/", "timestamp": "2020-08-07T20:45:38Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4OTczMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/41981#discussion_r464589732", "comments": [{"message": "Did the error message change?  If so, what did it change to.  If not, why change this test to not check the error message?  (Similar question for errors below)", "timestamp": "2020-08-03T18:30:06Z", "author": "MDQ6VXNlcjU3OTAzNDg="}, {"message": "This was [changed by request](https://github.com/tensorflow/tensorflow/pull/37400#issuecomment-598952789) of other reviewer. It looks less informative to me too, yet I don't think we should be changing it back. It doesn't look to me that parsing the error string is the right way to communicate the exact error here. It increases the fragility of tests (with immediate example: your proposition below to slightly change the format of exception string. Could break tests if they relied on parsing the error string). The better way to communicate error here IMHO would be to derive the custom exception class from InvalidArgumentError with something like `expected_signature` and `argument_signature` that hold the incompatible signatures in question (or something like that). Could be done as a standalone refactoring PR (could be also useful throughout the code in situations like this).", "timestamp": "2020-08-05T14:22:24Z", "author": "MDQ6VXNlcjIyNTY0Mg=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzNDIzOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/42019#discussion_r466034239", "comments": [{"message": "Can you be sure that the keys are not in sort order? IIUC this test correctly, it would pass if this dict was simply flattened?", "timestamp": "2020-08-05T22:14:54Z", "author": "MDQ6VXNlcjI2NDkxMDk2"}, {"message": "You are right, this test is not sensitive to that. I will change the order. ", "timestamp": "2020-08-05T23:22:04Z", "author": "MDQ6VXNlcjE5OTYzMTA5"}, {"message": "To better differentiate correct passing dict input, I made one of the input in different size so that it would error out if placed incorrectly.", "timestamp": "2020-08-06T00:12:17Z", "author": "MDQ6VXNlcjE5OTYzMTA5"}, {"message": "Can we make all 3 inputs different sizes? As it is we can still silently pass during failure if the two same-sized inputs are swapped.", "timestamp": "2020-08-06T15:37:16Z", "author": "MDQ6VXNlcjI2NDkxMDk2"}, {"message": "Thanks for the suggestion! Now the 3 inputs all have different sizes.", "timestamp": "2020-08-06T15:55:22Z", "author": "MDQ6VXNlcjE5OTYzMTA5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1MjM3Nw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/42019#discussion_r466352377", "comments": [{"message": "Can we have an example of how this dataset is expected to be formatted?", "timestamp": "2020-08-06T11:41:02Z", "author": "MDQ6VXNlcjE3NjI0NzU5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg2NjcyNQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/42074#discussion_r468866725", "comments": [{"message": "what's the error message we are expecting here?", "timestamp": "2020-08-11T21:10:15Z", "author": "MDQ6VXNlcjUxMTg4ODE="}, {"message": "Expecting `Unable to create file`. I'm changing this to `assertRaisesRegex`.", "timestamp": "2020-08-11T22:45:32Z", "author": "MDQ6VXNlcjE5OTYzMTA5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDI3MTcyMA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/42130#discussion_r470271720", "comments": [{"message": "why only 1 iter is supported? We've discussed this, and the fix is to build the model once but train it multiple iterations.", "timestamp": "2020-08-13T21:57:19Z", "author": "MDQ6VXNlcjM2Mjg1NzYz"}, {"message": "Yeah, I changed the iterations my code locally and want to figure the performance issue before update this PR. ", "timestamp": "2020-08-13T23:57:47Z", "author": "MDQ6VXNlcjIwMzg5MzY1"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3MzMxNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/42821#discussion_r480473314", "comments": [{"message": "Want to match the micro reference kernel and do the computation in double precision?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/0b175ea29563da11dcfed3c70fbb93beff36f118/tensorflow/lite/micro/kernels/mul.cc#L67-L69\r\n\r\nLite is different:\r\nhttps://github.com/tensorflow/tensorflow/blob/0b175ea29563da11dcfed3c70fbb93beff36f118/tensorflow/lite/kernels/mul.cc#L99-L100\r\n\r\nI'll let you decide since the inconsistency in on our side, whatever you prefer to avoid issues like this one:\r\nhttps://github.com/tensorflow/tensorflow/issues/42648\r\n\r\n", "timestamp": "2020-08-31T23:34:36Z", "author": "MDQ6VXNlcjI3ODk5NTg="}, {"message": "Good catch! I think we should match the micro reference kernel.", "timestamp": "2020-09-01T13:23:51Z", "author": "MDQ6VXNlcjU3OTMzODc="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTk0MzcyMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/43034#discussion_r549943722", "comments": [{"message": "Why is this true?  What prevents a compilation from running concurrently with backend destruction?  (Please add a comment.)", "timestamp": "2020-12-30T05:54:17Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "check is not needed. Memory allocator is stored in shared pointer, so even if Backend is destroyed, compilation will succeed.", "timestamp": "2021-01-12T08:22:39Z", "author": "MDQ6VXNlcjEwMDkxMDQ4"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTUzOTM1MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/43051#discussion_r485539350", "comments": [{"message": "Not clear why we care about cycles here. Please clarify the comment.", "timestamp": "2020-09-09T11:31:50Z", "author": "MDQ6VXNlcjM5OTQ3OTIx"}, {"message": "Revised. We simply should not have these comments here. They are confusing.", "timestamp": "2020-09-09T23:25:33Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTU1MTkyNw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/43051#discussion_r485551927", "comments": [{"message": "Why don't we know? Is it because the \"fusion too large\" heuristic will kick in at some time?", "timestamp": "2020-09-09T11:55:34Z", "author": "MDQ6VXNlcjM5OTQ3OTIx"}, {"message": "Right. It is because of the FusionWouldBeTooLarge constraint. I added the reason into the comments.", "timestamp": "2020-09-09T23:28:39Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjg5NDQyNg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/43320#discussion_r492894426", "comments": [{"message": "Is this really what clang-format wants?", "timestamp": "2020-09-22T17:00:42Z", "author": "MDQ6VXNlcjY5NDczOTE="}, {"message": "Not something I've touched. Surprisingly this is what clang-format likes \ud83e\udd14", "timestamp": "2020-09-22T17:22:27Z", "author": "MDQ6VXNlcjYzNzgyOTY1"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjE2MTU0NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/43850#discussion_r502161544", "comments": [{"message": "Is it always off by default? Should we retry with this strategy when we OOM?", "timestamp": "2020-10-09T03:14:25Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "The multiheap mode is an opt-in (and always off by default now) as in the PR. Our customers somehow are trained to feel fine with adding flags into their training scripts to use XLA (^^\").\r\n", "timestamp": "2020-10-09T19:22:27Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjE2MzMzMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/43850#discussion_r502163331", "comments": [{"message": "Could you explain what has changed here? Why do we have a loop instead of a single result? Because we might separate it across multiple allocations?", "timestamp": "2020-10-09T03:22:25Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "Right. After the change, the buffer assignment will produce multiple allocations. The loop iterates through allocations. I will add some comments.", "timestamp": "2020-10-09T19:24:54Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTI4NTU3OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/44022#discussion_r511285579", "comments": [{"message": "can we use utilities from tensorflow/core/util/tensor_format.h here?", "timestamp": "2020-10-24T03:15:32Z", "author": "MDQ6VXNlcjE5OTAwNzk="}, {"message": "Done. Thanks ", "timestamp": "2020-10-30T04:37:44Z", "author": "MDQ6VXNlcjcxNDIzMTc0"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk2MTY2NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/44540#discussion_r516961665", "comments": [{"message": "On GPU what happens if these constraints are violated?  Do we produce a bogus answer?  Or do crash in the CUDA kernel?\r\n\r\n(Please also add the answer as a comment.)", "timestamp": "2020-11-03T21:19:36Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "The kernel won't crash, just give bogus results. Comment added.", "timestamp": "2020-11-05T06:06:22Z", "author": "MDQ6VXNlcjM5NzkwOTY="}, {"message": "The kernel won't crash, just give bogus results.", "timestamp": "2020-11-08T10:55:02Z", "author": "MDQ6VXNlcjcyMDMwNjQy"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDM4MDY4Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/44712#discussion_r520380683", "comments": [{"message": "Just curious: Would this work on windows? And if not, is there a way to make it work? Not in this PR but in general.", "timestamp": "2020-11-10T08:38:19Z", "author": "MDQ6VXNlcjk3MjU4Mg=="}, {"message": "I do not know.\r\n\r\nROCm, and hence ROCm TF,  is currently only supported on the Linux platform, so I have had the need to look into whether or not this will work on Windows. The `clang-offload-bundler` utility being used here, is from the standard llvm install, so there is nothing ROCm specific about the utility itself.", "timestamp": "2020-11-11T02:12:14Z", "author": "MDQ6VXNlcjM2ODU4MzMy"}, {"message": "Thanks for the reply. Makes sense, lets leave it at linux for now then.", "timestamp": "2020-11-11T12:33:09Z", "author": "MDQ6VXNlcjk3MjU4Mg=="}, {"message": "`clang-offload-bundler` itself is a platform independent tool. It aggregates several binary objects for different architectures into one binary blob.\r\n\r\nIt does require a host-side object be inside the bundled blob. As we normally only produce GPU binaries in XLA or MLIR path, we populate a dummy host object just to tame `clang-offload-bundler`.\r\n\r\nAs ROCm runs strictly on the Linux platform, the current implementation from @deven-amd shall suffice for now.", "timestamp": "2020-11-11T15:34:00Z", "author": "MDQ6VXNlcjE2NzM1NzQ="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA0OTczMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/45010#discussion_r530049732", "comments": [{"message": "Not an expert on comiler flags, but it seems that this is not supported on all compilers. Could you add more flags taking abseil as a reference?\r\n\r\nhttps://github.com/abseil/abseil-cpp/blob/5d8fc9192245f0ea67094af57399d7931d6bd53f/absl/base/config.h#L452", "timestamp": "2020-11-25T01:29:05Z", "author": "MDQ6VXNlcjM0MjkwMDYz"}, {"message": "Yeah I can add some additional flag checks here. Even inside the TensorFlow codebase there have been a variaty of macro implementations to check endianness, but I think checking `__BYTE_ORDER__` is defined then checking the value is `__ORDER_BIG_ENDIAN__` should cover most compilers.", "timestamp": "2020-11-25T03:12:27Z", "author": "MDQ6VXNlcjY2MDI2MzY3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkyMTE0NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/45022#discussion_r528921144", "comments": [{"message": "Two branches are the same?", "timestamp": "2020-11-23T18:46:01Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "No, one use `arguments_inputs->size()` while the second use `arguments_buffer->size()`", "timestamp": "2020-11-23T21:19:50Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NTc0NTQzMw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/46172#discussion_r595745433", "comments": [{"message": "Is this needed?  Won't we have returned an error if any indices were out of range?", "timestamp": "2021-03-17T06:34:26Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "Good point, removed.", "timestamp": "2021-03-17T11:47:14Z", "author": "MDQ6VXNlcjM5NzkwOTY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3OTk0NjY4MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/46292#discussion_r579946680", "comments": [{"message": "just curious, why this test is disable?", "timestamp": "2021-02-22T02:53:53Z", "author": "MDQ6VXNlcjM2MjQ3MTkz"}, {"message": "Oh, that must be an oversight on my side. I'll re-enable it.", "timestamp": "2021-03-02T21:51:43Z", "author": "MDQ6VXNlcjI4NDA5MDE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjExMzU1NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/46376#discussion_r556113555", "comments": [{"message": "What is this?", "timestamp": "2021-01-12T21:41:01Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Leftover, I removed", "timestamp": "2021-01-12T22:37:21Z", "author": "MDQ6VXNlcjEwOTIzNTk5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjkyNDk5OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/46411#discussion_r556924998", "comments": [{"message": "do this in prepare?", "timestamp": "2021-01-13T22:42:48Z", "author": "MDQ6VXNlcjI3ODk5NTg="}, {"message": "done", "timestamp": "2021-01-15T21:33:00Z", "author": "MDQ6VXNlcjI3ODk5NTg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDIxODc1Mg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/46543#discussion_r564218752", "comments": [{"message": "I could be missing something, but why is removing something dependent on deprecating some API?  Do you mean to to say \"Can be removed when alternatives for API XYZ is available.\"?", "timestamp": "2021-01-26T05:06:56Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "This is meant to say that \"We can remove this when cudnnRNNForward*** and cudnnRNNForward***Ex are removed from the code since the new API doesn't need it.\"", "timestamp": "2021-01-26T21:04:58Z", "author": "MDQ6VXNlcjQwMDE0MjQ="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2Mzk5Mjg5MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/46614#discussion_r563992890", "comments": [{"message": "What do you think about rewriting this function using HLO matchers? (cf. pattern_matcher.h and the files which use it), it seems like it could be a lot smaller and a lot more readable?", "timestamp": "2021-01-25T19:38:35Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "Just took a look of pattern_matcher.h. It appears not a good fit here, because the ROOT Tuple instruction has a variable number of operands. It seems the pattern_matcher does not work well in this case?", "timestamp": "2021-01-25T21:48:54Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2Mzk5OTYxNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/46614#discussion_r563999614", "comments": [{"message": "Question: what does the codegen look like in this case? Is it also able to figure out it's a noop, fuse it all, and don't emit anything extra?", "timestamp": "2021-01-25T19:49:19Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "In the case of HorizontalLoopFusion, the concat is inserted intentionally to increase the parallelism degree (by combining several fusions). So the concat will still be generated in the codegen.\r\n\r\nThe concat is simply generated by the elemental emitter.\r\n", "timestamp": "2021-01-25T23:27:58Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDczODkyOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/46681#discussion_r564738928", "comments": [{"message": "remove? we aren't really using these.", "timestamp": "2021-01-26T18:32:04Z", "author": "MDQ6VXNlcjI3ODk5NTg="}, {"message": "Done.", "timestamp": "2021-01-26T22:39:14Z", "author": "MDQ6VXNlcjY5NDczOTE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NTg4MjQyMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/46749#discussion_r565882421", "comments": [{"message": "Can you explain why the neg op help here?", "timestamp": "2021-01-28T07:52:59Z", "author": "MDQ6VXNlcjEyMzUxNTAx"}, {"message": "It's somehow tricky. When there is no following ops (not limited to neg), temporary allocation with `kTfLiteArenaRw` type tends to re-claim the same memory across each evaluation and no other ops will modify values at that memory address (cause no other memory allocations take place). Therefore, the values of rhs are not polluted without neg op, and next evaluation can also get the same piece of memory, so the results are correct.\r\n\r\nhttps://colab.research.google.com/drive/1eFVszZZN-NcR64X_VHns0w-l3Vwci0CY?usp=sharing\r\n\r\nHere is the colab link from original post. Values of last tensor (temp tensor, transposed rhs) become\r\n\r\n```\r\n[[-51. -84.]\r\n [  3.   6.]\r\n [  7.   9.]]\r\n```\r\n\r\nwhere -51, -84 are the last two elements of `Identity` (output of neg op). While without neg op, everything works smoothly.", "timestamp": "2021-01-28T08:13:25Z", "author": "MDQ6VXNlcjExNjE1Mzkz"}, {"message": "Acked. Thanks for explanation.  May be it worths a comment then.", "timestamp": "2021-01-28T15:18:15Z", "author": "MDQ6VXNlcjEyMzUxNTAx"}, {"message": "Updated comments. See if it's better! Thanks!", "timestamp": "2021-01-28T18:39:54Z", "author": "MDQ6VXNlcjExNjE1Mzkz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzI1NTA1NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/46864#discussion_r573255054", "comments": [{"message": "What are these printfs for?", "timestamp": "2021-02-09T21:29:45Z", "author": "MDQ6VXNlcjY5NDczOTE="}, {"message": "added a comment", "timestamp": "2021-02-09T22:43:19Z", "author": "MDQ6VXNlcjI3ODk5NTg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NTUyMzE4NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/46965#discussion_r595523184", "comments": [{"message": "Since the frontend API is not enabled by default, can you revert the test change?", "timestamp": "2021-03-16T20:38:25Z", "author": "MDQ6VXNlcjExNTc0MzI="}, {"message": "Done.", "timestamp": "2021-03-17T02:33:54Z", "author": "MDQ6VXNlcjQwMDE0MjQ="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3NzI4ODA2NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/47121#discussion_r577288065", "comments": [{"message": "I'm not sure I've seen VLOG(0) before: what is the intended semantics? Same as VLOG(1)?", "timestamp": "2021-02-17T02:47:34Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "This is used in a few files. So this isn't a first in TF. I want that to always be printed. VLOG(0) does that.", "timestamp": "2021-02-23T16:15:41Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3ODU3MDE3MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/47215#discussion_r578570171", "comments": [{"message": "Is this the only place that use IsStaticShuffleCompatible? I wonder why we don't want to keep the routine here, such as just inlining the implementation or making it a lambda.", "timestamp": "2021-02-18T16:38:43Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Right now it is the only place, I have added a lambda here. In the future we might need to have more test like this before we `PrepareTensorForShape`.", "timestamp": "2021-02-22T12:49:38Z", "author": "MDQ6VXNlcjM2NzExMDY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjU5NDk3MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/47477#discussion_r612594970", "comments": [{"message": "This is a slightly surprising change - I just wanted to check that it was deliberate?", "timestamp": "2021-04-13T16:19:28Z", "author": "MDQ6VXNlcjE2MTQ1OQ=="}, {"message": "Yes that is intentional.  I saved 4 bytes during Init phase by not storing the zero point.  The correct zero point for output is checked during the Prepare phase.  So the zero point will always match kMaxT8 when calling this template.", "timestamp": "2021-04-13T22:11:03Z", "author": "MDQ6VXNlcjEzMTY5MTEy"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODAxNzk2OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/47580#discussion_r588017969", "comments": [{"message": "can we make additional improvement by turning this into Neon instructions?", "timestamp": "2021-03-05T04:00:57Z", "author": "MDQ6VXNlcjMyMDMwNTk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTE5MTQ5OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/47621#discussion_r589191499", "comments": [{"message": "Is this needed since we only support OpenCL?", "timestamp": "2021-03-08T06:16:34Z", "author": "MDQ6VXNlcjI5MDg1MDU="}, {"message": "Yes. Without this fix default behaviour is crash in case opencl failed to initialize. ", "timestamp": "2021-03-08T06:41:42Z", "author": "MDQ6VXNlcjE0MzI3MA=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTg1NzIyMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/47650#discussion_r589857221", "comments": [{"message": "What is the purpose of this variable?", "timestamp": "2021-03-09T00:49:16Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzk5MDE3Nw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/47754#discussion_r593990177", "comments": [{"message": "How is this change related with kernel testing?", "timestamp": "2021-03-15T00:42:35Z", "author": "MDQ6VXNlcjI5MDg1MDU="}, {"message": "@terryheo \r\nI show the detail in the comment.\r\nhttps://github.com/tensorflow/tensorflow/pull/47754/commits/582bf0d3ac33fc10156f737c0d42f3adee54409a\r\n\r\nIf we build with cmake, we can't generate the mutable schema header for the original UpdateOpVersion() call. So, we can't call the UpdateOpVersion() in cmake build.\r\n\r\nIf I change the UpdateOpVersion() implementation using the pack/unpack with immutable schema header, I need the additional --force-empty-vectors flag for the immutable header. Otherwise, some vector field will become null-ptr if the vector size is 0 for serialization.\r\nI don't regenerate the immutable schema header here, so I need to relax the buffers checking in interpreter_builder.cc. Some kernel tests doesn't have buffer. The buffer field will become null-ptr if the model comes from my new UpdateOpVersion() pack/unpack implementation.", "timestamp": "2021-03-15T02:32:30Z", "author": "MDQ6VXNlcjU4NDI2ODE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5OTIxNTQ1OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/47828#discussion_r599215458", "comments": [{"message": "Why is this a virtual function?", "timestamp": "2021-03-23T02:41:55Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "Good question. The ComputBNScale is only applicable to the MklFusedConvOp, but with the current design of mkl_conv_ops, the calling of ComputeBNScale is common to all the subclasses. I need to make this function virtual, so the computation is only invoked for MklFusedConvOp, and for the rest, no computation is invoked. ", "timestamp": "2021-03-23T17:30:33Z", "author": "MDQ6VXNlcjQyMTU2NDIw"}, {"message": "Thank you for the clarification! Then I think the signature of this function should be\r\n```c++\r\nvoid ComputeBNScale(...) override {...}\r\n```\r\ninstead, since it's in a derived class. I'll fix this internally.", "timestamp": "2021-03-24T04:46:13Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "thanks!", "timestamp": "2021-03-24T23:53:56Z", "author": "MDQ6VXNlcjQyMTU2NDIw"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzg1NTI3Ng==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/47900#discussion_r597855276", "comments": [{"message": "Why is TFE_GetMemoryInfo changing? Is this just a merge conflict?", "timestamp": "2021-03-19T17:26:03Z", "author": "MDQ6VXNlcjM3MzEwMjU="}, {"message": "It is modified by clang-format", "timestamp": "2021-03-20T02:05:33Z", "author": "MDQ6VXNlcjYzNDY4NTM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxOTgzMDkzMA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/47974#discussion_r619830930", "comments": [{"message": "What do you mean by \"or any\"?", "timestamp": "2021-04-25T15:10:11Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "Added comment explaining. When `indices` is provided, the first dim of `input_vec` is indexed indirectly and has no specific size requirement (as long as it covers the values in `indices`).", "timestamp": "2021-04-26T12:07:46Z", "author": "MDQ6VXNlcjM5NzkwOTY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxOTgzMTEzMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/47974#discussion_r619831132", "comments": [{"message": "Isn't this also handled in `SegmentReduceGPU`?", "timestamp": "2021-04-25T15:12:10Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "Good catch. I've removed the one in `SegmentReduceGPU` because the vectorized version should be faster.", "timestamp": "2021-04-26T12:07:50Z", "author": "MDQ6VXNlcjM5NzkwOTY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5OTc5NDMwNw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/47988#discussion_r599794307", "comments": [{"message": "When we reach here, layout can't be MIX. Am I right?", "timestamp": "2021-03-23T17:40:16Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Yes. Added a check to preferred_layout.", "timestamp": "2021-03-23T23:57:50Z", "author": "MDQ6VXNlcjM2NzExMDY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDk0OTcwOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48060#discussion_r600949709", "comments": [{"message": "Why do we want to change this? I though the previous way is more readable...", "timestamp": "2021-03-24T23:54:59Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Yes, the previous is more readable. But when we use [this to generate uppercase names](https://github.com/tensorflow/tensorflow/blob/9bc6793a6469be2582f660de575eded21d46df24/tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc#L503), then TRTEngineop constructor cannot use `ProfileStrategyFromName`.\r\n\r\nAlterantively `ProfileStrategyFromName` could be changed to lowercase the input string.", "timestamp": "2021-03-25T00:16:02Z", "author": "MDQ6VXNlcjM2NzExMDY="}, {"message": "Yes, let's add a line to ProfileStrategyFromName for this.", "timestamp": "2021-03-25T15:18:05Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Ok, I have restored the uppercase names here, and changed ProfileStrategyFromName.", "timestamp": "2021-03-25T16:47:37Z", "author": "MDQ6VXNlcjM2NzExMDY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYzMzE2NDg4Ng==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48125#discussion_r633164886", "comments": [{"message": "@mihaimaruseac let me know if this is the desired behavior. I realize that `file://` is the native filesystem, but maybe it should be treated like other ulr-like filesystems? What does `GFile(join(\"file://\", \"dir\", \"file.py\"))` expect?", "timestamp": "2021-05-17T00:45:35Z", "author": "MDQ6VXNlcjE3NTUwNzE="}, {"message": "I think it should be `\"file://dir/file.py\"`. It's similar to URLs", "timestamp": "2021-05-18T23:01:47Z", "author": "MDQ6VXNlcjMyMzE5OQ=="}, {"message": "easy enough change; and it simplifies the implementation/testing.", "timestamp": "2021-05-19T01:36:07Z", "author": "MDQ6VXNlcjE3NTUwNzE="}, {"message": "@mihaimaruseac I just pushed this change, please re-approve so CI runs when you get a chance \ud83d\ude03 ", "timestamp": "2021-05-20T05:01:02Z", "author": "MDQ6VXNlcjE3NTUwNzE="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwNTM3MTg4Nw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48193#discussion_r605371887", "comments": [{"message": "Where are you checking this condition and early exiting? (forgive me if I missed it)", "timestamp": "2021-04-01T04:43:50Z", "author": "MDQ6VXNlcjQ5NDIwOA=="}, {"message": "No I didn't. I will add the check in next round.", "timestamp": "2021-04-06T17:32:48Z", "author": "MDQ6VXNlcjY3Mzk1NDc5"}, {"message": "Oh actually we did. In legalize_tf.cc and legalize_tfl.cc, we do return failure() if matchPattern() fails.", "timestamp": "2021-04-06T19:41:24Z", "author": "MDQ6VXNlcjY3Mzk1NDc5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwOTEzMjc2NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48381#discussion_r609132764", "comments": [{"message": "That seems to be a layering violation, LaunchDimensions is meant to encapsulate how are we launching the kernel, it normally does not affect the codegen?", "timestamp": "2021-04-07T23:13:32Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "There is inter-dependency here. I removed that and added a long condition in parallel_loop_emitter.cc instead.", "timestamp": "2021-04-08T17:31:06Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYzMDQ5Mzc2NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48383#discussion_r630493765", "comments": [{"message": "Can you document what are the returned values?", "timestamp": "2021-05-11T19:59:12Z", "author": "MDQ6VXNlcjExNTc0MzI="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMDA2OTA2OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48414#discussion_r610069069", "comments": [{"message": "Sorry that I don't understand this comment. What is \"we only once\"?", "timestamp": "2021-04-08T20:19:04Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Hmm, the verb got deleted from the sentence. Fixed it.", "timestamp": "2021-04-08T22:57:01Z", "author": "MDQ6VXNlcjM2NzExMDY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYzMjg0MDM1OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48530#discussion_r632840358", "comments": [{"message": "Should this diff be here?", "timestamp": "2021-05-14T22:20:42Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "yes...thought you said\r\n\r\n>  Fixing unit test indeed seems preferable, since it's wrong to select an algorithm for the ROCm platform.\r\n\r\nand hence the change above", "timestamp": "2021-05-18T11:55:36Z", "author": "MDQ6VXNlcjM2ODU4MzMy"}, {"message": "@cheshire gentle ping", "timestamp": "2021-05-19T16:45:21Z", "author": "MDQ6VXNlcjM2ODU4MzMy"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNzM1OTI1MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48610#discussion_r617359251", "comments": [{"message": "I'm wondering if we can somehow mentioned on why there's changes on the implementation on the PR's description, previously we're using `backend` instead of `array_ops`.", "timestamp": "2021-04-21T09:33:09Z", "author": "MDQ6VXNlcjc0Mzk1OTA="}, {"message": "@irvifa \r\nCan you please elaborate on that?", "timestamp": "2021-04-21T09:35:23Z", "author": "MDQ6VXNlcjY0NDExMzA2"}, {"message": "Actually in the issue the user pointed out that some of the calls were deprecated. Also, backend also calls `array_ops` methods, so I called them directly.", "timestamp": "2021-04-21T09:37:02Z", "author": "MDQ6VXNlcjY0NDExMzA2"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyNDAzODE0OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48610#discussion_r624038148", "comments": [{"message": "Could you explain why we need to skip checks on the channel axis? Also please consider putting a comment before the if check.", "timestamp": "2021-04-30T17:17:41Z", "author": "MDQ6VXNlcjIyOTI1MDMx"}, {"message": "@chenmoneygithub \r\nThe channel axis is dependent on no. of filters, which are checked by the additions made in #48566 . ", "timestamp": "2021-04-30T17:26:26Z", "author": "MDQ6VXNlcjY0NDExMzA2"}, {"message": "Also the rest of the arguments for `conv_utils.conv_output_length` don't consider channels and batch size.", "timestamp": "2021-04-30T17:44:15Z", "author": "MDQ6VXNlcjY0NDExMzA2"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYzMTQyNDc2MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48806#discussion_r631424760", "comments": [{"message": "Haven't we just defined the lambda checking precisely this? Or more concretely: do we need the lambda at all then?", "timestamp": "2021-05-12T21:50:07Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "The lambda is needed because the absl::c_all_of() below requires a function form.\r\n\r\n`            absl::c_all_of(user->operands(), is_scheduled) &&\r\n            absl::c_all_of(user->control_predecessors(), is_scheduled)) {`\r\n\r\nIt is cleaner to also use the lambda here. Will update it.\r\n", "timestamp": "2021-05-13T19:12:15Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}, {"message": "Updated.", "timestamp": "2021-05-14T00:07:41Z", "author": "MDQ6VXNlcjEyMDE2MjA3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyNTY5NDAwOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48847#discussion_r625694009", "comments": [{"message": "What does being in use mean here?", "timestamp": "2021-05-04T11:09:22Z", "author": "MDQ6VXNlcjk3MjU4Mg=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NDM0Nzg5Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48883#discussion_r654347893", "comments": [{"message": "Why is this negated? If the useranges interfere, there can be no reuse, right?", "timestamp": "2021-06-18T11:14:41Z", "author": "MDQ6VXNlcjk3MjU4Mg=="}, {"message": "Yes, you are right. I'll add this to the comment and also move the negation into the method, because otherwise the name is misleading.", "timestamp": "2021-06-23T09:56:08Z", "author": null}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NDM1NjE1MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48883#discussion_r654356150", "comments": [{"message": "I am always confused why this is negated. If the reanges interfere, then no reuse is possible, no?", "timestamp": "2021-06-18T11:29:47Z", "author": "MDQ6VXNlcjk3MjU4Mg=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NTMwNTMxNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/48883#discussion_r665305314", "comments": [{"message": "Why is this needed?", "timestamp": "2021-07-07T12:03:09Z", "author": "MDQ6VXNlcjk3MjU4Mg=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYzMjc0OTI4NQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/49178#discussion_r632749285", "comments": [{"message": "What does this mean? What is wrong with passing use_gpu to `cached_session`, as was done before?", "timestamp": "2021-05-14T19:25:07Z", "author": "MDQ6VXNlcjY1MTAyMDM="}, {"message": "Maybe I'm mistaken, but from my experience of writing, running, and checking tests (and from looking at the `test_util` code), my understanding is that if you pass `use_gpu=False` into `self.session()` or `self.cached_session()` and run it on a GPU in eager mode, the ops will not be pinned to the CPU. This is because `self.session()` and `self.cached_session()` are only relevant in graph mode (they do nothing in eager mode). When we write test code that can be run in either graph or eager mode, there needs to be both a `self.session()` or `self.cached_session()` context and a separate, nested `test_util.device()` context.\r\n\r\nThe existing code is currently only running in eager mode, and, the way it's written, each case will be run twice on GPU if there is a GPU present (this is clearly not the intention of the test writer). The change in the PR makes it run each case on both CPU and GPU if there is a GPU present, but does not make it less compatible with graph mode (in case we want to run this test in graph mode).", "timestamp": "2021-05-17T22:17:01Z", "author": "MDQ6VXNlcjMzNTMyOTQx"}, {"message": "The next commit will remove the `self.cached_session()` and the comment on the next line. None of that is needed.", "timestamp": "2021-05-18T04:29:07Z", "author": "MDQ6VXNlcjMzNTMyOTQx"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0MDA3NTYwMA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/49178#discussion_r640075600", "comments": [{"message": "How incorrect are the gradients here and the next test? I am slightly worried there is a correctness problem, although the single class case is seems fairly unimportant.", "timestamp": "2021-05-26T19:51:42Z", "author": "MDQ6VXNlcjY1MTAyMDM="}, {"message": "Agreed, it is concerning, and the gradients are very different (if I remember correctly). I was already intending to change these tests so that they do check expected values for both deterministic and nondeterministic cases. The next commit will include that, so we'll have documentation in the test of the difference between the gradients for deterministic vs nondeterministic.", "timestamp": "2021-05-26T20:48:53Z", "author": "MDQ6VXNlcjMzNTMyOTQx"}, {"message": "After further investigation, it has been revealed that the gradients only mismatch between the nondeterministic and deterministic implementations when the labels vector is not a valid probability distribution, as required (but not enforced) by the API. Below are the docstring notes from `xent_op_deterministic_test.py` in its current state (after the most recent commit).\r\n\r\n`testSingleClass`:\r\n\r\nThe most recent commit also adds the fourth minibatch item: `labels=[1.]`, `logits=[1.]`.\r\n\r\n```\r\nThe deterministic implementation does not produce the gradients expected by\r\nthe original test (for the nondeterministic functionality) when the labels\r\nvector is not a valid probability distribution.\r\n\r\nlabels: [[-1.], [0.], [1.], [1.]]\r\nlogits: [[1.], [-1.], [0.], [1.]]\r\n\r\n               nondeterministic               deterministic\r\ndloss/dlogits: [[2.0], [1.0], [0.0], [0.0]]   [[0.0], [0.0], [0.0], [0.0]]\r\n\r\nNote that only the second two label vectors are valid probability\r\ndistributions (as required by the API) and that the gradient matches for\r\nthose cases.\r\n```\r\n\r\n`testLabelsBroadcast`:\r\n\r\nThe most recent commit also adds the third minibatch item: `labels=[0.25]`, `logits=[1., 2., 3., 4.]`.\r\n\r\n```\r\nThe deterministic implementation does not produce the gradients expected by\r\nthe original test (for the nondeterministic functionality) when the labels\r\nvector (after broadcasting) is not a valid probability distribution.\r\n\r\nlabels: [[0.], [2.], [0.25]]\r\nlogits: [[1., 1., 1., 1.],\r\n         [1., 2., 3., 4.],\r\n         [1., 2., 3., 4.]]\r\n\r\ndloss/dlogits (nondeterministic):\r\n    [[ 0.25 ,  0.25 ,  0.25 ,  0.25 ],\r\n     [-1.968, -1.913, -1.763, -1.355],\r\n     [-0.218, -0.163, -0.013,  0.394]]\r\n\r\ndloss/dlogits (determinsitic):\r\n    [[ 0.   ,  0.   ,  0.   ,  0.   ],\r\n     [-1.743, -1.303, -0.105,  3.150],\r\n     [-0.218, -0.163, -0.013,  0.394]]\r\n\r\nNote that neither of the first two broadcast label vectors is a valid\r\nprobability distribution (as required by the API) and that these are the\r\ncases that yield different gradients for nondeterministic vs determinsitic\r\nimplementations.\r\n```", "timestamp": "2021-05-28T00:04:07Z", "author": "MDQ6VXNlcjMzNTMyOTQx"}, {"message": "I propose that we remove these illegal label cases from the tests, replacing them with legal cases, thereby making the tests run the same way on both the original implementation and the deterministic implementation.", "timestamp": "2021-06-02T19:38:29Z", "author": "MDQ6VXNlcjMzNTMyOTQx"}, {"message": "I am slightly worried some people might accidentally rely on the current behavior when given illegal label cases. Also in practice, the labels might not add up exactly to 1 due to rounding errors. I think we should keep it as is for now, with the TODO to investigate this. I'll later try to think if there is a valid interpretation of softmax-cross-entropy when the labels do not add to 1.", "timestamp": "2021-06-02T19:44:09Z", "author": "MDQ6VXNlcjY1MTAyMDM="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYzOTAxODIwMQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/49488#discussion_r639018201", "comments": [{"message": "Shouldn't this depend on dpoly?", "timestamp": "2021-05-25T17:33:55Z", "author": "MDQ6VXNlcjM3MzEwMjU="}, {"message": "done", "timestamp": "2021-05-26T04:27:43Z", "author": "MDQ6VXNlcjIwODQzNTk2"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0MDk1MzYzNg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/49720#discussion_r640953636", "comments": [{"message": "Do we need to `Release` the intermediate `EagerOperation`s?\r\n\r\ncc: @allenlavoie \r\n", "timestamp": "2021-05-27T20:44:51Z", "author": "MDQ6VXNlcjM5Njc0ODg="}, {"message": "Thanks for the comment. `post_op` is of type `std::unique_ptr<EagerOperation>*`, and therefore `*post_op` is a `std::unique_ptr` which should release its target in the assignment to another pointer. Does this answer your question?", "timestamp": "2021-05-27T22:52:04Z", "author": "MDQ6VXNlcjgwMjk2MTY0"}, {"message": "`out_op` and `post_op` are the same thing, right? Maybe the main thing to do here is simplify a bit, removing `post_op` and just setting `pre_op = out_op->get()` instead of release/reset.", "timestamp": "2021-05-27T23:00:15Z", "author": "MDQ6VXNlcjM3MzEwMjU="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0NDAwMzA0Mw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/49925#discussion_r644003043", "comments": [{"message": "Looks great. Should we just put this in `tensorflow/c/c_api_test.cc`?", "timestamp": "2021-06-02T14:11:41Z", "author": "MDQ6VXNlcjg0Mzcy"}, {"message": "ok, done.", "timestamp": "2021-06-02T14:37:23Z", "author": "MDQ6VXNlcjg3NjI5NQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2ODE5MzQwOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/49974#discussion_r668193409", "comments": [{"message": "Question on consistency: sometimes you use 1., other times 2.0 - should we settle on a single version? There's also the .0 corner case, but that's rare.", "timestamp": "2021-07-12T19:21:35Z", "author": "MDQ6VXNlcjI2NjI4NTQ3"}, {"message": "@mdanatg Here I used `.0` because of the succeeding comma (future-proofing lints / spell-checkers)", "timestamp": "2021-07-12T23:13:19Z", "author": "MDQ6VXNlcjgwNzU4MA=="}, {"message": "I see. If at all practical, using .0 in the code as well would sound good to me. No worries if that cannot be easily controlled though - it's just a minor consistency issue.", "timestamp": "2021-07-15T16:07:47Z", "author": "MDQ6VXNlcjI2NjI4NTQ3"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0OTQ0MzE4Nw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/50020#discussion_r649443187", "comments": [{"message": "This won't work for Operations with region I think?", "timestamp": "2021-06-10T18:55:03Z", "author": "MDQ6VXNlcjMzNzIzMDA="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0OTQ1OTM3Nw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/50020#discussion_r649459377", "comments": [{"message": "What about blocks already inside a fusion?", "timestamp": "2021-06-10T19:20:47Z", "author": "MDQ6VXNlcjMzNzIzMDA="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0Nzg1OTMyOQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/50153#discussion_r647859329", "comments": [{"message": "Why is this method available only for fusions?", "timestamp": "2021-06-08T23:21:57Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "I based my implementation on the `AddParameter()` and `RemoveParameter()` methods. They had those restriction, so I kept it for safety.", "timestamp": "2021-06-11T14:06:51Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0Nzg1OTYzOA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/50153#discussion_r647859638", "comments": [{"message": "The error message does not seem to add value, same below.\r\n\r\nWhy is the method for different shape used?", "timestamp": "2021-06-08T23:22:53Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "Removed the debug leftover.\r\nIn my cases, I need to change the shape rank. So I can't use `ReplaceAllUsesWith`.\r\nI could rename that method to `ReplaceParameterDifferentShape` if you think this is useful.", "timestamp": "2021-06-11T14:09:18Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NTcwNDc3MA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/50342#discussion_r655704770", "comments": [{"message": "What is the MathDialect used for in this patch?", "timestamp": "2021-06-21T21:06:55Z", "author": "MDQ6VXNlcjMzNzIzMDA="}, {"message": "Ah I think nothing new here, it was a missing dependency before.", "timestamp": "2021-06-21T21:08:20Z", "author": "MDQ6VXNlcjMzNzIzMDA="}]}
{"thread_id": "PRRC_kwDOArmXAs4rdzTh", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/50408#discussion_r729232609", "comments": [{"message": "I have a question on this:\r\nHere we silently use a different algorithm_id that is not provided by users.\r\nShall we just issue an error if the user provided is not within the range instead?", "timestamp": "2021-10-14T18:19:20Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "For example, user wants to set `forced_algorithm_id=1` and there is one layer in the network which has only 1 allowed algorithm id. Because of that, user won't be able to set anything but `forced_algorithm_id=0`. \r\nBut I agree, we should notify the user that the choice was rejected for a particular layer. Would issuing a warning would suffice here?", "timestamp": "2021-10-14T19:27:41Z", "author": null}, {"message": "Addressed.", "timestamp": "2021-10-15T05:33:09Z", "author": null}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1OTA3MzU0OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/50465#discussion_r659073548", "comments": [{"message": "What's the rationale for this logic?\r\n\r\nFrom the shape inference function, it looks like the minor dim is used for vector as well. \r\nhttps://github.com/tensorflow/tensorflow/blob/81aa90bf6c86b6af7d9d37d08e92915ee6096801/tensorflow/core/framework/common_shape_fns.cc#L2366", "timestamp": "2021-06-25T22:52:42Z", "author": "MDQ6VXNlcjE5OTAwNzk="}, {"message": "I ran through an error lowering down some transformer's training graph from TF to MHLO using IREE, and turns out they got the windows_dim to be negative when computing. Turns out because the indices dim is 1 it subtracted:\r\n```\r\nnum_index_dims.shape() //prints [512]\r\nnum_index_dims = indices.getShape.back() //num_index_dim becomes 512\r\nwindow_dim = tensor_rank - num_index_dims //which is 2 - 512\r\n=> window_dim = -510\r\n```\r\n\r\nLooks like the minimum dims the non-resource variant handle is 2 dim, so we need to handle it as such:\r\nif we get indices.shape = [N], we'd make it [N,1]", "timestamp": "2021-06-25T23:48:49Z", "author": "MDQ6VXNlcjY4MDg3Njk5"}, {"message": "That seems like an invalid op. \r\nhttps://github.com/tensorflow/tensorflow/blob/7c96db88e54cc417d416aa1aa405145e15ca0731/tensorflow/compiler/tf2xla/lib/scatter.cc#L51", "timestamp": "2021-06-28T09:55:38Z", "author": "MDQ6VXNlcjE5OTAwNzk="}, {"message": "hmm is it perhaps because, when they check it is [512,1], and then after that, we do remove_suffix as below:\r\nhttps://github.com/tensorflow/tensorflow/blob/7c96db88e54cc417d416aa1aa405145e15ca0731/tensorflow/compiler/tf2xla/lib/scatter.cc#L58\r\nso after the check it becomes [512,] \ud83e\udd14 ", "timestamp": "2021-06-28T18:07:40Z", "author": "MDQ6VXNlcjY4MDg3Njk5"}, {"message": "Where do we add 1 in the suffix if the original indices were [512]?\r\n\r\nI think, it will fail the following check and not generate the negative window_dim -510.\r\n\r\nnum_index_dims > buffer_shape.rank()", "timestamp": "2021-06-28T18:30:54Z", "author": "MDQ6VXNlcjE5OTAwNzk="}, {"message": "I see I see, does that mean we should just add this check to the MLIR version?", "timestamp": "2021-06-28T18:54:26Z", "author": "MDQ6VXNlcjY4MDg3Njk5"}, {"message": "for reference this is the mlir line that got generated:\r\n```\r\n%1437 = \"tf.TensorScatterAdd\"(%arg0, %arg1, %arg2) : (tensor<100x768xf32>, tensor<512xi32>, tensor<512x768xf32>) -> tensor<*xf32>\r\n```", "timestamp": "2021-06-28T19:19:23Z", "author": "MDQ6VXNlcjY4MDg3Njk5"}, {"message": "and if we do add the check\r\n```\r\n( if (num_index_dims > buffer_shape.rank()))\r\n```\r\ndoesn't that mean we can't compile any 1d vector with indices and updates that has greater rank than the buffer?\r\nFor example:\r\n<img width=\"495\" alt=\"Screen Shot 2021-06-28 at 12 25 45 PM\" src=\"https://user-images.githubusercontent.com/68087699/123692874-2d879700-d80c-11eb-9a5c-f7eec1b95659.png\">\r\nwhose num_index_dim = 8, and buffer.rank = 1 \ud83e\udd14 ", "timestamp": "2021-06-28T19:28:06Z", "author": "MDQ6VXNlcjY4MDg3Njk5"}, {"message": "Maybe the XLA compiler take care of this issue in the higher level? that's why it doesn't have this problem in the XLA compiler?", "timestamp": "2021-06-28T19:32:49Z", "author": "MDQ6VXNlcjY4MDg3Njk5"}, {"message": "These indices need to be of rank 2 in that example. Please check the code that generated this op and that would probably require a fix.\r\n\r\nTensorScatterAddOp doc mentions this check.\r\nhttps://github.com/tensorflow/tensorflow/blob/4fb5c5f49d92edafe5883b154aa546a0c00c7d65/tensorflow/compiler/mlir/tensorflow/ir/tf_generated_ops.td#L17908", "timestamp": "2021-06-29T09:11:43Z", "author": "MDQ6VXNlcjE5OTAwNzk="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NTU5Nzc0OQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/50569#discussion_r665597749", "comments": [{"message": "ditto, how about caching the json?", "timestamp": "2021-07-07T18:05:02Z", "author": "MDQ6VXNlcjExNTc0MzI="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5MjUxNjUzNw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/50756#discussion_r692516537", "comments": [{"message": "At least we need to update the comment. Could you extract this into a constant? Was it tuned manually? On which examples?", "timestamp": "2021-08-19T22:02:53Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "I can remove this now. I added that late to remove a small V100 slowdown.\r\nIt was needed for the fusion_5_simpler* HLO in the description.\r\nDo you want me to remove it or keep it?", "timestamp": "2021-08-20T15:13:35Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}, {"message": "OK let's keep it.", "timestamp": "2021-08-24T19:41:08Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5MjUxNjkwMw==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/50756#discussion_r692516903", "comments": [{"message": "Update the comment? This no longer hurts performance I understand?", "timestamp": "2021-08-19T22:03:43Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "Done.", "timestamp": "2021-08-20T14:31:27Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5MjUxOTU3Ng==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/50756#discussion_r692519576", "comments": [{"message": "Does it mean that the cap becomes `threads_per_block_y` times larger? Why?", "timestamp": "2021-08-19T22:09:28Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "I have renamed `capped_threads_per_block` to `capped_threads_per_block_x`. Hopefully this make it more clear.\r\nMostly, this try to cap the number of blocks created. As sometimes there is less blocks then the cap we are doing.\r\nI also capped the number of threads in the X dimensions. I forgot if it was to prevent regression in some kernels or because it was faster on some kernels.\r\n\r\n`capped_threads_per_block_x * threads_per_block_y` Is just. the number of threads that I want to create.\r\n\r\n", "timestamp": "2021-08-20T14:40:20Z", "author": "MDQ6VXNlcjE4MDk4Nw=="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MjY3NzY1NA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/50782#discussion_r672677654", "comments": [{"message": "Why tensorRT 6?", "timestamp": "2021-07-19T22:36:33Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Fair enough, let's remove this comment I had a doubt on how useful it was anyway.\r\nRemoved", "timestamp": "2021-07-19T23:34:13Z", "author": "MDQ6VXNlcjEwOTIzNTk5"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NDA2MjYzNA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/50876#discussion_r674062634", "comments": [{"message": "Do we need this change?", "timestamp": "2021-07-21T15:09:04Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "The PassFix<> was added to fix AMD HIP issues. If the change in AMDGPUCompiler::OptimizeHloConvolutionCanonicalization eliminates the need for that, I am ok removing the PassFix.", "timestamp": "2021-07-21T15:16:58Z", "author": "MDQ6VXNlcjY1MTk0NTAz"}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NjM5MjU1OA==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/51094#discussion_r686392558", "comments": [{"message": "Why is checking `noutput > 1` necessary? Seems to workaround the `num_bits=0` issue, but you've fixed that in this PR.", "timestamp": "2021-08-10T23:44:33Z", "author": "MDQ6VXNlcjY1MTAyMDM="}, {"message": "This is just an optimization that avoids unnecessary temporaries and copies in this case.", "timestamp": "2021-08-11T01:16:31Z", "author": "MDQ6VXNlcjM5NzkwOTY="}]}
{"thread_id": "PRRC_kwDOArmXAs4qg6-5", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/51471#discussion_r713273273", "comments": [{"message": "Does this routine only \"evaluate\" but not \"train\" the model?\r\nLooks like we don't follow the rule \"using third person singular verb forms in subroutine document\" in this whole file.", "timestamp": "2021-09-21T17:40:36Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Right, it just evaluates. Updated the docstring, and corrected the verb form.", "timestamp": "2021-09-22T20:45:00Z", "author": "MDQ6VXNlcjM2NzExMDY="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5MTI5Mzc1MQ==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/51538#discussion_r691293751", "comments": [{"message": "@mihaimaruseac @mdanatg What do you think might be the best approach when we have a 1-to-N relation between a ticket and its failing tests. \r\nI don't want that this test will fail at some point as this case will be solved but other expecting failing test connected to the same issue are still passing.\r\nFor the nature of these tests do you think that that a team member will need to just grep this URL and looking for other related expecting failing tests in the source code before evaluating to close the mentioned bug?\r\n\r\nIt might work but it still seems a bit too manual and someone might forget to do it.  ", "timestamp": "2021-08-18T14:28:27Z", "author": "MDQ6VXNlcjE3MTA1Mjg="}, {"message": "I think the risk of having stale tests that still fail even after the bug is marked fixed is low. Most of the time, the PR which fixes the bug should cause the expectedFailure to raise an error. If we end up seeing a lot, we can think of a more automated way to identify them.", "timestamp": "2021-08-18T14:53:16Z", "author": "MDQ6VXNlcjI2NjI4NTQ3"}, {"message": "I am more concerned of silently passing expected failing tests connected to the same issue. \r\nE.g. Something like an internal or contributor PR that don't cover all the testing use cases and so It cannot close the related github issue and instead It will close It.\r\n\r\nThe alternative is to have an atomic github issues approach for tracking the use cases that we are testing more strictly with a 1-to-1 approach.", "timestamp": "2021-08-18T15:34:46Z", "author": "MDQ6VXNlcjE3MTA1Mjg="}, {"message": "I meant it is really related on our atomicity concept of a PR as a transatcion of all the expected failing tests connected to a single issue.", "timestamp": "2021-08-18T15:36:51Z", "author": "MDQ6VXNlcjE3MTA1Mjg="}]}
{"thread_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5ODk1OTMyMg==", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/51733#discussion_r698959322", "comments": [{"message": "What error is this catching?", "timestamp": "2021-08-31T03:42:29Z", "author": "MDQ6VXNlcjEzNjI5MQ=="}, {"message": "I don't think this is needed. Reverted in the local import.", "timestamp": "2021-10-27T17:10:30Z", "author": "MDQ6VXNlcjMyMzE5OQ=="}]}
{"thread_id": "PRRC_kwDOArmXAs4q6D4N", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/52012#discussion_r719863309", "comments": [{"message": "What will the \"TODO\" be doing?\r\nIf the TRTEngineOp is not executed, we can't build the engine. \r\nI think we may just issue a warning and skip the TRTEngineOp?", "timestamp": "2021-10-01T00:39:35Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "When the engine is not built, we place an [empty context into the cache](https://github.com/tensorflow/tensorflow/blob/41e66148999080da5b2b80a59740a8f2a59f17da/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc#L1114). The question what is the right way to handle that during serialization. \r\n\r\nI decided to just return without changing engine_data ptr. As you propose, I have issued the warning.", "timestamp": "2021-10-01T20:17:30Z", "author": "MDQ6VXNlcjM2NzExMDY="}]}
{"thread_id": "PRRC_kwDOArmXAs4q7-r6", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/52012#discussion_r720366330", "comments": [{"message": "Do we have evidence that shows this step is needed? I found that GrapplerItemFromMetaGraphDef does inlining, see [here](https://github.com/tensorflow/tensorflow/blob/e110eadd4147a6fb8a82663a54e19c1918428754/tensorflow/core/grappler/grappler_item_builder.cc#L631-L632)\r\n", "timestamp": "2021-10-01T16:00:23Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Thanks for pointing this out!\r\n\r\nI confirm that inlining the function is needed, without that the functions are not inlined. I have tried to set \r\n`item_config.inline_functions = true;`, but it had no effect. \r\n\r\nWithout inlining the functions, we will get the following message \r\n\r\n```\r\ntrt_optimization_pass.cc:156] Attribute _tftrt_convert_function was not found\r\n```\r\nand the function will not be converted.\r\n\r\nI was experimenting with freezing the graph, and in that case I have needed a separate inlining step, because the [FreezeSavedModel](https://github.com/tensorflow/tensorflow/blob/8968874a677b34f734e8fead39a14a8de511d83d/tensorflow/cc/tools/freeze_saved_model.h#L36) does not inline and only freezes variables in the main graph. In contrast, the python convert_variables_to_constant functions do an inlining step.  \r\n\r\nSince freezing the model is pushed out to a follow up PR, we can remove the InlineFunctions step from here, and add the `function` grappler pass in `RunTfTrt`. I have made this change. ", "timestamp": "2021-10-01T22:03:44Z", "author": "MDQ6VXNlcjM2NzExMDY="}]}
{"thread_id": "PRRC_kwDOArmXAs4qdyuE", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/52047#discussion_r712452996", "comments": [{"message": "Why do we need this? Isn't this part of line 772 already?", "timestamp": "2021-09-20T19:28:22Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "You are right, it is not needed.", "timestamp": "2021-09-21T05:45:50Z", "author": "MDQ6VXNlcjM2NzExMDY="}]}
{"thread_id": "PRRC_kwDOArmXAs4qv-8X", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/52110#discussion_r717221655", "comments": [{"message": "what's happening if we don't have this?", "timestamp": "2021-09-28T04:53:01Z", "author": "MDQ6VXNlcjI5MDg1MDU="}, {"message": "If there is no delegate available, calling _delegate_list_util_.AppendCmdlineFlags(&flags) (line 46) appends no command line flags. Since at least _--help_ flag is expected to be available (line 49), the condition returns false and an error is reported (specifically: \"help was not found\", as implemented in _tensorflow/lite/tools/tool_params.cc:53_). So this check aims to provide the user with more instructive/helpful error message, should this happen again in the future.", "timestamp": "2021-09-29T09:35:25Z", "author": "MDQ6VXNlcjM0MjYzMTI4"}, {"message": "It turns out this change breaks many tests. You'd better revert it.\r\n```\r\nbazel test tensorflow/lite/kernels:optional_tensor_test\r\n```", "timestamp": "2021-10-07T09:49:28Z", "author": "MDQ6VXNlcjI5MDg1MDU="}, {"message": "The check has been reverted in the latest commits.", "timestamp": "2021-10-08T08:21:41Z", "author": "MDQ6VXNlcjM0MjYzMTI4"}]}
{"thread_id": "PRRC_kwDOArmXAs4q2f2p", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/52181#discussion_r718929321", "comments": [{"message": "Can you explain in what situation getNBOptimizationProfiles() is 0 but n_inputs > 0?\r\nOn the other hand, adding this line will make the path where RestoreProfiles() is executed different from the path when we build profile and don't execute RestoreProfiles(). I meant, if there is no profiles, but n_inputs > 0, then one path has need_profiles_ true but another path has need_profiles_ false.", "timestamp": "2021-09-29T22:32:08Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Can you response to the comment?", "timestamp": "2021-09-30T16:12:56Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "> Can you explain in what situation getNBOptimizationProfiles() is 0 but n_inputs > 0?\r\n\r\nThe question is other way around, is there a case where getNBOptimizationProfiles() > 0 but n_inputs==0?\r\nYes. Assume that we have a single input tensor x to INetworkDefinition, and we define one profile with min=opt=max. Assume further that only the shape of x is used. TRT 7 in this case would prune the input, so the engine itself would have 0 inputs. \r\n\r\n> On the other hand, adding this line will make the path where RestoreProfiles() is executed different from the path when we build profile and don't execute RestoreProfiles(). \r\n\r\nValid point (but again other way around: we are concerned with the case when n_inputs == 0 && n_profiles > 0). \r\n\r\nI will fix this.", "timestamp": "2021-09-30T16:45:14Z", "author": "MDQ6VXNlcjM2NzExMDY="}, {"message": "- When we convert the engine, then profiles min, opt, max vectors are initialized from the input shapes. We will have `min.size() == 2*ctx->num_inputs()` [ref](https://github.com/tensorflow/tensorflow/blob/f0014080fb29a9e4038254019f105f0108565e5d/tensorflow/compiler/tf2tensorrt/utils/trt_shape_optimization_profiles.h#L79).\r\n- When we restore the engines then `min.size() == 2 * n_inputs`, where `n_inputs` is the number of input bindings. This can differ from `ctx->num_inputs()` if an input was pruned. This can happen only in TRT 7. TRT 8 does not prune inputs.\r\n- This shows that the profile vectors (min, max, opt) differ on the two code paths.\r\n- The line `need_profiles_ &= n_inputs > 0;` introduces another difference in the code path, with the aim to fix profile selection (disable it), because profile selection would [fail here](https://github.com/tensorflow/tensorflow/blob/f0014080fb29a9e4038254019f105f0108565e5d/tensorflow/compiler/tf2tensorrt/utils/trt_shape_optimization_profiles.h#L117) if an input was pruned and the engine was restored.\r\n- This solves the case when there are no inputs left for the engine.\r\n- There can be a case where `n_inputs > 0`, but `n_inputs != ctx->num_inputs()` , I expect that such a case would still fail. A proper implementation would probably need to modify `IncludesShapes` so that we map `ctx->input` to the `binding_idx`, or modify the `RestoreProfiles` to set some placeholder values for the pruned inputs.\r\n- I will prepare a separate PR with that fix.\r\n- The unit test added in this PR does not trigger the pruned input case. Therefore I can remove the `need_profiles_ &= n_inputs > 0;` condition and add a proper fix in a follow up PR. ", "timestamp": "2021-09-30T17:51:52Z", "author": "MDQ6VXNlcjM2NzExMDY="}, {"message": "Removed this line.", "timestamp": "2021-09-30T18:01:56Z", "author": "MDQ6VXNlcjM2NzExMDY="}]}
{"thread_id": "PRRC_kwDOArmXAs4sPW5d", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/52248#discussion_r742223453", "comments": [{"message": "Why VLOG(0)?", "timestamp": "2021-11-03T18:28:31Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "removed", "timestamp": "2021-11-08T17:45:30Z", "author": "MDQ6VXNlcjMwMzIzMTg3"}]}
{"thread_id": "PRRC_kwDOArmXAs4rWfrH", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/52271#discussion_r727317191", "comments": [{"message": "Sorry, what do you really want to say here?", "timestamp": "2021-10-12T16:41:26Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Removed", "timestamp": "2021-10-28T19:20:11Z", "author": "MDQ6VXNlcjMwMzIzMTg3"}]}
{"thread_id": "PRRC_kwDOArmXAs4r3cIz", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/52625#discussion_r735953459", "comments": [{"message": "The function can either return a string, or directly print the string. How do we decide which one we should use?\r\nTo me, making summary to return a string instead would make it more useful. What do you think?", "timestamp": "2021-10-25T20:43:43Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Fixed with `print_fn` as agreed on the call", "timestamp": "2021-11-03T21:19:33Z", "author": "MDQ6VXNlcjEwOTIzNTk5"}]}
{"thread_id": "PRRC_kwDOArmXAs4s2nh6", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/52942#discussion_r752515194", "comments": [{"message": "Can you find a way to share code with line 77-80?", "timestamp": "2021-11-18T18:28:38Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "It's what I was initially doing, but in your previous review you wrote:\r\n\r\n> We don't need to store this to pass to the next stage, as we can recalculate it as\r\n> attrs.src_format.size() -2\r\n\r\nI'm a bit confused, do you prefer to store `spatial_dim_count`or recompute it?", "timestamp": "2021-11-22T11:11:38Z", "author": "MDQ6VXNlcjE3NDQxMDYy"}, {"message": "Sorry for not being clear, I meant, we can use a common subroutine or macro to calculate the value, so that we don't have to duplicate the login of src_format.size() - 2 in two places -- if we duplicate the logic, we better duplicate the comment that explains \"-2\" -- that is what I am trying to avoid.", "timestamp": "2021-11-22T16:32:03Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}]}
{"thread_id": "PRRC_kwDOArmXAs4vjrYr", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/53310#discussion_r797881899", "comments": [{"message": "I don't understand why \"|| node.op() == \"Identity\"\" and the \"else\" in line 244 is necessary.\r\nCan you explain?\r\nDoes the test that you added test this situation?", "timestamp": "2022-02-02T18:06:17Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "- I have refactored this section to separate handling the `Identity` and `ReadVariableOp` nodes and to make the structure cleaner.\r\n- We have to handle identity nodes, because we need to change their dtype from `DT_RESOURCE` to the actual variable type.\r\n- The test that I have added creates the following node sequence: `VarHandleOp -> Identity -> ReadVariableOp`. In such a chain all the ops have to be changed, otherwise the resulting graph would be invalid, and the new test would fail with the following message:\r\n```\r\n  (frozen_session->Create(frozen_graph_def))\r\n    Which is: INVALID_ARGUMENT: Input 0 of node identity was passed float from var:0 incompatible with expected resource.\r\n```", "timestamp": "2022-02-04T22:35:25Z", "author": "MDQ6VXNlcjM2NzExMDY="}]}
{"thread_id": "PRRC_kwDOArmXAs4uDd02", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/53490#discussion_r772660534", "comments": [{"message": "It's not clear to me what you're checking for here? Which test does this correspond to?", "timestamp": "2021-12-20T20:51:20Z", "author": "MDQ6VXNlcjMzNzIzMDA="}, {"message": "I think `if (type == getOperand().getType())` is enough. I will remove this checking.", "timestamp": "2021-12-21T02:56:53Z", "author": "MDQ6VXNlcjI5OTU2Njkz"}]}
{"thread_id": "PRRC_kwDOArmXAs4ueZQV", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/53591#discussion_r779719701", "comments": [{"message": "Is the reason you call is \"Unsafe\" because it may return -1?\r\nIf the previous routine call \"Static\", this one can handle dynamic, what would be a good name for this?\r\n", "timestamp": "2022-01-06T17:29:09Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "I think it was more because it couldn't handle some \"scalar shape\" edge case. I'm going to rebase this on the PR for adding `DimsAdapter` since that one will merge before this and these functions go away.", "timestamp": "2022-01-11T21:28:47Z", "author": "MDQ6VXNlcjMwMzIzMTg3"}]}
{"thread_id": "PRRC_kwDOArmXAs4uZqD1", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/53623#discussion_r778477813", "comments": [{"message": "No need for this TODO anymore?", "timestamp": "2022-01-05T00:44:46Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "fixed", "timestamp": "2022-01-05T16:55:09Z", "author": "MDQ6VXNlcjMwMzIzMTg3"}]}
{"thread_id": "PRRC_kwDOArmXAs4vDkcG", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/53853#discussion_r789464838", "comments": [{"message": "Same question as above: is this covered by the verifier?", "timestamp": "2022-01-21T08:51:47Z", "author": "MDQ6VXNlcjMzNzIzMDA="}, {"message": "Likewise. There is no verifier checks for `lmhlo.Pad` regarding `interior_padding` and `edge_padding_*` attributes.", "timestamp": "2022-01-21T09:56:46Z", "author": "MDQ6VXNlcjY3ODg3ODU3"}, {"message": "OK, I guess another question then may be: **should** there be a verifier for these?", "timestamp": "2022-01-25T02:05:00Z", "author": "MDQ6VXNlcjMzNzIzMDA="}, {"message": "Yes, in my opinion, there should be a verifier implemented for the these.\r\n\r\nAs per : https://www.tensorflow.org/xla/operation_semantics#pad , it is required by padding configurations to specify the amount of edge padding and the interior padding for `each` dimension.\r\n\r\nWe don't have verifier for this both on `mhlo` and `lhlo`.\r\n\r\nI can implement it for both and include it as part of this PR.\r\n\r\nLet me know if we should do that OR get rid of these checks from this PR since they `should` be taken care by a verifier and later on raise a PR for verifiers.", "timestamp": "2022-01-25T03:36:32Z", "author": "MDQ6VXNlcjY3ODg3ODU3"}, {"message": "If you can introduce the verifier and remove the checks that would be perfect, thanks!", "timestamp": "2022-01-25T04:10:51Z", "author": "MDQ6VXNlcjMzNzIzMDA="}, {"message": "Hi @joker-eph \r\n\r\nI've included a verifier for `lmhlo.Pad` and all this logic will now be taken care by the verifier itself.\r\nI've included another verifier check for the padding configurations too which check for the expected output shape after paddings are applied.\r\n\r\nYou may re-review this now.", "timestamp": "2022-01-31T10:53:33Z", "author": "MDQ6VXNlcjY3ODg3ODU3"}]}
{"thread_id": "PRRC_kwDOArmXAs4vJCrR", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/53909#discussion_r790899409", "comments": [{"message": "What happens if F16 is called now?", "timestamp": "2022-01-24T16:11:31Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "it does not generate atomic add.  it simply generates a CAS loop.  i will remove that comment.  generating atomic add is a future task.", "timestamp": "2022-01-24T16:46:09Z", "author": "MDQ6VXNlcjcwMjgwOTM1"}]}
{"thread_id": "PRRC_kwDOArmXAs4vXUtM", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/53909#discussion_r794643276", "comments": [{"message": "What is the main difference between NVidia and AMD for atomic calls? Is it just the casting? Maybe we could share at a lower granularity?", "timestamp": "2022-01-28T16:10:59Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "it's not just the casting.  it is also the sync scope.  i would imagine that nvidia would simply use the default 'system' sync scope but we need to use 'agent' for performance reasons.", "timestamp": "2022-01-28T16:42:05Z", "author": "MDQ6VXNlcjcwMjgwOTM1"}, {"message": "i think the 'agent' sync scope is AMD specific.", "timestamp": "2022-01-28T18:07:11Z", "author": "MDQ6VXNlcjcwMjgwOTM1"}]}
{"thread_id": "PRRC_kwDOArmXAs4vfceH", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/54190#discussion_r796772231", "comments": [{"message": "The check doesn't close the file, does it? Why the error message says \"error closing the file\"?\r\n", "timestamp": "2022-02-01T16:25:48Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Fixed", "timestamp": "2022-03-16T00:04:04Z", "author": "MDQ6VXNlcjEwOTIzNTk5"}]}
{"thread_id": "PRRC_kwDOArmXAs4zo6LR", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/54319#discussion_r866362065", "comments": [{"message": "Why does soft placement matter here?", "timestamp": "2022-05-05T22:34:34Z", "author": "MDQ6VXNlcjMxNjYzMjY3"}, {"message": "To make the behavior to be consistent if config.get_soft_device_placement() is disabled.  An error is raised when an Op cannot be placed onto its intended device. So just to avoid sneakily fallbacking to CPU.", "timestamp": "2022-05-07T16:18:18Z", "author": "MDQ6VXNlcjI1NTkwMDI4"}, {"message": "Makes sense", "timestamp": "2022-05-17T17:07:45Z", "author": "MDQ6VXNlcjMxNjYzMjY3"}]}
{"thread_id": "PRRC_kwDOArmXAs4xXIBt", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/55245#discussion_r828145773", "comments": [{"message": "Can you explain why we change the constant from 10 to 1?", "timestamp": "2022-03-16T15:25:33Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}]}
{"thread_id": "PRRC_kwDOArmXAs4xchuy", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/55245#discussion_r829561778", "comments": [{"message": "Where we will release this memory?", "timestamp": "2022-03-17T22:28:28Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Actually, this memory is never released. I just followed the pattern previously used for [`UnaryOperation`](https://github.com/tensorflow/tensorflow/pull/55082/files#diff-e911ebd6321178662cc770f8a5bd2e7262b3a8d1662004fe68b7477c7e0e237bL3731) (see lines 3731-3636)\r\n```\r\nconst std::unordered_map<string, nvinfer1::UnaryOperation>*\r\nUnaryOperationMap() {\r\n  static auto* const m =\r\n      new std::unordered_map<string, nvinfer1::UnaryOperation>({\r\n        {\"Neg\", nvinfer1::UnaryOperation::kNEG},\r\n            {\"Exp\", nvinfer1::UnaryOperation::kEXP},\r\n```\r\nand for [ActivationType](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L3204-L3217).\r\n\r\nIs it OK?", "timestamp": "2022-03-17T23:39:50Z", "author": "MDQ6VXNlcjMyOTEwNDYx"}, {"message": "It was my fault to let it slip in. Would you please submit another PR to fix the existing cases?\r\n\r\nYou may mimic [here](https://github.com/tensorflow/tensorflow/blob/bc5315da4f34bd9ec0a52654978cb15da9d522a7/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc#L156-L164)\r\n(1) create a routine that \"new\" a map\r\n(2) use call_once to call that routine, and put the returned pointer inside a unique pointer", "timestamp": "2022-03-18T20:55:45Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "OK, I will work on that PR. But I think that it would be better to create it after that one and PRs [#55229](https://github.com/tensorflow/tensorflow/pull/55229/), [#55319](https://github.com/tensorflow/tensorflow/pull/55319) will be merged.", "timestamp": "2022-03-22T00:49:11Z", "author": "MDQ6VXNlcjMyOTEwNDYx"}, {"message": "@bixia1: It looks like we don't need to change anything in \r\n```\r\nconst std::unordered_map<string, nvinfer1::UnaryOperation>*\r\nUnaryOperationMap() {\r\n```\r\nor similar functions for the other maps. At least the implementation of your suggestion **create a routine that \"new\" a map** won't change anything from what we have now unless I missed something.\r\n\r\nI just ran the following experiment. I added the output of the **m** pointer into **/Test/ccc.txt** file :\r\n```\r\nconst std::unordered_map<string, nvinfer1::UnaryOperation>*\r\nUnaryOperationMap() {\r\n  static auto* const m =\r\n      new std::unordered_map<string, nvinfer1::UnaryOperation>({\r\n        {\"Neg\", nvinfer1::UnaryOperation::kNEG},\r\n            {\"Exp\", nvinfer1::UnaryOperation::kEXP},\r\n. . .\r\n     });\r\n  FILE *f = fopen(\"/Test/ccc.txt\", \"a\");\r\n  fprintf(f, \"I am in UnaryOperationMap m = %p\\n\", m);\r\n  fclose(f);\r\n  return m;\r\n}\r\n```\r\nand launched the `ConvertUnary` test.  After that I opened `/Test/ccc.txt` and I saw that all pointers, returned by `UnaryOperationMap()` are the same:\r\n```\r\nroot@319e3ba87abd:/opt/tensorflow# grep \"m = 0x55888e076e70\" /Test/ccc.txt -c\r\n290\r\nroot@319e3ba87abd:/opt/tensorflow# grep \"m = \" /Test/ccc.txt -c\r\n290\r\nroot@319e3ba87abd:/opt/tensorflow# \r\n```\r\nAnd it's is exactly what we would have with  **a routine that \"new\" a map**.\r\nI think this is because m is defined as `static` and it's generated only once, when the program is loaded.\r\n\r\nI will try to find another solution without **new** involved, but I think that we can live with that one. \r\n\r\n\r\n", "timestamp": "2022-03-22T03:47:27Z", "author": "MDQ6VXNlcjMyOTEwNDYx"}, {"message": "@bixia1 : The easiest way to avoid \"new\" is the following one. By using templates:\r\n```\r\ntemplate <typename T>\r\nusing operationMap = std::unordered_map<std::string, T>;\r\n// Map of all supported UnaryOperations\r\ntypedef operationMap<nvinfer1::UnaryOperation> unaryOperationMap;\r\nconst unaryOperationMap* UnaryOperationMap();\r\n```\r\nwe define the maps\r\n```\r\nunaryOperationMap kUnaryOperationMap = {{\r\n {\"Neg\", nvinfer1::UnaryOperation::kNEG},\r\n {\"Exp\", nvinfer1::UnaryOperation::kEXP},\r\n {\"Log\", nvinfer1::UnaryOperation::kLOG},\r\n\r\n#if IS_TRT_VERSION_GE(8, 2, 0, 0)\r\n {\"Round\", nvinfer1::UnaryOperation::kROUND},\r\n {\"Sign\", nvinfer1::UnaryOperation::kSIGN}\r\n#endif\r\n}};\r\n```\r\nand use their addresses:\r\n```\r\nconst unaryOperationMap* UnaryOperationMap() {\r\n return &kUnaryOperationMap;\r\n}\r\n```\r\nIt will be a very easy PR.", "timestamp": "2022-03-22T18:01:51Z", "author": "MDQ6VXNlcjMyOTEwNDYx"}, {"message": "> @bixia1: It looks like we don't need to change anything in\r\n> \r\n> ```\r\n> const std::unordered_map<string, nvinfer1::UnaryOperation>*\r\n> UnaryOperationMap() {\r\n> ```\r\n> \r\n> or similar functions for the other maps. At least the implementation of your suggestion **create a routine that \"new\" a map** won't change anything from what we have now unless I missed something.\r\n> \r\n> I just ran the following experiment. I added the output of the **m** pointer into **/Test/ccc.txt** file :\r\n> \r\n> ```\r\n> const std::unordered_map<string, nvinfer1::UnaryOperation>*\r\n> UnaryOperationMap() {\r\n>   static auto* const m =\r\n>       new std::unordered_map<string, nvinfer1::UnaryOperation>({\r\n>         {\"Neg\", nvinfer1::UnaryOperation::kNEG},\r\n>             {\"Exp\", nvinfer1::UnaryOperation::kEXP},\r\n> . . .\r\n>      });\r\n>   FILE *f = fopen(\"/Test/ccc.txt\", \"a\");\r\n>   fprintf(f, \"I am in UnaryOperationMap m = %p\\n\", m);\r\n>   fclose(f);\r\n>   return m;\r\n> }\r\n> ```\r\n> \r\n> and launched the `ConvertUnary` test. After that I opened `/Test/ccc.txt` and I saw that all pointers, returned by `UnaryOperationMap()` are the same:\r\n> \r\n> ```\r\n> root@319e3ba87abd:/opt/tensorflow# grep \"m = 0x55888e076e70\" /Test/ccc.txt -c\r\n> 290\r\n> root@319e3ba87abd:/opt/tensorflow# grep \"m = \" /Test/ccc.txt -c\r\n> 290\r\n> root@319e3ba87abd:/opt/tensorflow# \r\n> ```\r\n> \r\n> And it's is exactly what we would have with **a routine that \"new\" a map**. I think this is because m is defined as `static` and it's generated only once, when the program is loaded.\r\n> \r\n> I will try to find another solution without **new** involved, but I think that we can live with that one.\r\n\r\nThe concert is not about create different pointer for different test, but rather, the storage is not release.\r\nUsing a std unique pointer to wrap the value will ensure that the storage will be released.", "timestamp": "2022-03-23T15:26:43Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "> @bixia1 : The easiest way to avoid \"new\" is the following one. By using templates:\r\n> \r\n> ```\r\n> template <typename T>\r\n> using operationMap = std::unordered_map<std::string, T>;\r\n> // Map of all supported UnaryOperations\r\n> typedef operationMap<nvinfer1::UnaryOperation> unaryOperationMap;\r\n> const unaryOperationMap* UnaryOperationMap();\r\n> ```\r\n> \r\n> we define the maps\r\n> \r\n> ```\r\n> unaryOperationMap kUnaryOperationMap = {{\r\n>  {\"Neg\", nvinfer1::UnaryOperation::kNEG},\r\n>  {\"Exp\", nvinfer1::UnaryOperation::kEXP},\r\n>  {\"Log\", nvinfer1::UnaryOperation::kLOG},\r\n> \r\n> #if IS_TRT_VERSION_GE(8, 2, 0, 0)\r\n>  {\"Round\", nvinfer1::UnaryOperation::kROUND},\r\n>  {\"Sign\", nvinfer1::UnaryOperation::kSIGN}\r\n> #endif\r\n> }};\r\n> ```\r\n> \r\n> and use their addresses:\r\n> \r\n> ```\r\n> const unaryOperationMap* UnaryOperationMap() {\r\n>  return &kUnaryOperationMap;\r\n> }\r\n> ```\r\n> \r\n> It will be a very easy PR.\r\n\r\nwe can't use a global variable or file scope variable (static) for this, see style guide https://google.github.io/styleguide/cppguide.html#Static_and_Global_Variables.", "timestamp": "2022-03-23T15:27:56Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "I am fine with fixing this in another PR.", "timestamp": "2022-03-23T15:30:32Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}]}
{"thread_id": "PRRC_kwDOArmXAs4xvQOh", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/55332#discussion_r834470817", "comments": [{"message": "This line return a string, what does \"feature_name in string\" mean?\r\nHave you tested this?", "timestamp": "2022-03-24T15:57:49Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "This code currently tests that the `feature_name` is a substring of the env variable value, and though this will work in most cases, it can have false positives. It should probably be:\r\n```\r\nreturn (\r\n    feature_name in\r\n    os.environ.get(\"TF_TRT_EXPERIMENTAL_FEATURES\", default=\"\").split(\",\")\r\n)\r\n```", "timestamp": "2022-03-28T15:01:01Z", "author": "MDQ6VXNlcjE3NDQxMDYy"}, {"message": "Fixed with @Nyrio suggestion", "timestamp": "2022-03-29T21:25:09Z", "author": "MDQ6VXNlcjEwOTIzNTk5"}]}
{"thread_id": "PRRC_kwDOArmXAs40BsXU", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/55685#discussion_r872859092", "comments": [{"message": "Isn't the returned `Status` and passed in `Status` the same type? What's the point of this function?", "timestamp": "2022-05-13T21:45:35Z", "author": "MDQ6VXNlcjU0Nzg0MTA="}]}
{"thread_id": "PRRC_kwDOArmXAs4zAjjC", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/55686#discussion_r855783618", "comments": [{"message": "Shall we allow user to point to the TRTEngine node to inspect the details there, in a follow PR?", "timestamp": "2022-04-22T05:43:04Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Yes there are multiple possible improvements we could bring to this graph feature. This is one of them. As for now it's a first iteration of this tool. I expect this tool to grow as we need.", "timestamp": "2022-04-24T03:13:58Z", "author": "MDQ6VXNlcjEwOTIzNTk5"}]}
{"thread_id": "PRRC_kwDOArmXAs40B5Dm", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/56086#discussion_r872911078", "comments": [{"message": "Why is this change needed? tests seem to pass without this change. Also, this change caused some results mismatch in //tensorflow/core/kernels/mkl:mkl_quantize_op_test (this test only runs with --config=mkl).", "timestamp": "2022-05-14T01:37:27Z", "author": "MDQ6VXNlcjI0OTYzMDYx"}, {"message": "Without this change the test //tensorflow/python:quantized_ops_test fails. The following code is part of the test: \r\n```\r\n#!/usr/bin/python3\r\n\r\nimport numpy as np\r\nfrom tensorflow.python.framework import constant_op\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.framework import dtypes\r\n\r\nvalues = np.array([-1, -0.5, 0.555, 1], dtype=np.float32)\r\nquant_values = np.array([-128, -64, 71, 127], dtype=np.int32)\r\n\r\ninputs = constant_op.constant(values)\r\nexpected_quantized = quant_values\r\n\r\nmin_range, max_range = -1.0, 0.8\r\n\r\nquantized =  array_ops.quantize(\r\n    inputs,\r\n    min_range,\r\n    max_range,\r\n    T=dtypes.qint8,\r\n    mode=\"SCALED\",\r\n    round_mode=\"HALF_TO_EVEN\",\r\n    axis=None).output\r\n\r\nprint('Returned: ', quantized.numpy())\r\nprint('Expected: ', expected_quantized)\r\n```\r\nWithout above change the returned values are `[-127, -64, 70, 127]` instead of `[-128, -64, 71, 127]`. Also the above formula is formula that is here: https://intellabs.github.io/distiller/algo_quantization.html for calculating scale factor for full range.", "timestamp": "2022-05-18T08:57:02Z", "author": "MDQ6VXNlcjc5OTE2MzU4"}]}
{"thread_id": "PRRC_kwDOArmXAs40B5Dn", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/56086#discussion_r872911079", "comments": [{"message": "Why is this change needed? tests seem to pass without this change. Also, this change caused some results mismatch in //tensorflow/core/kernels/mkl:mkl_quantize_op_test (this test only runs with --config=mkl).", "timestamp": "2022-05-14T01:37:27Z", "author": "MDQ6VXNlcjI0OTYzMDYx"}, {"message": "Please see above comment.", "timestamp": "2022-05-18T08:57:29Z", "author": "MDQ6VXNlcjc5OTE2MzU4"}]}
{"thread_id": "PRRC_kwDOArmXAs41E0sS", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/56369#discussion_r890456850", "comments": [{"message": "So it interfered xnnpack delegate option?", "timestamp": "2022-06-06T19:09:12Z", "author": "MDQ6VXNlcjI5MDg1MDU="}, {"message": "Yes. Cannot be executed with the xnnpack delegate option.\r\n```\r\nType mismatch while accessing parameter.\r\nAbort\r\n```\r\n\r\n\r\nWhen specifying the xnnpack delegate option, the specification in `Interpreter::SetNumThreads` does not work. Must be specified in `DelegateProviders`.", "timestamp": "2022-06-06T22:24:43Z", "author": "MDQ6VXNlcjE3OTU0Njcz"}, {"message": "Got it. Thanks for the confirmation.", "timestamp": "2022-06-06T22:28:04Z", "author": "MDQ6VXNlcjI5MDg1MDU="}]}
{"thread_id": "PRRC_kwDOArmXAs432Vzw", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/56572#discussion_r936991984", "comments": [{"message": "This will be confusing on machines where there's e.g. 1 Ampere GPU and 2 GPUs overall.  Improve this error?", "timestamp": "2022-08-03T18:09:49Z", "author": "MDQ6VXNlcjE1MDY2Mw=="}, {"message": "Done.", "timestamp": "2022-08-03T22:52:32Z", "author": "MDQ6VXNlcjQwMDE0MjQ="}]}
{"thread_id": "PRRC_kwDOArmXAs43R1Qv", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/56653#discussion_r927421487", "comments": [{"message": "This means that the condition input cannot be a weight, right? Why do we have this limitation?", "timestamp": "2022-07-22T08:31:21Z", "author": "MDQ6VXNlcjM2NzExMDY="}, {"message": "@tfeher : The point is that the condition input of `addSelect(...)` must be of type `nvinfer1::DataType::kBOOL`,  otherwise, we get an error:\r\n```\r\n2022-07-22 ... : E tensorflow/ ...  /my_select-selectv2:SELECT: condition tensor must have boolean type\r\n```\r\nBut, unfortunately, today we cannot pass boolean as weight due to an error:\r\n``` \r\n2022-07-22 ...: E tensorflow/...: (Unnamed Layer* 0) [Constant]: invalid weights type of Bool\r\n```\r\nThat's why I introduced this restriction.", "timestamp": "2022-07-23T01:28:24Z", "author": "MDQ6VXNlcjMyOTEwNDYx"}, {"message": "Thanks for the explanation! This is a TRT limitation. Please add a note to state that TRT does not allow boolean weights.", "timestamp": "2022-08-01T09:37:58Z", "author": "MDQ6VXNlcjM2NzExMDY="}, {"message": "DONE", "timestamp": "2022-08-01T17:25:01Z", "author": "MDQ6VXNlcjMyOTEwNDYx"}, {"message": "DONE", "timestamp": "2022-08-01T17:36:21Z", "author": "MDQ6VXNlcjMyOTEwNDYx"}]}
{"thread_id": "PRRC_kwDOArmXAs43-adH", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/56656#discussion_r939108167", "comments": [{"message": "Is the motivation here to preload the relevant cudnn libraries so that you don't measure the loading time during autotune? Can you add a comment stating the motivation to this function?", "timestamp": "2022-08-05T18:45:36Z", "author": "MDQ6VXNlcjY1MTAyMDM="}]}
{"thread_id": "PRRC_kwDOArmXAs46hivP", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/56761#discussion_r981871567", "comments": [{"message": "My intent is to help others who come along and read this code understand not just *what* we're doing, but *why* we're doing it.  In this case, it's correct and to say that we exchange the add and slice, but it's not obvious to me why this is necessary.  Could we add add sentence?", "timestamp": "2022-09-28T02:01:16Z", "author": "MDQ6VXNlcjE1MDY2Mw=="}]}
{"thread_id": "PRRC_kwDOArmXAs4620qP", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/56761#discussion_r987449999", "comments": [{"message": "Why this change?  I think it's the same as the old code, just more verbose?", "timestamp": "2022-10-05T01:18:19Z", "author": "MDQ6VXNlcjE1MDY2Mw=="}]}
{"thread_id": "PRRC_kwDOArmXAs48Mr0R", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/56761#discussion_r1009958161", "comments": [{"message": "What check beforehand guarantees that this is true? I am seeing a test fail due to user_count being 2.\r\n\r\nShould `m::MaximumAnyOrder` in `HandleMaximum` add an additional `WithOneUser()`?", "timestamp": "2022-10-31T23:40:28Z", "author": "MDQ6VXNlcjU0Nzg0MTA="}, {"message": "In fact, is there an issue if `binary` has multiple users? Wont `binary` be replaced with the new fused instruction? Then all the users will have the new fused instructions as input.", "timestamp": "2022-11-01T00:02:13Z", "author": "MDQ6VXNlcjU0Nzg0MTA="}, {"message": "It seems that the `TF_RET_CHECK` in line 353 is too strict and can be removed. The maximum instruction, i.e. `*binary`, can be allowed to have multiple users since the fused GEMM produces an equivalent result.", "timestamp": "2022-11-01T00:04:24Z", "author": "MDQ6VXNlcjgwMjk2MTY0"}, {"message": "Thanks for confirming, @philipphack . I will make this change.", "timestamp": "2022-11-01T00:05:19Z", "author": "MDQ6VXNlcjU0Nzg0MTA="}]}
{"thread_id": "PRRC_kwDOArmXAs426qsZ", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/56762#discussion_r921348889", "comments": [{"message": "Would that fail in NVIDIA?", "timestamp": "2022-07-14T16:30:02Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}]}
{"thread_id": "PRRC_kwDOArmXAs43ggVf", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/56905#discussion_r931267935", "comments": [{"message": "@reedwm What do you think about this?", "timestamp": "2022-07-27T16:27:15Z", "author": "MDQ6VXNlcjE3MTA1Mjg="}, {"message": "I said above that this means Bincount is likely deterministic. But it turns out that is wrong. The comment in that links is:\r\n\r\n> assuming that your histogram output is an integral type\r\n\r\nBut the histogram output can be floating-point, if the `weights` passed to `tf.math.bincount` is floating-point.\r\n\r\nAnyway, I would still mention `DeviceHistogram::HistogramEven` for context. Maybe say something like \r\n\r\n    Is there really nondeterministic? DeviceHistogram::HistogramEven is called, and it is unclear if it is deterministic on floating-point inputs. See https://github.com/NVIDIA/cub/issues/471#issuecomment-1194682443.", "timestamp": "2022-07-27T17:30:39Z", "author": "MDQ6VXNlcjY1MTAyMDM="}]}
{"thread_id": "PRRC_kwDOArmXAs44WLnN", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/57108#discussion_r945338829", "comments": [{"message": "Why 128?", "timestamp": "2022-08-14T20:29:18Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "The original purpose here is to skip those large constants in case copying their initializers to multiple llvm modules may increase the executable size while not benefiting from constant-folding (not sure if this claim is valid). 128 is a magic number I picked for this purpose.\r\n\r\nBased on the models I've tested, only scalar constants like `u64[] constant(32), f32[] constant(0), f16[] constant(0)` in HLO were translated into llvm global constants with num_elements = `8, 4, 2` respectively. Constant arrays like `pred[6]{0} constant({0, 0, 1, 1, 1, 1})` in HLO were translated into non-constant llvm global variable.\r\n\r\nSo should I remove this check assuming all llvm global constants are scalar types in XLA? Any suggestions?", "timestamp": "2022-08-17T18:24:50Z", "author": "MDQ6VXNlcjE5NDgxMzA4"}, {"message": "Nit: is it possible to extract it to a `static constexpr int` or something with a self-descriptive name?", "timestamp": "2022-08-18T19:35:34Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "@timshen91 has more context here, deferring to him. Logic in https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc;l=572?q=f:ir_emitter_unnested.cc:572 might be relevant.", "timestamp": "2022-08-18T19:41:40Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "Yanming, have you actually seen constant global variables that has more than 1 element? XLA is supposed to generate constants only for single-element variables. See https://github.com/tensorflow/tensorflow/blob/04fd9533800eacbf7a07c3c5751b4d874acd3287/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc#L572", "timestamp": "2022-08-18T19:42:56Z", "author": "MDQ6VXNlcjExNTc0MzI="}, {"message": "@timshen91 Thanks for the info. No, I only saw HLO constants with element = 1 like `u64[] constant(32), f32[] constant(0), f16[] constant(0)` in HLO got converted into llvm constant global variables with their corresponding llvm IRs:\r\n```\r\n@buffer_for_constant_143 = constant [8 x i8] c\" \\00\\00\\00\\00\\00\\00\\00\", align 128\r\n@buffer_for_constant_1020 = constant [4 x i8] zeroinitializer, align 128\r\n@buffer_for_constant_5279 = constant [2 x i8] zeroinitializer, align 128\r\n```\r\nNote the number of elements in llvm ir are 8, 4, 2.\r\n\r\nHLO constants with elements > 1 like `constant_4431 = pred[512]{0} constant({...})` were translated into \r\n```\r\n@buffer_for_constant_4775 = global [512 x i8] zeroinitializer, align 128\r\n```\r\nUpon searching the codebase, I only saw [embedded_protocol_buffers.cc](https://github.com/tensorflow/tensorflow/blob/98d7b126cddbb3aa9f93ebc8786ae3e1bcd49bcb/tensorflow/compiler/aot/embedded_protocol_buffers.cc#L53-L56) can potentially generate constant global variable with more than one element whereas [ir_emitter_nested.cc](https://github.com/tensorflow/tensorflow/blob/945f871ce0bfef2a6513e68a5c4e05e36c4d7aa3/tensorflow/compiler/xla/service/gpu/ir_emitter_nested.cc#L224-L230) and [ir_emitter_unested.cc](https://github.com/tensorflow/tensorflow/blob/5f813501f79b0713bbfe87e3b911330fd0f1882f/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc#L600-L606) uses this `should_emit_initializer` check you mentioned.\r\n\r\n", "timestamp": "2022-08-18T21:56:41Z", "author": "MDQ6VXNlcjE5NDgxMzA4"}, {"message": "Yeah, for this line I think we can select all constants without the \"<= 128\" filtering.", "timestamp": "2022-08-18T23:44:07Z", "author": "MDQ6VXNlcjExNTc0MzI="}, {"message": "Sounds good. I've removed it in the revision.", "timestamp": "2022-08-18T23:58:59Z", "author": "MDQ6VXNlcjE5NDgxMzA4"}]}
{"thread_id": "PRRC_kwDOArmXAs44WKzB", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/57138#discussion_r945335489", "comments": [{"message": "What is the failure mode otherwise? Why is it ROCm-specific?", "timestamp": "2022-08-14T19:56:16Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "the testMinimizeLoss test fails when the actual value is quite a bit different than the expected value (many orders of magnitude different).\r\n\r\nthis happens when the test mode is eager, number of GPUs is 0, and use_devices_arg == true.\r\n\r\ni am not sure if it really is ROCm specific but i have no way to test it on CUDA.", "timestamp": "2022-08-15T18:45:49Z", "author": "MDQ6VXNlcjcwMjgwOTM1"}, {"message": "i was finally able to test this on cuda and it fails in the exact same way as it does on ROCm.\r\n\r\nthe failure is caused by the following commit:\r\n\r\nhttps://github.com/ROCmSoftwarePlatform/tensorflow-upstream/commit/d69e1652e48a564c84e8d95f75f1f203c878b1a6\r\n\r\nit seems like there is a bug here and this PR fixes that bug.", "timestamp": "2022-08-15T23:04:18Z", "author": "MDQ6VXNlcjcwMjgwOTM1"}]}
{"thread_id": "PRRC_kwDOArmXAs45w6fv", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/57518#discussion_r969123823", "comments": [{"message": "Is this true for all floating point types supported by this op?", "timestamp": "2022-09-13T03:51:49Z", "author": "MDQ6VXNlcjcwNjc2Ng=="}, {"message": "Do you mean f16? As tfl.gelu only supports f32 in floating point type, the legalization here only handles f32.", "timestamp": "2022-09-15T16:42:32Z", "author": "MDQ6VXNlcjc4ODE0Njk0"}]}
{"thread_id": "PRRC_kwDOArmXAs45z6ih", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/57519#discussion_r969910433", "comments": [{"message": "Did clang-format produce this?", "timestamp": "2022-09-13T17:37:44Z", "author": "MDQ6VXNlcjcwNjc2Ng=="}, {"message": "Sorry. I didn't run clang-format before this submission. In the next patch, I run clang-format-10 on it.", "timestamp": "2022-09-15T00:56:24Z", "author": "MDQ6VXNlcjc4ODE0Njk0"}]}
{"thread_id": "PRRC_kwDOArmXAs46WQoY", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/57519#discussion_r978913816", "comments": [{"message": "How does the equivalent ternary op formulation look here?", "timestamp": "2022-09-23T17:34:37Z", "author": "MDQ6VXNlcjcwNjc2Ng=="}, {"message": "Look nice.", "timestamp": "2022-09-27T05:24:40Z", "author": "MDQ6VXNlcjc4ODE0Njk0"}]}
{"thread_id": "PRRC_kwDOArmXAs45wuON", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/57648#discussion_r969073549", "comments": [{"message": "what does `also` mean in this case?", "timestamp": "2022-09-13T02:07:33Z", "author": "MDQ6VXNlcjE1MDY2Mw=="}, {"message": "This explanation is preceded by the explanation of the attempted match of the operand. See the expected output in the test.", "timestamp": "2022-09-13T19:05:26Z", "author": "MDQ6VXNlcjgwMjk2MTY0"}]}
{"thread_id": "PRRC_kwDOArmXAs455GEQ", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/57674#discussion_r971268368", "comments": [{"message": "Would it be beneficial to add a warning when bfloat16 is used and the GPU is Ampere- ?", "timestamp": "2022-09-14T20:32:50Z", "author": "MDQ6VXNlcjQyMzg1NTc3"}, {"message": "Why do I need another warning message, since we already have the error message?", "timestamp": "2022-09-14T23:31:26Z", "author": "MDQ6VXNlcjQwMDE0MjQ="}]}
{"thread_id": "PRRC_kwDOArmXAs455M-R", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/57674#discussion_r971296657", "comments": [{"message": "So I assume we can't do `GpuAtomicCasHelper` with bfloat16 which is the reason for the extra conversion?", "timestamp": "2022-09-14T21:06:41Z", "author": "MDQ6VXNlcjQyMzg1NTc3"}, {"message": "Yes, GpuAtomicCasHelper only has the instance for fp16. Instantiating it for bf16 is not trivial so I am thinking about reusing the fp16 instance since both fp16 and bf16 have 16-bit float and the logic of atomic cas for them is same.", "timestamp": "2022-09-14T23:30:27Z", "author": "MDQ6VXNlcjQwMDE0MjQ="}]}
{"thread_id": "PRRC_kwDOArmXAs46R8ej", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/57791#discussion_r977782691", "comments": [{"message": "Could you please explain why we need to change the order here? (I'm approving now because this remapper change only affects TF-oneDNN code.)", "timestamp": "2022-09-22T15:12:57Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "I need to move the add_input to before calling the CopyConv2DAttributes due to the change from the commit 5be9a59, specifically, the following line requires all the add_input has been called to get the correct input_size. \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper.cc#L2139\r\n\r\nThanks for the quick review.", "timestamp": "2022-09-22T15:35:56Z", "author": "MDQ6VXNlcjQyMTU2NDIw"}]}
{"thread_id": "PRRC_kwDOArmXAs46fv24", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/57854#discussion_r981401016", "comments": [{"message": "Does this needed the `run_deprecated_v1` and `with self.session()`?  We're trying to slowly update all these tests to run in V2 mode.", "timestamp": "2022-09-27T15:31:45Z", "author": "MDQ6VXNlcjI1Mzg3Mzk="}]}
{"thread_id": "PRRC_kwDOArmXAs47Dq5D", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58029#discussion_r990817859", "comments": [{"message": "Is this a correct change? It changes functionality?", "timestamp": "2022-10-09T17:44:15Z", "author": "MDQ6VXNlcjMyMzE5OQ=="}, {"message": "It was refactored automatically by Pycharm IDE. I am not sure, so reverting back the changes to original. \r\nThank you !", "timestamp": "2022-10-10T03:17:59Z", "author": "MDQ6VXNlcjg2NDY0NjQ5"}]}
{"thread_id": "PRRC_kwDOArmXAs47PRKw", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58078#discussion_r993858224", "comments": [{"message": "Is this correct even in quantized case?", "timestamp": "2022-10-12T20:17:17Z", "author": "MDQ6VXNlcjcwNjc2Ng=="}, {"message": "I will add the quantized lit test cases to verify this. ", "timestamp": "2022-10-13T18:28:34Z", "author": "MDQ6VXNlcjI0NDUxODU5"}]}
{"thread_id": "PRRC_kwDOArmXAs470s0G", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58147#discussion_r1003670790", "comments": [{"message": "I'm not sure I understand why we check for x4 and none of the others? \r\n\r\nAlso can you make sure that the unit-test for strcat covers this case? (and others here)", "timestamp": "2022-10-24T19:25:04Z", "author": "MDQ6VXNlcjMzNzIzMDA="}, {"message": "@joker-eph The use case that triggered the undefined behaviour warning had the fourth item possibly be null. The first 3 items were static strings. This change results in the desired behaviour in that case. I am not sure what the correct behaviour would be if one of the earlier items was null.\r\nPossible ways I saw to fix the reported problem\r\n\r\n1. This suggested change\r\n2. Create Append3 function and change call site to test for null and choose appropriate function Append3 or Append4, but that changes the interface and I am not sure what that would entail.\r\n3. Change call site to check for the fourth item being null and modify it to be an empty string instead.\r\n4. Put null check on all items in all versions of Append but then what to do if null found? Just skip item or return early or??\r\n\r\nI chose 1. as being simplest but will modify it if an alternative is more desirable. Please tell me what you would like to see.", "timestamp": "2022-10-25T09:28:25Z", "author": "MDQ6VXNlcjEwNDQyMDAx"}, {"message": "I don't quite follow. You found a case where this method was called with a nullptr and discovered it does not null check before calling memcpy.\r\n\r\n\r\nFrom there we have two possibilities:\r\n\r\n1) The contract of all these helpers in strcat.cc is that passing in nullptr is UB and it is a bug on the caller side.\r\n\r\n2) These helpers should be resilient to nullptr inputs and check to avoid UB.\r\n\r\n\r\nIf we are in case 2, then your option 1 does not seems like a valid way forward to me: you're fixing one example instead of the principled approach of fixing the contract of these API generally. That is not only Append4 but all of the helpers in this file.\r\nAs you mentioned that requires to specify that we would skip over an element if null (but that seems fine to me).\r\n\r\n\r\n(and we should have tests still)", "timestamp": "2022-10-25T16:46:52Z", "author": "MDQ6VXNlcjMzNzIzMDA="}, {"message": "@joker-eph  Fixed all Append helpers and added tests.", "timestamp": "2022-10-26T17:32:45Z", "author": "MDQ6VXNlcjEwNDQyMDAx"}]}
{"thread_id": "PRRC_kwDOArmXAs470MTl", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58150#discussion_r1003537637", "comments": [{"message": "What do you want to do with these lines?", "timestamp": "2022-10-24T16:47:02Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "These are optional debugging statements. I wanted to keep them commented because slice_ops.cc seems to be a place where we debug for many operations like split, unpack, slice, strided slice etc. and these lines with mask properties just make it simpler to debug. I can remove them if they are better removed. ", "timestamp": "2022-10-24T16:54:13Z", "author": "MDQ6VXNlcjMzODA5ODU3"}, {"message": "It is either in or out, we can keep debug code for this. You may want to make them VLOG(3).", "timestamp": "2022-10-24T17:00:07Z", "author": "MDQ6VXNlcjM1ODIwNjM5"}, {"message": "Done", "timestamp": "2022-10-24T17:14:19Z", "author": "MDQ6VXNlcjMzODA5ODU3"}]}
{"thread_id": "PRRC_kwDOArmXAs5XQapy", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58204#discussion_r1463921266", "comments": [{"message": "Are you sure you want to pass a vector by value?", "timestamp": "2024-01-23T19:56:26Z", "author": "MDQ6VXNlcjI1Mzg3Mzk="}, {"message": "This is still unresolved.  You shouldn't really be creating empty vectors for default arguments here, and I don't think you want to be passing vectors by value.  I suggest either refactoring so that all calls pass in a vector by const-reference, or if you want to keep an \"optional\" parameter, writing two overloads.", "timestamp": "2024-01-30T05:37:48Z", "author": "MDQ6VXNlcjI1Mzg3Mzk="}, {"message": "I have changed it to const reference and and removed default arguments. Thanks! Not sure why it is not reflected here. But here is the commit (https://github.com/tensorflow/tensorflow/pull/58204/commits/bea3d6056a9c12e043aa27fc48c5f676295fb0e5)", "timestamp": "2024-01-30T16:09:43Z", "author": "MDQ6VXNlcjI3NTIxNzY3"}]}
{"thread_id": "PRRC_kwDOArmXAs5ELqpK", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58394#discussion_r1143908938", "comments": [{"message": "Don't do explicit device placement.  Why doesn't this work on GPU?", "timestamp": "2023-03-21T19:38:19Z", "author": "MDQ6VXNlcjI1Mzg3Mzk="}, {"message": "Changes updated.", "timestamp": "2023-03-24T20:27:41Z", "author": "U_kgDOBtdR7A"}]}
{"thread_id": "PRRC_kwDOArmXAs5ELrNM", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58394#discussion_r1143911244", "comments": [{"message": "Why do we need the graph?", "timestamp": "2023-03-21T19:40:48Z", "author": "MDQ6VXNlcjI1Mzg3Mzk="}, {"message": "Changes updated.", "timestamp": "2023-03-24T20:27:59Z", "author": "U_kgDOBtdR7A"}]}
{"thread_id": "PRRC_kwDOArmXAs5ELrSp", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58394#discussion_r1143911593", "comments": [{"message": "Why do we need the conversion to the numpy type?", "timestamp": "2023-03-21T19:41:14Z", "author": "MDQ6VXNlcjI1Mzg3Mzk="}, {"message": "****In the test, to verify the output of tf_reduce sum in BF16 we compare it with np.sum(). But np.sum() seems to have issues with bfloat16. I will illustrate the issue with an example.\r\nA **arr=np.ones([512, 512], dtype=dtypes.bfloat16.as_numpy_dtype)**, this will initialize a numpy ndarray of size **512x512** will all ones. Further, if **np.sum(arr, axis=1)** is performed, the expected output should be numpy array with size=512 and sum[0],...sum[511] = 512 (i.e. all the elements in summation equals 512). But with BFloat16 as dtype np.sum() returns array of size=512 with all elements equal to 256. This is only observed with array of larger size. Interestingly, if an ndarray of 512x512 size has all 2s, the sum across the rows returns an array of size 512, with all elements equal to 512 instead of 1024. Which suggests np.sum() with BF16 only considers first 256 elements from the array. So, to continue with the testing, we perform the np.sum() in FP32 and then convert it to BF16 to finally compare with the TF BF16 sum output.\r\n\r\nTo better understand the issue, I am attaching few screenshots of output from FP32 NP Sum, BF16 NP Sum and BF16 TF Sum.\r\n**FP32 Numpy Sum**\r\n![fp32](https://user-images.githubusercontent.com/114774508/227644337-8beb922c-e229-48b1-a7d0-3ebd5ead0c9d.png)\r\n**BF16 Numpy Sum**\r\n![bf16](https://user-images.githubusercontent.com/114774508/227644384-682842f8-c8bd-4ca8-84d0-48603624ae61.png)\r\n**BF16 TF Sum**\r\n![tf_bf16](https://user-images.githubusercontent.com/114774508/227644424-ebee5ade-1603-48aa-ba30-03c9782eb12e.png)\r\n\r\nIt can be observed from the above screenshots that np.sum() with BF16 gives wrong output.", "timestamp": "2023-03-24T20:54:15Z", "author": "U_kgDOBtdR7A"}, {"message": "It's because bfloat16, which only has 7 bits of precision, can't represent the number 257, so `256+1` gets rounded down to 256, and you get stuck here.  Adding by 2s, you skip over that one issue, but then run into where bfloat16 can't represent 513, 514 or 515, so `512+2` is rounded down to 512 and you get stuck again.  Numpy seems to be doing a naive sequential sum here.\r\n\r\nYou should probably modify your inputs so you're not summing a bunch of ones.\r\n", "timestamp": "2023-03-24T21:56:35Z", "author": "MDQ6VXNlcjI1Mzg3Mzk="}, {"message": "I have updated the input size by bringing it below 256. I tried working with random decimal values, but due to the difference in precision handling for TF and Numpy, the test was still failing. As this being a numpy issue, bringing the input size down to avoid the rounding down error, passes the test.", "timestamp": "2023-04-06T22:57:37Z", "author": "U_kgDOBtdR7A"}]}
{"thread_id": "PRRC_kwDOArmXAs49k7Ny", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58400#discussion_r1033089906", "comments": [{"message": "Where is op version 9?", "timestamp": "2022-11-28T03:24:53Z", "author": "MDQ6VXNlcjEyMzUxNTAx"}, {"message": "op version 9 seems to be here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/versioning/op_version.cc#L204 \r\nPlease note, I had to update the op version to 11 as it seems op version 10 has been added. \r\nPlease let me know if I should not do this. Thanks.", "timestamp": "2022-11-30T11:16:59Z", "author": "MDQ6VXNlcjQ0MzY0NTcz"}, {"message": "SGTM", "timestamp": "2022-12-03T07:57:37Z", "author": "MDQ6VXNlcjEyMzUxNTAx"}]}
{"thread_id": "PRRC_kwDOArmXAs4-Bw_d", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58400#discussion_r1040650205", "comments": [{"message": "Why increment max version by 2?", "timestamp": "2022-12-06T08:33:30Z", "author": "MDQ6VXNlcjUxMTIyNjc="}]}
{"thread_id": "PRRC_kwDOArmXAs4-BxDG", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58400#discussion_r1040650438", "comments": [{"message": "Why increment max version by 2?", "timestamp": "2022-12-06T08:33:47Z", "author": "MDQ6VXNlcjUxMTIyNjc="}]}
{"thread_id": "PRRC_kwDOArmXAs5C7D5n", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58400#discussion_r1122778727", "comments": [{"message": "@SaoirseARM \r\nCould you update it to \"2.13.0\" like `BuiltinOperator_TRANSPOSE_CONV`?", "timestamp": "2023-03-02T08:59:15Z", "author": "MDQ6VXNlcjM3NjQzMjQ4"}]}
{"thread_id": "PRRC_kwDOArmXAs5PAnMi", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58400#discussion_r1325560610", "comments": [{"message": "This has broken an internal test. Could you please explain the change in logic here?", "timestamp": "2023-09-14T08:16:26Z", "author": "MDQ6VXNlcjUxMTIyNjc="}]}
{"thread_id": "PRRC_kwDOArmXAs486DcB", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58494#discussion_r1021851393", "comments": [{"message": "Who does define this?", "timestamp": "2022-11-14T17:22:33Z", "author": "MDQ6VXNlcjI5MDg1MDU="}, {"message": "`FLATBUFFERS_LITTLEENDIAN` is defined in the `flatbuffers` header file here: https://github.com/google/flatbuffers/blob/6f895f54c25aa19f5d84ac6cf7fa8bc955a14e1d/include/flatbuffers/base.h#L119-L139", "timestamp": "2022-11-14T18:04:01Z", "author": "MDQ6VXNlcjc4MTU2Njg4"}, {"message": "Isn't it better to use \"#ifdef FLATBUFFERS_LITTLEENDIAN\" ?", "timestamp": "2022-12-08T18:14:16Z", "author": "MDQ6VXNlcjI5MDg1MDU="}, {"message": "Yes, I'll adjust the code accordingly. Thanks!", "timestamp": "2022-12-08T19:38:17Z", "author": "MDQ6VXNlcjc4MTU2Njg4"}, {"message": "Done.", "timestamp": "2022-12-09T18:34:34Z", "author": "MDQ6VXNlcjc4MTU2Njg4"}]}
{"thread_id": "PRRC_kwDOArmXAs486Dga", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58494#discussion_r1021851674", "comments": [{"message": "for what?", "timestamp": "2022-11-14T17:22:51Z", "author": "MDQ6VXNlcjI5MDg1MDU="}, {"message": "The built `constant_tensor` has a total of 1 * 2 * 5 = 10 bytes data. If the TensorType is set to 0 (FLOAT32), These 10 bytes are not enough for 3 FLOAT32 data which needs 12 bytes. \r\n\r\nThis will cause error when doing LE/BE conversion and it's also a potential bug on LE platform. I think setting the TensorType to 3 (UINT8) is an easy solution for this issue.", "timestamp": "2022-11-14T18:04:05Z", "author": "MDQ6VXNlcjc4MTU2Njg4"}, {"message": "Got it. Then `schema_fb.TensorAddType(builder, TensorType_UINT8)` ?", "timestamp": "2022-12-08T18:16:16Z", "author": "MDQ6VXNlcjI5MDg1MDU="}, {"message": "Yes, using `TensorType_UINT8` is better. I'll update the code. Thanks!", "timestamp": "2022-12-08T19:38:23Z", "author": "MDQ6VXNlcjc4MTU2Njg4"}, {"message": "Used `schema_fb.TensorType.UINT8` instead of value `3`. ", "timestamp": "2022-12-09T18:35:30Z", "author": "MDQ6VXNlcjc4MTU2Njg4"}]}
{"thread_id": "PRRC_kwDOArmXAs48zmf9", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58513#discussion_r1020159997", "comments": [{"message": "What about use count being less than one, do we want to match those as well? I'd imagine we do (e.g. computation root?)", "timestamp": "2022-11-11T12:04:29Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "That's a good point. Currently, the top-level operation in the pattern can have any number of users (see the comment in pattern_matcher.h), and it seems that intermediates/operands by definition have at least one user.", "timestamp": "2022-11-11T19:00:15Z", "author": "MDQ6VXNlcjgwMjk2MTY0"}]}
{"thread_id": "PRRC_kwDOArmXAs48zmst", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/58513#discussion_r1020160813", "comments": [{"message": "Does this pass? What users does multiply itself have?", "timestamp": "2022-11-11T12:05:37Z", "author": "MDQ6VXNlcjM0ODk1OQ=="}, {"message": "This passes because the top-level operation in the prescribed pattern can have an arbitrary number of users, including zero. The requirement of a single user only applies to the intermediates, i.e. the operands of the top-level operation (and their respective operands).", "timestamp": "2022-11-11T19:07:30Z", "author": "MDQ6VXNlcjgwMjk2MTY0"}]}
{"thread_id": "PRRC_kwDOArmXAs4_7FOe", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/59100#discussion_r1072452510", "comments": [{"message": "Why was this change needed?", "timestamp": "2023-01-17T16:46:32Z", "author": "MDQ6VXNlcjcwNjc2Ng=="}, {"message": "Without this change there's a crash generating the lookup table while creating the `UniformQuantizedType`. Honestly, I didn't investigate too much because this change seems to simplify the code and fix the issue. Happy to take another look if you feel it would be worth investigating?", "timestamp": "2023-01-24T09:17:47Z", "author": "MDQ6VXNlcjM1NTM1MDky"}, {"message": "If everything still works for this I'm happy, I don't have context as to why this was like this originally.", "timestamp": "2023-01-26T22:59:30Z", "author": "MDQ6VXNlcjcwNjc2Ng=="}]}
{"thread_id": "PRRC_kwDOArmXAs4_s0BX", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/59101#discussion_r1068712023", "comments": [{"message": "To confirm, collective scatter reduce also requires an upcast? If yes, can you update the text to reflect reducescatter collective?", "timestamp": "2023-01-12T22:02:58Z", "author": "MDQ6VXNlcjcyMzYyNA=="}, {"message": "To be honest, I didn't try running it with int32. There is nothing in NCCL that would prevent it from working, but I know that TensorFlow itself has many limitations for int32 + GPU and I saw similar restrictions throughout DTensor. I was going to ask if anyone working on DTensor knew more about where this upcast came from in `LowerAllReduce`. @rainwoodman \r\n\r\nI can give it a try and see what happens as well.", "timestamp": "2023-01-13T20:20:35Z", "author": "MDQ6VXNlcjEyOTgxNDc0"}, {"message": "I don't know if the NCCL path CollectiveReduceV2 requires the upcast workaround. When I added the workaround dtensor didn't yet work with NCCL. I'd also suggest try to run without the upcast on the NCCL path, and see what happens.\r\n\r\nTensorFlow's distributed runtime doesn't support int32 reductions. The limitation from what I read, indeed runs pretty deep into TensorFlow baked in the assumption to use int32 as a proxy represent host-side shape Tensors for GPU, since the very early days. \r\n\r\nThis reminded me we have some NCCL specific unit tests that are not yet OSSed. (Some use multiprocessing and may be a bit complicated to port). ", "timestamp": "2023-01-17T17:22:58Z", "author": "MDQ6VXNlcjEzODA2MA=="}, {"message": "Thanks @rainwoodman.\r\n\r\nI did some experiments, it looks like both CollectiveReduceV2 and CollectiveReduceScatterV2 work with int32 on GPU using the NCCL path. If it uses the RingReduce path, it will segfault during `tensorflow::collective_util::ComputeBinOp`.\r\n\r\nI'll go ahead and remove the upcast since we are always using NCCL for reduce scatter. It looks like we could remove it for CollectiveReduceV2 too if we are using NCCL in a follow-up perhaps.", "timestamp": "2023-01-17T23:02:35Z", "author": "MDQ6VXNlcjEyOTgxNDc0"}]}
{"thread_id": "PRRC_kwDOArmXAs5AAoyE", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/59256#discussion_r1073908868", "comments": [{"message": "Maybe users just installed it to the wrong path. How about \"Could not find TensorRT.\" ?\r\n\r\n@reedwm What do you think?", "timestamp": "2023-01-18T18:17:57Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "\"Could not find TensorRT.\" SGTM.\r\n\r\nI think ideally, we would only give this message if CUDA drivers were installed. But I'm not sure how difficult that is, and regardless no need to do this in this PR.", "timestamp": "2023-01-18T19:45:04Z", "author": "MDQ6VXNlcjY1MTAyMDM="}, {"message": "Thank you, Reed!", "timestamp": "2023-01-19T12:53:55Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "Thanks Penporn and Reed, I will make the change.", "timestamp": "2023-01-19T21:09:37Z", "author": "MDQ6VXNlcjI1Mzc0ODA4"}]}
{"thread_id": "PRRC_kwDOArmXAs5H33SC", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/59880#discussion_r1205826690", "comments": [{"message": "This is 1 and -1 right, why are both allowed here?", "timestamp": "2023-05-25T17:33:21Z", "author": "MDQ6VXNlcjcwNjc2Ng=="}, {"message": "good catch. let me fix this.", "timestamp": "2023-05-25T17:44:20Z", "author": "MDQ6VXNlcjY1MDQyMDY="}, {"message": "turned out the original code always added a stride dimension of abs(stride[i]).\r\nthe refactor in this patch is simply to filter out cases where abs(stride[i]) is 1 because there is no need to add a extra dimension of value 1.\r\n", "timestamp": "2023-05-25T19:52:36Z", "author": "MDQ6VXNlcjY1MDQyMDY="}]}
{"thread_id": "PRRC_kwDOArmXAs5D33b7", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/60005#discussion_r1138718459", "comments": [{"message": "Could this be avoided? E.g., could we detect rather than recover from?", "timestamp": "2023-03-16T14:00:00Z", "author": "MDQ6VXNlcjcwNjc2Ng=="}, {"message": "Tks for the review! \r\nWe had some discussions on this in our internal review and it's more or less a code reuse issue: \r\n\r\n1. We could catch this special case ( i-input_zp == 0 ) for Rsqrt and simply push_back a INT8_MAX. But this would lead to more duplicate code. \r\n2. By catching exceptions, we would be able to handle more cases other than only Rsqrt in the future. \r\n\r\ncc @eric-k256 if has more comments. \r\n\r\nJerry", "timestamp": "2023-03-16T16:34:56Z", "author": "MDQ6VXNlcjI0NDUxODU5"}, {"message": "I think I've got a better option now. In theory the existing code could do something if NaN or infinities were present, but I think we're better off avoiding those when generating table values.\r\n\r\nWe can pre-test overflow with this code:\r\n```\r\ndouble max = (output_scale > 1.0) ? DBL_MAX : DBL_MAX * output_scale;\r\nif (transformed > max) {\r\n  rescaled = INT8_MAX;\r\n}\r\nelse {\r\n  rescaled = std::llround(transformed / output_scale);\r\n}\r\n```", "timestamp": "2023-03-16T20:29:33Z", "author": "MDQ6VXNlcjcxNjU4ODk4"}, {"message": "New patch set pushed. ", "timestamp": "2023-03-17T21:33:10Z", "author": "MDQ6VXNlcjI0NDUxODU5"}]}
{"thread_id": "PRRC_kwDOArmXAs5Lrqf-", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/60026#discussion_r1269737470", "comments": [{"message": "Is this always going to work?  This seems like it would be very platform-dependent.", "timestamp": "2023-07-20T17:02:37Z", "author": "MDQ6VXNlcjI1Mzg3Mzk="}, {"message": "Thanks @cantonios for the comment. Yes, as you have said it is platform dependent. I have made updates in 528c1dc to address this. For platforms that pass through `ifdefs` if file doesn't exists we will simply just revert to default behaviour as will not be picking up heuristics.", "timestamp": "2023-07-21T10:33:29Z", "author": "MDQ6VXNlcjc5OTE2MzU4"}]}
{"thread_id": "PRRC_kwDOArmXAs5Lhliv", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/60375#discussion_r1267095727", "comments": [{"message": "What clang error did we get without the extra pair of parentheses? Could you please post the error message? Thank you!", "timestamp": "2023-07-18T17:23:24Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}, {"message": "@penpornk Below is the error from Clang:\r\n```\r\ntensorflow/core/kernels/mkl/mkl_concat_op.cc:367:35: warning: parentheses were disambiguated as a function declaration [-Wvexing-parse]\r\n      dnnl::memory::desc source_md(memory::desc(GET_MEMORY_DESC(srcs_md[i])));\r\n                                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/kernels/mkl/mkl_concat_op.cc:367:36: note: add a pair of parentheses to declare a variable\r\n      dnnl::memory::desc source_md(memory::desc(GET_MEMORY_DESC(srcs_md[i])));\r\n                                   ^\r\n                                   (                                        )\r\ntensorflow/core/kernels/mkl/mkl_concat_op.cc:368:23: error: no matching member function for call to 'push_back'\r\n      context_.src_md.push_back(source_md);\r\n      ~~~~~~~~~~~~~~~~^~~~~~~~~\r\n``` \r\n", "timestamp": "2023-07-18T17:46:01Z", "author": "MDQ6VXNlcjI4MTEzMjQx"}, {"message": "Got it. Thank you very much!", "timestamp": "2023-07-18T18:08:26Z", "author": "MDQ6VXNlcjM4MDg1OTA5"}]}
{"thread_id": "PRRC_kwDOArmXAs5SX1JE", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/60585#discussion_r1381978692", "comments": [{"message": "What changed in this block?", "timestamp": "2023-11-03T16:54:36Z", "author": "MDQ6VXNlcjI1Mzg3Mzk="}, {"message": "It seems to have no impact. I reversed it to previous version.", "timestamp": "2024-01-10T14:17:46Z", "author": "MDQ6VXNlcjMyODkzMjk5"}]}
{"thread_id": "PRRC_kwDOArmXAs5LwLxJ", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/60749#discussion_r1270922313", "comments": [{"message": "Could you explain why this change is needed?", "timestamp": "2023-07-21T17:33:51Z", "author": "MDQ6VXNlcjI5MDg1MDU="}, {"message": "Hi @terryheo , When I ran the bazel test command, it gave error saying the DCHECK_EQ macro/function was not defined.\r\n\r\nSo I wrote the check via if condition.", "timestamp": "2023-07-21T18:13:02Z", "author": "U_kgDOB8AbiQ"}, {"message": "Oh I see. We might consider using TFLITE_DCHECK_EQ but this looks fine to me. Thanks!", "timestamp": "2023-07-21T18:34:39Z", "author": "MDQ6VXNlcjI5MDg1MDU="}, {"message": "Could you apply TF code style?\r\n\r\nhttps://www.tensorflow.org/community/contribute/code_style", "timestamp": "2023-08-31T21:10:55Z", "author": "MDQ6VXNlcjI5MDg1MDU="}, {"message": "Sure, will update the formatting.", "timestamp": "2023-08-31T21:14:34Z", "author": "U_kgDOB8AbiQ"}]}
{"thread_id": "PRRC_kwDOArmXAs5LdjHF", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/61207#discussion_r1266037189", "comments": [{"message": "there is a linker error `ld.lld: error: undefined reference due to --no-allow-shlib-undefined: typeinfo for tensorflow::NextPluggableDeviceFactory`. Do you know the cause? Will moving the implementation to next_pluggable_device_factory.cc fix it?", "timestamp": "2023-07-18T00:16:46Z", "author": "MDQ6VXNlcjUxMTgwOTg="}, {"message": "I think it may caused by `dynamic_cast`, dynamic_cast needs rtti link option,  I didn't reproduce it yet, does it fail when building android/tf-lite? I saw they will disable rtti in tf-opt\r\nhttp://web.archive.org/web/20100503172629/http://www.pubbs.net/201004/gcc/25970-linker-error-undefined-reference-to-typeinfo-for-a-with-no-rtti-option.html\r\nhttps://stackoverflow.com/questions/307352/g-undefined-reference-to-typeinfo\r\n```\r\nThis can also happen when you mix -fno-rtti and -frtti code. Then you need to ensure that any class, which type_info is accessed in the -frtti code, have their key method compiled with -frtti. Such access can happen when you create an object of the class, use dynamic_cast etc.\r\n```\r\n```\r\n-fno-rtti\r\nDisable generation of information about every class with virtual functions for use by the C++ runtime type identification features (dynamic_cast and typeid). If you don't use those parts of the language, you can save some space by using this flag. Note that exception handling uses the same information, but it will generate it as needed. The dynamic_cast operator can still be used for casts that do not require runtime type information, i.e. casts to void * or to unambiguous base classes.\r\n```\r\n```\r\n182     NextPluggableDeviceFactory* device_factory =\r\n183         dynamic_cast<NextPluggableDeviceFactory*>(\r\n184             DeviceFactory::GetFactory(device_name));\r\n```", "timestamp": "2023-07-18T02:46:32Z", "author": "MDQ6VXNlcjYzNDY4NTM="}, {"message": "Got it. Linking some CPU tests (e.g. tensorflow/compiler/jit/xla_platform_info_test_cpu) and GPU tests (e.g. tensorflow/c/env_test_gpu) are failing. ", "timestamp": "2023-07-19T17:44:10Z", "author": "MDQ6VXNlcjUxMTgwOTg="}, {"message": "@jyingl3  seems the test failure is caused by dynamic_cast<NextPluggableDeviceFactory>, even I removed compilation_device_name.  I modify it as IsPluggableDevice and can create PjRtClient to idendify whether it is a nextpluggabledevice.  seems test passed now. can you help to have a look? thanks\r\n```\r\n  if (DeviceFactory::IsPluggableDevice(device_name) &&\r\n      GetOrCreatePjRtClient(DeviceType(device_name)).ok()) {\r\n\r\n```", "timestamp": "2023-07-20T06:07:53Z", "author": "MDQ6VXNlcjYzNDY4NTM="}, {"message": "Got it. Can we use GetPjRtClient from tensorflow/core/tfrt/common/pjrt_util.cc instead of GetOrCreatePjRtClient? Just do not want to create a PJRT client unintendedly", "timestamp": "2023-07-20T17:00:25Z", "author": "MDQ6VXNlcjUxMTgwOTg="}, {"message": "done", "timestamp": "2023-07-21T02:15:31Z", "author": "MDQ6VXNlcjYzNDY4NTM="}]}
{"thread_id": "PRRC_kwDOArmXAs5Pw9u1", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/61941#discussion_r1338235829", "comments": [{"message": "This looks like a bug fix that can land in a separate PR, together with a regression test?", "timestamp": "2023-09-27T08:20:39Z", "author": "MDQ6VXNlcjE0MzA5Nzcy"}, {"message": "makes sense, I can remove this change and we will add it as a separate PR", "timestamp": "2023-09-27T10:14:24Z", "author": "U_kgDOCGm34A"}]}
{"thread_id": "PRRC_kwDOArmXAs5SvvkR", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/62362#discussion_r1388247313", "comments": [{"message": "Why is the max error larger when you have more information for computing it?  What if use_gpu is `False` - then I'd expect the error to not change.  And have you tested with rocm to know there is a difference?  ", "timestamp": "2023-11-09T16:22:19Z", "author": "MDQ6VXNlcjI1Mzg3Mzk="}, {"message": "Fixed", "timestamp": "2023-11-10T16:51:33Z", "author": "MDQ6VXNlcjQyMTIzOTg="}]}
{"thread_id": "PRRC_kwDOArmXAs5VZ1vd", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/62648#discussion_r1432837085", "comments": [{"message": "Are you sure it's only enabled with MKL?  I believe I added this for CPU in commit 8cb3e0a4901bb7d5f62acc033220408ade3dc4e8.", "timestamp": "2023-12-20T15:09:17Z", "author": "MDQ6VXNlcjI1Mzg3Mzk="}, {"message": "Good catch! Fixed it.", "timestamp": "2023-12-21T20:13:03Z", "author": "MDQ6VXNlcjI4MTEzMjQx"}, {"message": "Looks like removing the MKL check is causing the test to fail on GPU. I've fixed it by disabling the test on GPU.", "timestamp": "2023-12-21T22:16:34Z", "author": "MDQ6VXNlcjI4MTEzMjQx"}]}
{"thread_id": "PRRC_kwDOArmXAs5V8y-6", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/62734#discussion_r1442000826", "comments": [{"message": "Is this true for XLA as well?", "timestamp": "2024-01-04T16:36:19Z", "author": "MDQ6VXNlcjI1Mzg3Mzk="}, {"message": "@tf-security-autoupdate-bot @tensorflow-jenkins @googlebot @snyk-bot ", "timestamp": "2024-01-04T18:00:38Z", "author": "MDQ6VXNlcjUyNjc0OTM1"}, {"message": "> @tf-security-autoupdate-bot @tensorflow-jenkins @googlebot @snyk-bot \n\n@davidmerwin @tensorflow-copybara ", "timestamp": "2024-01-04T18:01:10Z", "author": "MDQ6VXNlcjUyNjc0OTM1"}, {"message": "\r\n\r\n\r\n> Is this true for XLA as well?\r\n\r\n@cantonios ,\r\n\r\nWith XLA, all out of bound `ids` are replaced with **maximum** of `ids` inferred from the input `params`.Both xla-cpu and xla-gpu behaviour is same.But entirely different from non-xla results. Please refer attached [gist](https://colab.sandbox.google.com/gist/SuryanarayanaY/98ae4c2a85e4f3c7cac58bf8cddc1737/62628.ipynb).", "timestamp": "2024-01-05T10:46:54Z", "author": "U_kgDOBur8Og"}, {"message": "Right, so this added comment is not always true.  Please update the comment then.", "timestamp": "2024-01-05T18:28:29Z", "author": "MDQ6VXNlcjI1Mzg3Mzk="}, {"message": "@cantonios , Updated the XLA behaviour as suggested. Thanks!", "timestamp": "2024-01-07T13:43:15Z", "author": "U_kgDOBur8Og"}]}
{"thread_id": "PRRC_kwDOArmXAs5WEKqx", "owner": "", "url": "https://github.com/tensorflow/tensorflow/pull/62750#discussion_r1443932849", "comments": [{"message": "A little unclear on what works/doesn't work in graph/eager modes still. This seems to be ok with my test cases though", "timestamp": "2024-01-07T05:20:33Z", "author": "MDQ6VXNlcjc4NzMzMjM="}]}
